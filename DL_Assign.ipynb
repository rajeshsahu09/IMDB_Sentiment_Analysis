{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Assign.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajeshsahu09/CS69002_9A_18CS60R19/blob/master/DL_Assign.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "wuB0HM37teAc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Movie Review Sentiment Analysis"
      ]
    },
    {
      "metadata": {
        "id": "rKlXTA5zfx20",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Run on **GPU**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "_NV6CE-qtvGO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Import Header Files"
      ]
    },
    {
      "metadata": {
        "id": "WE7OEcPOtzJf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v4037ccUuFFp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load the Dataset and Visualise"
      ]
    },
    {
      "metadata": {
        "id": "EhWFepP2t3eA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# import io\n",
        "# df_train = pd.read_csv(io.StringIO(uploaded['Train_20K.csv'].decode('utf-8')), sep='\\t')\n",
        "# df_train.head()\n",
        "url = \"https://raw.githubusercontent.com/binny-mathew/IITKGP_CS69002_Spring_2019/master/Dataset/Train_20K.csv\"\n",
        "df = pd.read_csv(url, sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mvAXyCoDaR3b",
        "colab_type": "code",
        "outputId": "b02b6dd4-dc6d-4a7e-b163-4cc01a7f5cc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17999, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "44YtF48vX24a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_1 = df[df['label']==1] # get only label == 1\n",
        "df_0 = df[df['label']==0] # get only label == 0\n",
        "\n",
        "range_1 = int(0.9*len(df_1))\n",
        "range_2 = int(0.9*len(df_0))\n",
        "\n",
        "np.random.seed(42) # fix the seed value\n",
        "df_1 = df_1.iloc[np.random.permutation(len(df_1))] # shuffled the data set of label==1\n",
        "df_0 = df_0.iloc[np.random.permutation(len(df_0))] # shuffled the data set of label==0\n",
        "\n",
        "temp_1_train = df_1.iloc[:range_1]\n",
        "temp_2_train = df_0.iloc[:range_2]\n",
        "df_train = pd.concat([temp_1_train, temp_2_train])\n",
        "\n",
        "temp_1_val = df_1.iloc[range_1:]\n",
        "temp_2_val = df_0.iloc[range_2:]\n",
        "df_val = pd.concat([temp_1_val, temp_2_val])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T9FBTqABuRlO",
        "colab_type": "code",
        "outputId": "d69bd76f-f55c-407c-c85c-393ffa2280da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "df_train = df_train.iloc[np.random.permutation(len(df_train))] # shuffled the train data set\n",
        "df_val = df_val.iloc[np.random.permutation(len(df_val))] # shuffled the validation data set\n",
        "len(df_train), len(df_val)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16198, 1801)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "0WFB1TtsxD3F",
        "colab_type": "code",
        "outputId": "58f961b7-c08e-4b14-bda3-620d7be7bf5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "print('Number of Negative movie reviews', len(df_train[df_train['label']==0]))\n",
        "print('Number of Positive movie reviews', len(df_train[df_train['label']==1]))\n",
        "print('Number of movie reviews', len(df_train['label']))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Negative movie reviews 8094\n",
            "Number of Positive movie reviews 8104\n",
            "Number of movie reviews 16198\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sYtzJk8r3lm5",
        "colab_type": "code",
        "outputId": "360bccf5-0423-4f11-b671-171b772e13d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "print('Number of Negative movie reviews', len(df_val[df_val['label']==0]))\n",
        "print('Number of Positive movie reviews', len(df_val[df_val['label']==1]))\n",
        "print('Number of movie reviews', len(df_val['label']))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Negative movie reviews 900\n",
            "Number of Positive movie reviews 901\n",
            "Number of movie reviews 1801\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uuZxOMpkQoka",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/binny-mathew/IITKGP_CS69002_Spring_2019/master/Dataset/Test_5K.csv\"\n",
        "df_test = pd.read_csv(url, sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TVzhPcGGxf2-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data pre-processing"
      ]
    },
    {
      "metadata": {
        "id": "u-8IR64TKH0F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Get Train Data"
      ]
    },
    {
      "metadata": {
        "id": "JprKnOXgxXkE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_raw_text_reviews = df_train['text'].astype(str).tolist()\n",
        "train_text_labels = df_train['label'].astype(int).tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TwU4zROpKLdQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Get Validation Data"
      ]
    },
    {
      "metadata": {
        "id": "7TsL1ACWJ_3V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "val_raw_text_reviews = df_val['text'].astype(str).tolist()\n",
        "val_text_labels = df_val['label'].astype(int).tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YixDzEnXLZN-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Get Test Data"
      ]
    },
    {
      "metadata": {
        "id": "MQHaSpnJJ9eT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_raw_text_reviews = df_test['text'].astype(str).tolist()\n",
        "test_text_labels = df_test['label'].astype(int).tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ui-q6WRqOrB4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Cleaning the raw input data"
      ]
    },
    {
      "metadata": {
        "id": "8_4b4FmShb0u",
        "colab_type": "code",
        "outputId": "5bb8f1df-c795-4c29-83fc-9d591e680b04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Finding stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# import spacy\n",
        "# print('spaCy Version: %s' % (spacy.__version__))\n",
        "# spacy_nlp = spacy.load('en_core_web_sm')\n",
        "# # stop word list\n",
        "# stop_words = set(spacy.lang.en.stop_words.STOP_WORDS)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "s3AxEhD_z1DM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess_document(doc):\n",
        "    # negative sense should not be eleminated + some short representation\n",
        "    CONTRACTIONS = {\"didn't\":\"did not\", \"mayn't\":\"may not\", \"can't\":\"can not\", \"won't\":\"will not\", \"isn't\":\"is not\", \"amn't\":\"am not\",\\\n",
        "                  \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\", \"couldn't\":\"could not\", \\\n",
        "                  \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\\\n",
        "                  \"i'll\":\"i will\", \"you'll\":\"you will\", \"they'll\":\"they will\",\\\n",
        "                  \"may've\":\"may have\", \"can've\":\"can have\", \"will've\":\"will have\", \"you've\":\"you have\", \\\n",
        "                  \"could've\":\"could have\", \"would've\":\"would have\", \"you've\":\"you have\", \"they\":\"they have\",\\\n",
        "                  \"i've\":\"i have\", \"you've\":\"you have\", \"we've\":\"we have\", \"there's\":\"there is\", \"i'm\":\"i am\",\\\n",
        "                  \"it's\":\"it is\", \"what's\":\"what is\", \"where's\":\"where is\", \"how's\":\"how is\", \"i'd\":\"i had\"}\n",
        "    punctuation = string.punctuation + \"\\n\\n\"\n",
        "    punc_replace = ''.join([' ' for s in punctuation]) # required for replacing punctuation with null ('')\n",
        "    doc_clean = doc.replace('-', ' ') # replace - with null str\n",
        "    doc_clean = (doc_clean.encode('ascii', 'ignore')).decode(\"utf-8\")\n",
        "    doc_clean = doc_clean.replace('<br />', '') # replace <br /> with ''\n",
        "    doc_clean = doc_clean.replace(\"â€™\", \"'\") # replace <br /> with null str\n",
        "    doc_clean = [CONTRACTIONS[word] if word in CONTRACTIONS else word for word in doc_clean.split(' ')] # replacing some common short forms\n",
        "    doc_clean = \" \".join(doc_clean) # list to sentence of strings\n",
        "    doc_clean = re.sub(r'\\W +', ' ', doc_clean) # except [a-zA-Z0-9_]\n",
        "    doc_clean = re.sub(r'\\d+', ' ', doc_clean) # remove numbers [0-9]\n",
        "    trans_table = str.maketrans(punctuation, punc_replace); # replace punctuations with ' '\n",
        "    doc_clean = ' '.join([word.translate(trans_table) for word in doc_clean.split(' ')])\n",
        "    doc_clean = doc_clean.split(' ')\n",
        "    doc_clean = [word for word in doc_clean if len(word) > 1]\n",
        "    # removing the stopwords from a sentence\n",
        "    doc_clean = [word for word in doc_clean if not word.lower() in stop_words or word.lower() == 'not' or word.lower() == 'no']\n",
        "    return doc_clean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rSNuXUopzavX",
        "colab_type": "code",
        "outputId": "98e86697-1f7c-4a30-f957-15056c44d062",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "train_text_reviews = [preprocess_document(review.lower()) for review in train_raw_text_reviews]\n",
        "print (train_text_reviews[len(train_text_reviews)-2])\n",
        "print (train_text_labels[len(train_text_labels)-2])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['three', 'tales', 'told', 'film', 'seemed', 'shot', 'without', 'knowledge', 'combined', 'vignette', 'film', 'makers', 'relate', 'three', 'vignettes', 'connected', 'shrink', 'martin', 'kove', 'although', 'never', 'see', 'leads', 'kove', 'first', 'vignette', 'sexy', 'vivian', 'schilling', 'woman', 'afraid', 'everything', 'sun', 'makes', 'adrian', 'monk', 'look', 'brave', 'paranoia', 'laced', 'evening', 'home', 'alone', 'literally', 'scream', 'vivian', 'ridiculous', 'things', 'spends', 'majority', 'time', 'nighty', 'shows', 'amazing', 'features', 'film', 'worst', 'not', 'nail', 'biting', 'second', 'vignette', 'owned', 'bill', 'paxton', 'portrays', 'roommate', 'hell', 'geeky', 'roommate', 'allows', 'take', 'complete', 'advantage', 'bill', 'whenever', 'last', 'vignette', 'funny', 'man', 'fears', 'death', 'take', 'moment', 'much', 'like', 'pal', 'choked', 'death', 'olive', 'not', 'interesting', 'movie', 'whole', 'seems', 'chopped', 'together', 'little', 'thought', 'involved', 'must', 'bill', 'paxton', 'fans']\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "W6psZ2jxNOoP",
        "colab_type": "code",
        "outputId": "ba24cbab-fc55-40a9-9e82-f2d832f61de1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "val_text_reviews = [preprocess_document(review.lower()) for review in val_raw_text_reviews]\n",
        "print (val_text_reviews[len(val_text_reviews)-2])\n",
        "print (val_text_labels[len(val_text_labels)-2])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['even', 'one', 'not', 'realize', 'sellers', 'poor', 'health', 'time', 'filming', 'passed', 'away', 'film', 'release', 'fiendish', 'plot', 'dr', 'fu', 'manchu', 'would', 'painful', 'viewing', 'supposedly', 'lampoon', 'sax', 'rohmer', 'famous', 'oriental', 'villain', 'lacks', 'focus', 'potential', 'satirical', 'commentary', 'anti', 'oriental', 'overtones', 'rohmer', 'concept', 'ignored', 'indeed', 'movie', 'employs', 'racist', 'insults', 'hardly', 'actual', 'jokes', 'gags', 'mostly', 'actors', 'behaving', 'idiotically', 'spouting', 'dreary', 'lines', 'especially', 'distressing', 'see', 'sid', 'caesar', 'forced', 'spout', 'curses', 'racial', 'slurs', 'attempted', 'laughs', 'actors', 'embarrass', 'well', 'peter', 'sellers', 'plays', 'dual', 'roles', 'sinister', 'fu', 'manchu', 'trying', 'concoct', 'formula', 'regain', 'youth', 'stalwart', 'british', 'foe', 'nayland', 'smith', 'sellers', 'not', 'one', 'hundred', 'per', 'cent', 'bad', 'conveys', 'quirky', 'warmth', 'smith', 'discusses', 'fetishistic', 'attachment', 'lawn', 'mower', 'oddly', 'moving', 'manchu', 'expresses', 'love', 'english', 'music', 'hall', 'entertainment', 'time', 'plays', 'roles', 'weary', 'grimness', 'thus', 'sabotaging', 'comical', 'possibilities', 'sellers', 'routines', 'revitalizes', 'fading', 'strength', 'electric', 'shocks', 'particularly', 'excruciating', 'seems', 'convincingly', 'agonized', 'funny', 'genuinely', 'witty', 'lines', 'apt', 'slapstick', 'bit', 'burt', 'kwouk', 'cato', 'pink', 'panther', 'films', 'one', 'manchu', 'minions', 'helen', 'mirren', 'amusing', 'musical', 'numbers', 'cannot', 'salvage', 'mess', 'anyone', 'wants', 'understand', 'peter', 'sellers', 'considered', 'comedic', 'genius', 'not', 'learn', 'anything', 'fiendish', 'plot', 'dr', 'fu', 'manchu']\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "REdaGj3VNOEX",
        "colab_type": "code",
        "outputId": "2cb2ade6-6646-4805-de43-e8acfc4838db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "test_text_reviews = [preprocess_document(review.lower()) for review in test_raw_text_reviews]\n",
        "print (test_text_reviews[len(test_text_reviews)-2])\n",
        "print (test_text_labels[len(test_text_labels)-2])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['shining', 'know', 'weird', 'movie', 'movie', 'everyone', 'people', 'claim', 'not', 'like', 'horror', 'films', 'always', 'say', 'shining', 'terrific', 'film', 'stanley', 'kubrick', 'classic', 'vision', 'stephen', 'king', 'horror', 'tale', 'madness', 'blood', 'incredible', 'film', 'wither', 'seen', 'not', 'heard', 'know', 'lines', 'know', 'classic', 'images', 'could', 'forget', 'jack', 'johnny', 'could', 'forget', 'work', 'no', 'play', 'make', 'jack', 'dull', 'boy', 'could', 'forget', 'chilling', 'ending', 'film', 'unforgettable', 'honestly', 'opinion', 'kubrick', 'best', 'work', 'know', 'lot', 'argument', 'department', 'lot', 'people', 'say', 'space', 'odyssey', 'clockwork', 'orange', 'even', 'dr', 'strangelove', 'film', 'pioneered', 'film', 'making', 'shining', 'perfected', 'tale', 'isolation', 'madness', 'terrifying', 'images', 'ultimate', 'ghost', 'story', 'crawl', 'underneath', 'skin', 'jack', 'torrance', 'jack', 'son', 'danny', 'jack', 'wife', 'wendy', 'arrive', 'overlook', 'hotel', 'closing', 'day', 'elderly', 'african', 'american', 'chef', 'dick', 'hallorann', 'surprises', 'danny', 'speaking', 'telepathically', 'offering', 'ice', 'cream', 'explains', 'danny', 'grandmother', 'shared', 'gift', 'called', 'communication', 'shining', 'danny', 'asks', 'anything', 'afraid', 'hotel', 'particularly', 'room', 'dick', 'tells', 'danny', 'hotel', 'certain', 'shine', 'many', 'memories', 'not', 'good', 'advises', 'stay', 'room', 'circumstances', 'danny', 'curiosity', 'room', 'finally', 'gets', 'better', 'sees', 'room', 'opened', 'danny', 'shows', 'injured', 'visibly', 'traumatized', 'jack', 'tells', 'wendy', 'loves', 'family', 'seeing', 'wendy', 'thinks', 'jack', 'abusing', 'danny', 'jack', 'wanders', 'hotel', 'gold', 'room', 'meets', 'ghostly', 'bartender', 'named', 'lloyd', 'danny', 'starts', 'calling', 'word', 'redrum', 'frantically', 'scribbling', 'walls', 'goes', 'trance', 'withdraws', 'says', 'tony', 'imaginary', 'friend', 'jack', 'sabotages', 'hotel', 'radio', 'cutting', 'communication', 'outside', 'world', 'hallorann', 'received', 'danny', 'telepathic', 'cry', 'help', 'way', 'wendy', 'discovers', 'jack', 'typing', 'endless', 'pages', 'manuscript', 'repeating', 'work', 'no', 'play', 'makes', 'jack', 'dull', 'boy', 'formatted', 'various', 'ways', 'horrified', 'jack', 'threatens', 'knocks', 'unconscious', 'baseball', 'bat', 'locking', 'storage', 'locker', 'kitchen', 'jack', 'converses', 'grady', 'door', 'locker', 'unlocks', 'releasing', 'danny', 'written', 'redrum', 'lipstick', 'door', 'wendy', 'bedroom', 'looks', 'mirror', 'sees', 'murder', 'spelled', 'backwards', 'jack', 'picks', 'axe', 'begins', 'chop', 'door', 'leading', 'family', 'living', 'quarters', 'johnny', 'jack', 'legendary', 'image', 'born', 'shining', 'one', 'films', 'seriously', 'make', 'time', 'see', 'incredible', 'film', 'still', 'gives', 'nightmares', 'jack', 'nicholson', 'performance', 'timeless', 'unforgettable', 'one', 'also', 'feel', 'extremely', 'overlooked', 'shelley', 'duvall', 'scene', 'finding', 'jack', 'rant', 'work', 'incredible', 'look', 'horror', 'see', 'fear', 'face', 'realizing', 'husband', 'mad', 'also', 'another', 'incredible', 'scene', 'jack', 'sees', 'ghost', 'woman', 'bathtub', 'honestly', 'one', 'terrifying', 'scenes', 'horror', 'cinema', 'reason', 'film', 'well', 'known', 'film', 'perfection', 'simpsons', 'shown', 'films', 'film', 'forever', 'stay', 'see', 'trust']\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ydrqMX261kQ-",
        "colab_type": "code",
        "outputId": "ae045c65-5aca-46db-d35d-d5c754c5af72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "xWfJK2HvwwQn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_vocabulary(sentences):\n",
        "    # Build vocabulary\n",
        "    dictWordCount = {}\n",
        "    for sent in sentences:\n",
        "        for word in sent:\n",
        "            dictWordCount[word] = 0 # initialising the dict value to zero\n",
        "    for sent in sentences:\n",
        "        for word in sent:\n",
        "            dictWordCount[word] += 1 # updating the dictionary count\n",
        "    \n",
        "    temp = dictWordCount.copy()\n",
        "    for key, val in temp.items():\n",
        "        if(dictWordCount[key] <= 5):\n",
        "            del dictWordCount[key]\n",
        "    \n",
        "    # Mapping from index to word\n",
        "    vocabulary_inv = sorted(dictWordCount, key=dictWordCount.__getitem__, reverse=True)\n",
        "    \n",
        "    # Mapping from word to index\n",
        "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
        "    return vocabulary, vocabulary_inv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MKifaS85Dkap",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Creating Tokens and Types"
      ]
    },
    {
      "metadata": {
        "id": "9cpQlJ3TytgZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_to_ix, ix_to_word = build_vocabulary(train_text_reviews+val_text_reviews+test_text_reviews)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PLBgxuf0xDB9",
        "colab_type": "code",
        "outputId": "c6fcadc9-0f42-4040-ff1c-5f88f88613fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = len(word_to_ix)\n",
        "ix_to_word[word_to_ix['kick']]=='kick', word_to_ix['kick'], VOCAB_SIZE"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, 1814, 25047)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "GMbWbZdr1reC",
        "colab_type": "code",
        "outputId": "1d70fc36-748c-4b1d-f8be-3bfeefda9b6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "403"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "yNUlKuJeNJEu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## PyTorch"
      ]
    },
    {
      "metadata": {
        "id": "SqLOymKPLPug",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable # Automatic gradients are calculated and back-propagated through the computational graph\n",
        "import copy\n",
        "import csv\n",
        "import time\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sWqAwOAI7D54",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generate the BOW Vectors"
      ]
    },
    {
      "metadata": {
        "id": "UZK9TAQI7Fbm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_bow_vector(sentence, word_to_ix):\n",
        "    # create a vector of zeros of vocab size = len(word_to_idx)\n",
        "    vec = torch.zeros(len(word_to_ix)).to('cuda:0') #, device=device) # make 1D vector of len = vocab size\n",
        "    for word in sentence:\n",
        "        if word not in word_to_ix:\n",
        "#             raise ValueError('Word',word,' not present in the dictionary. Sorry!')\n",
        "            pass\n",
        "        else:\n",
        "            vec[word_to_ix[word]]+=1 # count the number of occurance of same word in a sentences\n",
        "            \n",
        "    return vec.view(1, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6p3T6oo_T99z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Prepare Input Train and Validation torch vector"
      ]
    },
    {
      "metadata": {
        "id": "W7xoiOiAyZN5",
        "colab_type": "code",
        "outputId": "64778fe9-6c28-4383-fc6c-85ac7f5c5f0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# store the bag of word vectors for each sentences and wraping to tensor of torch type\n",
        "tic = time.time()\n",
        "train_data = [Variable(make_bow_vector(instance, word_to_ix)).to('cuda:0') for instance in train_text_reviews]\n",
        "num_train_data = len(train_text_reviews)\n",
        "\n",
        "val_data = [Variable(make_bow_vector(instance, word_to_ix)).to('cuda:0') for instance in val_text_reviews]\n",
        "num_val_data = len(val_text_reviews)\n",
        "toc = time.time()\n",
        "num_train_data, num_val_data, (toc-tic)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16198, 1801, 94.47736620903015)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "HAi56d_fmMwI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Prepare Test Instance"
      ]
    },
    {
      "metadata": {
        "id": "fkYBaktsF-sm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_test = []\n",
        "data_test = [Variable(make_bow_vector(instance, word_to_ix)).to('cuda:0') for instance in test_text_reviews]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "deO_hz8vT19u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Validation Accuracy Computation"
      ]
    },
    {
      "metadata": {
        "id": "cStQIVFfESWM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate_validation_accuracy(data, net):\n",
        "    sum_loss = 0\n",
        "    \n",
        "    true_positive = 0\n",
        "    true_negative = 0\n",
        "    false_positive = 0\n",
        "    false_negative = 0\n",
        "    \n",
        "    for i, instance in enumerate(data):\n",
        "        label = val_text_labels[i] # get the label of the corresponding instace\n",
        "        label = Variable(torch.FloatTensor([label])).resize_((1,1)).to('cuda:0') # wraps a tensor for label\n",
        "        \n",
        "#         vec = Variable(make_bow_vector(instance, word_to_ix)).to('cuda:0') # wrap to tensor of torch type for instance\n",
        "        \n",
        "#         prob = net.forward(vec) # forward pass\n",
        "\n",
        "        prob = net.forward(instance)\n",
        "    \n",
        "        _class = 1 if prob.item() > 0.5 else 0 # sigmoid activated\n",
        "        \n",
        "        loss = loss_function(prob, label) # compute the loss\n",
        "        \n",
        "        sum_loss += float(loss.item())\n",
        "        \n",
        "#         if(int(label) == _class and _class == 1):\n",
        "#             true_positive += 1\n",
        "            \n",
        "#         if(int(label) == _class and _class == 0):\n",
        "#             true_negative += 1\n",
        "        \n",
        "#         if(_class == 1 and int(label) == 0):\n",
        "#             false_positive += 1\n",
        "            \n",
        "#         if(_class == 0 and int(label) == 1):\n",
        "#             false_negative += 1\n",
        "    \n",
        "    \n",
        "#     precision = float(true_positive) / (true_positive + false_positive)\n",
        "#     recall = float(true_positive) / (true_positive + false_negative)\n",
        "#     f_score = float(2)*precision*recall / (precision + recall)\n",
        "#     accuracy = float(1)*(true_positive+true_negative)/(true_positive+true_negative+false_positive+false_negative)\n",
        "    \n",
        "    return float(sum_loss)/len(data)#, float(100)*accuracy, precision, recall, f_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SaSwXEBh6565",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Task-1"
      ]
    },
    {
      "metadata": {
        "id": "aCYGwbP7zy8i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "task1 = [(50, 'Task1A.mdl'), (100, 'Task1B.mdl'), (200, 'Task1C.mdl'), (500, 'Task1D.mdl')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PM5I4GP6lFGu",
        "colab_type": "code",
        "outputId": "98fbc06c-5f74-4215-c04e-d0025ba109f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7738
        }
      },
      "cell_type": "code",
      "source": [
        "for num_of_hidden, task_name in task1:\n",
        "    class BOWClassifier(nn.Module):\n",
        "        def __init__(self, input_size, hidden_size, output_size):\n",
        "            super(BOWClassifier, self).__init__()\n",
        "            SEED = 42\n",
        "            torch.manual_seed(SEED)\n",
        "            torch.cuda.manual_seed(SEED)\n",
        "            self.hidden_size = hidden_size\n",
        "            self.i2h = nn.Linear(input_size, hidden_size) # initialises weights and biases i2h\n",
        "            self.h2o = nn.Linear(hidden_size, output_size) # initialises weights and biases h2o\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = torch.relu(self.i2h(x)) # relu activation @ hidden layer\n",
        "            x = torch.sigmoid(self.h2o(x)) # sigmoid activation @ output layer\n",
        "            return x\n",
        "\n",
        "    num_of_input = VOCAB_SIZE # Bag word restriction has to be equal to vocabulary\n",
        "    num_of_output = 1 # binary sentiment classes (+ve, -ve)\n",
        "\n",
        "    bow = BOWClassifier(num_of_input, num_of_hidden, num_of_output).to('cuda:0') # initialises weights and biases\n",
        "    \n",
        "    # define a loss function and an optimizer\n",
        "    loss_function = nn.BCELoss()\n",
        "    opt = torch.optim.SGD(bow.parameters(), lr = 0.0001)\n",
        "    \n",
        "    gc.collect()\n",
        "\n",
        "    # Train The Model\n",
        "    epochs = 100\n",
        "    # the training loop\n",
        "    total_time = 0.0\n",
        "    prev_val_loss = float('inf')\n",
        "    val_loss = 0\n",
        "    early_stop_bow = 0\n",
        "\n",
        "    for e in range(epochs):\n",
        "        tic = time.time() # start the timer\n",
        "        correct = 0\n",
        "        cumulative_loss = 0\n",
        "        incorrect = 0\n",
        "\n",
        "        for i, instance in enumerate(train_data): # train_text_reviews \n",
        "            # get the training data\n",
        "            label = train_text_labels[i] # get the label of the corresponding instace\n",
        "            label = Variable(torch.FloatTensor([label])).resize_((1,1)).to('cuda:0') # wraps a tensor for label\n",
        "\n",
        "            bow.zero_grad() # reset the gradient for each instance\n",
        "\n",
        "            pred = bow.forward(instance)\n",
        "\n",
        "            loss = loss_function(pred, label) # compute the loss\n",
        "            loss.backward() # backprop the loss\n",
        "            opt.step() # performs parameter updation based on the current gradient\n",
        "\n",
        "            cumulative_loss += float(loss.item()) # accumulate the loss over whole training sample\n",
        "\n",
        "            pred_class = 1 if pred.item() > 0.5 else 0 # as sigmoid activated\n",
        "            if(int(label) == pred_class): # counting correct prediction in each epoch\n",
        "                correct += 1\n",
        "\n",
        "        train_loss = float(cumulative_loss)/num_train_data\n",
        "        train_accuracy = correct*float(100)/num_train_data\n",
        "\n",
        "#         val_loss, val_accuracy, p, r, f = evaluate_validation_accuracy(val_data, bow) # test val-data-set on currently trained model\n",
        "        val_loss = evaluate_validation_accuracy(val_data, bow) # test val-data-set on currently trained model\n",
        "        \n",
        "        if(prev_val_loss-val_loss < 0.0): # early stop if nearly no change\n",
        "            bow = copy.deepcopy(early_stop_bow)\n",
        "            break\n",
        "\n",
        "        early_stop_bow = copy.deepcopy(bow)\n",
        "        \n",
        "        prev_val_loss = val_loss\n",
        "\n",
        "        toc = time.time() # final time\n",
        "        total_time += (toc-tic)\n",
        "\n",
        "        print(\"Epoch {}/{}\\n[On Training] ==> Time: {:.2f}s, Train Loss: {:.9f}, Train Accuracy: {:.2f}%\".format(e+1, epochs, (toc-tic), train_loss, train_accuracy))\n",
        "#         print(\"[On Validation] ==> Precision: {:.3f}, Recall: {:.3f}, F-Score: {:.3f}, Val loss: {:.9f}, Val Accuracy: {:.2f}% Total Time: {:.2f}s\".format(p, r, f, val_loss, val_accuracy, total_time))\n",
        "        print(\"[On Validation] ==> Val loss: {:.9f}, Total Time: {:.2f}s\".format(val_loss, total_time))\n",
        "    \n",
        "    torch.save(bow, task_name) ## Save the model\n",
        "    \n",
        "    print('--- AFTER TRAINING ---\\tModel: {}'.format(task_name))\n",
        "    correct = 0\n",
        "    tic = time.time()\n",
        "    for i, instance in enumerate(data_test):\n",
        "        label = test_text_labels[i] # get the label of the corresponding instace\n",
        "        label = Variable(torch.FloatTensor([label])).resize_((1,1)).to('cuda:0') # wraps a tensor for label\n",
        "\n",
        "        pred = bow.forward(instance)\n",
        "        pred_class = 1 if pred.item() > 0.5 else 0 # sigmoid activated\n",
        "\n",
        "        if(int(label) == pred_class):\n",
        "            correct += 1\n",
        "    toc = time.time()\n",
        "    print(\"[On Testing] Time: {}, Test Accuracy: {:.2f}%\".format((toc-tic), correct*100/len(data_test)))\n",
        "    \n",
        "    print ('\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "[On Training] ==> Time: 26.26s, Train Loss: 0.679907998, Train Accuracy: 63.10%\n",
            "[On Validation] ==> Val loss: 0.662730204, Total Time: 26.26s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 24.77s, Train Loss: 0.641387382, Train Accuracy: 73.63%\n",
            "[On Validation] ==> Val loss: 0.617228091, Total Time: 51.03s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 24.75s, Train Loss: 0.592229105, Train Accuracy: 77.21%\n",
            "[On Validation] ==> Val loss: 0.566256706, Total Time: 75.79s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 24.73s, Train Loss: 0.540704002, Train Accuracy: 79.55%\n",
            "[On Validation] ==> Val loss: 0.518085913, Total Time: 100.52s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 24.76s, Train Loss: 0.494395833, Train Accuracy: 81.34%\n",
            "[On Validation] ==> Val loss: 0.478558819, Total Time: 125.28s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 24.85s, Train Loss: 0.456540306, Train Accuracy: 82.70%\n",
            "[On Validation] ==> Val loss: 0.448150814, Total Time: 150.13s\n",
            "Epoch 7/100\n",
            "[On Training] ==> Time: 25.98s, Train Loss: 0.426290917, Train Accuracy: 83.73%\n",
            "[On Validation] ==> Val loss: 0.424913818, Total Time: 176.11s\n",
            "Epoch 8/100\n",
            "[On Training] ==> Time: 25.35s, Train Loss: 0.401651425, Train Accuracy: 84.65%\n",
            "[On Validation] ==> Val loss: 0.406790013, Total Time: 201.45s\n",
            "Epoch 9/100\n",
            "[On Training] ==> Time: 24.78s, Train Loss: 0.380927817, Train Accuracy: 85.49%\n",
            "[On Validation] ==> Val loss: 0.392261892, Total Time: 226.24s\n",
            "Epoch 10/100\n",
            "[On Training] ==> Time: 24.46s, Train Loss: 0.362975655, Train Accuracy: 86.16%\n",
            "[On Validation] ==> Val loss: 0.380333070, Total Time: 250.70s\n",
            "Epoch 11/100\n",
            "[On Training] ==> Time: 24.50s, Train Loss: 0.347090534, Train Accuracy: 86.79%\n",
            "[On Validation] ==> Val loss: 0.370358674, Total Time: 275.20s\n",
            "Epoch 12/100\n",
            "[On Training] ==> Time: 24.52s, Train Loss: 0.332822122, Train Accuracy: 87.35%\n",
            "[On Validation] ==> Val loss: 0.361919350, Total Time: 299.72s\n",
            "Epoch 13/100\n",
            "[On Training] ==> Time: 24.49s, Train Loss: 0.319888207, Train Accuracy: 87.89%\n",
            "[On Validation] ==> Val loss: 0.354717116, Total Time: 324.21s\n",
            "Epoch 14/100\n",
            "[On Training] ==> Time: 24.52s, Train Loss: 0.308087761, Train Accuracy: 88.52%\n",
            "[On Validation] ==> Val loss: 0.348540301, Total Time: 348.73s\n",
            "Epoch 15/100\n",
            "[On Training] ==> Time: 24.58s, Train Loss: 0.297267828, Train Accuracy: 89.08%\n",
            "[On Validation] ==> Val loss: 0.343225247, Total Time: 373.30s\n",
            "Epoch 16/100\n",
            "[On Training] ==> Time: 24.53s, Train Loss: 0.287311089, Train Accuracy: 89.44%\n",
            "[On Validation] ==> Val loss: 0.338642500, Total Time: 397.83s\n",
            "Epoch 17/100\n",
            "[On Training] ==> Time: 24.51s, Train Loss: 0.278111747, Train Accuracy: 89.93%\n",
            "[On Validation] ==> Val loss: 0.334681107, Total Time: 422.34s\n",
            "Epoch 18/100\n",
            "[On Training] ==> Time: 24.59s, Train Loss: 0.269582198, Train Accuracy: 90.31%\n",
            "[On Validation] ==> Val loss: 0.331260150, Total Time: 446.94s\n",
            "Epoch 19/100\n",
            "[On Training] ==> Time: 25.41s, Train Loss: 0.261652436, Train Accuracy: 90.72%\n",
            "[On Validation] ==> Val loss: 0.328295935, Total Time: 472.34s\n",
            "Epoch 20/100\n",
            "[On Training] ==> Time: 25.28s, Train Loss: 0.254250413, Train Accuracy: 90.96%\n",
            "[On Validation] ==> Val loss: 0.325721574, Total Time: 497.62s\n",
            "Epoch 21/100\n",
            "[On Training] ==> Time: 24.80s, Train Loss: 0.247318157, Train Accuracy: 91.21%\n",
            "[On Validation] ==> Val loss: 0.323483080, Total Time: 522.42s\n",
            "Epoch 22/100\n",
            "[On Training] ==> Time: 24.83s, Train Loss: 0.240806212, Train Accuracy: 91.53%\n",
            "[On Validation] ==> Val loss: 0.321533801, Total Time: 547.25s\n",
            "Epoch 23/100\n",
            "[On Training] ==> Time: 24.84s, Train Loss: 0.234665481, Train Accuracy: 91.88%\n",
            "[On Validation] ==> Val loss: 0.319840367, Total Time: 572.09s\n",
            "Epoch 24/100\n",
            "[On Training] ==> Time: 24.78s, Train Loss: 0.228858143, Train Accuracy: 92.17%\n",
            "[On Validation] ==> Val loss: 0.318368594, Total Time: 596.87s\n",
            "Epoch 25/100\n",
            "[On Training] ==> Time: 24.84s, Train Loss: 0.223350875, Train Accuracy: 92.37%\n",
            "[On Validation] ==> Val loss: 0.317095247, Total Time: 621.71s\n",
            "Epoch 26/100\n",
            "[On Training] ==> Time: 24.78s, Train Loss: 0.218115058, Train Accuracy: 92.60%\n",
            "[On Validation] ==> Val loss: 0.315999440, Total Time: 646.48s\n",
            "Epoch 27/100\n",
            "[On Training] ==> Time: 24.82s, Train Loss: 0.213125037, Train Accuracy: 92.85%\n",
            "[On Validation] ==> Val loss: 0.315061810, Total Time: 671.30s\n",
            "Epoch 28/100\n",
            "[On Training] ==> Time: 24.57s, Train Loss: 0.208358902, Train Accuracy: 93.07%\n",
            "[On Validation] ==> Val loss: 0.314268457, Total Time: 695.87s\n",
            "Epoch 29/100\n",
            "[On Training] ==> Time: 24.54s, Train Loss: 0.203796743, Train Accuracy: 93.24%\n",
            "[On Validation] ==> Val loss: 0.313605411, Total Time: 720.41s\n",
            "Epoch 30/100\n",
            "[On Training] ==> Time: 24.56s, Train Loss: 0.199421962, Train Accuracy: 93.40%\n",
            "[On Validation] ==> Val loss: 0.313060364, Total Time: 744.97s\n",
            "Epoch 31/100\n",
            "[On Training] ==> Time: 24.93s, Train Loss: 0.195219303, Train Accuracy: 93.57%\n",
            "[On Validation] ==> Val loss: 0.312624788, Total Time: 769.89s\n",
            "Epoch 32/100\n",
            "[On Training] ==> Time: 24.87s, Train Loss: 0.191175802, Train Accuracy: 93.68%\n",
            "[On Validation] ==> Val loss: 0.312290641, Total Time: 794.76s\n",
            "Epoch 33/100\n",
            "[On Training] ==> Time: 25.18s, Train Loss: 0.187279860, Train Accuracy: 93.84%\n",
            "[On Validation] ==> Val loss: 0.312049600, Total Time: 819.95s\n",
            "Epoch 34/100\n",
            "[On Training] ==> Time: 24.73s, Train Loss: 0.183520749, Train Accuracy: 94.00%\n",
            "[On Validation] ==> Val loss: 0.311894850, Total Time: 844.68s\n",
            "Epoch 35/100\n",
            "[On Training] ==> Time: 24.81s, Train Loss: 0.179889787, Train Accuracy: 94.10%\n",
            "[On Validation] ==> Val loss: 0.311818498, Total Time: 869.48s\n",
            "Epoch 36/100\n",
            "[On Training] ==> Time: 24.80s, Train Loss: 0.176378282, Train Accuracy: 94.26%\n",
            "[On Validation] ==> Val loss: 0.311816581, Total Time: 894.28s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type BOWClassifier. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--- AFTER TRAINING ---\tModel: Task1A.mdl\n",
            "[On Testing] Time: 2.378115653991699, Test Accuracy: 88.60%\n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "[On Training] ==> Time: 26.83s, Train Loss: 0.681274268, Train Accuracy: 64.59%\n",
            "[On Validation] ==> Val loss: 0.664423299, Total Time: 26.83s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 26.84s, Train Loss: 0.643418346, Train Accuracy: 74.10%\n",
            "[On Validation] ==> Val loss: 0.619357091, Total Time: 53.67s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 26.83s, Train Loss: 0.594328215, Train Accuracy: 77.30%\n",
            "[On Validation] ==> Val loss: 0.568000911, Total Time: 80.50s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 26.83s, Train Loss: 0.542414091, Train Accuracy: 79.57%\n",
            "[On Validation] ==> Val loss: 0.519250212, Total Time: 107.33s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 26.89s, Train Loss: 0.495759835, Train Accuracy: 81.33%\n",
            "[On Validation] ==> Val loss: 0.479240916, Total Time: 134.22s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 27.01s, Train Loss: 0.457674535, Train Accuracy: 82.60%\n",
            "[On Validation] ==> Val loss: 0.448449805, Total Time: 161.23s\n",
            "Epoch 7/100\n",
            "[On Training] ==> Time: 26.86s, Train Loss: 0.427273337, Train Accuracy: 83.83%\n",
            "[On Validation] ==> Val loss: 0.424916991, Total Time: 188.09s\n",
            "Epoch 8/100\n",
            "[On Training] ==> Time: 27.11s, Train Loss: 0.402521684, Train Accuracy: 84.76%\n",
            "[On Validation] ==> Val loss: 0.406574492, Total Time: 215.20s\n",
            "Epoch 9/100\n",
            "[On Training] ==> Time: 26.89s, Train Loss: 0.381698854, Train Accuracy: 85.62%\n",
            "[On Validation] ==> Val loss: 0.391906509, Total Time: 242.09s\n",
            "Epoch 10/100\n",
            "[On Training] ==> Time: 26.89s, Train Loss: 0.363655706, Train Accuracy: 86.25%\n",
            "[On Validation] ==> Val loss: 0.379893812, Total Time: 268.99s\n",
            "Epoch 11/100\n",
            "[On Training] ==> Time: 26.88s, Train Loss: 0.347681276, Train Accuracy: 86.82%\n",
            "[On Validation] ==> Val loss: 0.369874581, Total Time: 295.87s\n",
            "Epoch 12/100\n",
            "[On Training] ==> Time: 26.95s, Train Loss: 0.333332367, Train Accuracy: 87.41%\n",
            "[On Validation] ==> Val loss: 0.361403566, Total Time: 322.82s\n",
            "Epoch 13/100\n",
            "[On Training] ==> Time: 26.95s, Train Loss: 0.320322740, Train Accuracy: 87.92%\n",
            "[On Validation] ==> Val loss: 0.354179937, Total Time: 349.77s\n",
            "Epoch 14/100\n",
            "[On Training] ==> Time: 26.96s, Train Loss: 0.308455508, Train Accuracy: 88.49%\n",
            "[On Validation] ==> Val loss: 0.347998258, Total Time: 376.73s\n",
            "Epoch 15/100\n",
            "[On Training] ==> Time: 27.01s, Train Loss: 0.297581016, Train Accuracy: 89.09%\n",
            "[On Validation] ==> Val loss: 0.342689079, Total Time: 403.74s\n",
            "Epoch 16/100\n",
            "[On Training] ==> Time: 27.01s, Train Loss: 0.287578376, Train Accuracy: 89.54%\n",
            "[On Validation] ==> Val loss: 0.338117488, Total Time: 430.74s\n",
            "Epoch 17/100\n",
            "[On Training] ==> Time: 27.08s, Train Loss: 0.278343817, Train Accuracy: 89.87%\n",
            "[On Validation] ==> Val loss: 0.334177549, Total Time: 457.83s\n",
            "Epoch 18/100\n",
            "[On Training] ==> Time: 27.04s, Train Loss: 0.269788735, Train Accuracy: 90.26%\n",
            "[On Validation] ==> Val loss: 0.330776263, Total Time: 484.86s\n",
            "Epoch 19/100\n",
            "[On Training] ==> Time: 27.27s, Train Loss: 0.261836730, Train Accuracy: 90.64%\n",
            "[On Validation] ==> Val loss: 0.327832831, Total Time: 512.13s\n",
            "Epoch 20/100\n",
            "[On Training] ==> Time: 26.89s, Train Loss: 0.254416972, Train Accuracy: 91.00%\n",
            "[On Validation] ==> Val loss: 0.325280098, Total Time: 539.02s\n",
            "Epoch 21/100\n",
            "[On Training] ==> Time: 26.90s, Train Loss: 0.247469149, Train Accuracy: 91.30%\n",
            "[On Validation] ==> Val loss: 0.323067537, Total Time: 565.92s\n",
            "Epoch 22/100\n",
            "[On Training] ==> Time: 26.82s, Train Loss: 0.240942368, Train Accuracy: 91.58%\n",
            "[On Validation] ==> Val loss: 0.321144607, Total Time: 592.73s\n",
            "Epoch 23/100\n",
            "[On Training] ==> Time: 26.86s, Train Loss: 0.234790192, Train Accuracy: 91.86%\n",
            "[On Validation] ==> Val loss: 0.319471767, Total Time: 619.59s\n",
            "Epoch 24/100\n",
            "[On Training] ==> Time: 26.80s, Train Loss: 0.228972704, Train Accuracy: 92.14%\n",
            "[On Validation] ==> Val loss: 0.318019045, Total Time: 646.39s\n",
            "Epoch 25/100\n",
            "[On Training] ==> Time: 26.79s, Train Loss: 0.223456720, Train Accuracy: 92.37%\n",
            "[On Validation] ==> Val loss: 0.316761786, Total Time: 673.18s\n",
            "Epoch 26/100\n",
            "[On Training] ==> Time: 26.84s, Train Loss: 0.218212601, Train Accuracy: 92.60%\n",
            "[On Validation] ==> Val loss: 0.315677179, Total Time: 700.02s\n",
            "Epoch 27/100\n",
            "[On Training] ==> Time: 26.94s, Train Loss: 0.213214841, Train Accuracy: 92.83%\n",
            "[On Validation] ==> Val loss: 0.314748471, Total Time: 726.96s\n",
            "Epoch 28/100\n",
            "[On Training] ==> Time: 26.84s, Train Loss: 0.208441366, Train Accuracy: 93.02%\n",
            "[On Validation] ==> Val loss: 0.313958109, Total Time: 753.80s\n",
            "Epoch 29/100\n",
            "[On Training] ==> Time: 27.10s, Train Loss: 0.203872715, Train Accuracy: 93.15%\n",
            "[On Validation] ==> Val loss: 0.313295815, Total Time: 780.90s\n",
            "Epoch 30/100\n",
            "[On Training] ==> Time: 26.82s, Train Loss: 0.199491856, Train Accuracy: 93.36%\n",
            "[On Validation] ==> Val loss: 0.312748338, Total Time: 807.72s\n",
            "Epoch 31/100\n",
            "[On Training] ==> Time: 27.26s, Train Loss: 0.195283579, Train Accuracy: 93.54%\n",
            "[On Validation] ==> Val loss: 0.312305915, Total Time: 834.98s\n",
            "Epoch 32/100\n",
            "[On Training] ==> Time: 26.91s, Train Loss: 0.191234234, Train Accuracy: 93.70%\n",
            "[On Validation] ==> Val loss: 0.311960942, Total Time: 861.89s\n",
            "Epoch 33/100\n",
            "[On Training] ==> Time: 26.88s, Train Loss: 0.187332751, Train Accuracy: 93.84%\n",
            "[On Validation] ==> Val loss: 0.311707473, Total Time: 888.76s\n",
            "Epoch 34/100\n",
            "[On Training] ==> Time: 26.87s, Train Loss: 0.183568554, Train Accuracy: 93.93%\n",
            "[On Validation] ==> Val loss: 0.311538604, Total Time: 915.63s\n",
            "Epoch 35/100\n",
            "[On Training] ==> Time: 26.83s, Train Loss: 0.179932541, Train Accuracy: 94.07%\n",
            "[On Validation] ==> Val loss: 0.311449502, Total Time: 942.47s\n",
            "Epoch 36/100\n",
            "[On Training] ==> Time: 26.88s, Train Loss: 0.176416313, Train Accuracy: 94.18%\n",
            "[On Validation] ==> Val loss: 0.311433629, Total Time: 969.34s\n",
            "--- AFTER TRAINING ---\tModel: Task1B.mdl\n",
            "[On Testing] Time: 2.3514413833618164, Test Accuracy: 88.38%\n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "[On Training] ==> Time: 35.60s, Train Loss: 0.683804138, Train Accuracy: 54.30%\n",
            "[On Validation] ==> Val loss: 0.669674159, Total Time: 35.60s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 35.68s, Train Loss: 0.650442265, Train Accuracy: 72.05%\n",
            "[On Validation] ==> Val loss: 0.628048870, Total Time: 71.29s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 35.58s, Train Loss: 0.603416425, Train Accuracy: 76.88%\n",
            "[On Validation] ==> Val loss: 0.577331097, Total Time: 106.87s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 35.77s, Train Loss: 0.551410545, Train Accuracy: 79.27%\n",
            "[On Validation] ==> Val loss: 0.527667000, Total Time: 142.64s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 35.55s, Train Loss: 0.503686908, Train Accuracy: 80.89%\n",
            "[On Validation] ==> Val loss: 0.486265519, Total Time: 178.19s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 35.58s, Train Loss: 0.464445226, Train Accuracy: 82.21%\n",
            "[On Validation] ==> Val loss: 0.454230180, Total Time: 213.77s\n",
            "Epoch 7/100\n",
            "[On Training] ==> Time: 35.59s, Train Loss: 0.433162558, Train Accuracy: 83.49%\n",
            "[On Validation] ==> Val loss: 0.429746865, Total Time: 249.35s\n",
            "Epoch 8/100\n",
            "[On Training] ==> Time: 35.50s, Train Loss: 0.407798811, Train Accuracy: 84.45%\n",
            "[On Validation] ==> Val loss: 0.410698983, Total Time: 284.85s\n",
            "Epoch 9/100\n",
            "[On Training] ==> Time: 35.53s, Train Loss: 0.386548319, Train Accuracy: 85.20%\n",
            "[On Validation] ==> Val loss: 0.395492562, Total Time: 320.38s\n",
            "Epoch 10/100\n",
            "[On Training] ==> Time: 35.48s, Train Loss: 0.368186875, Train Accuracy: 85.92%\n",
            "[On Validation] ==> Val loss: 0.383056242, Total Time: 355.85s\n",
            "Epoch 11/100\n",
            "[On Training] ==> Time: 35.71s, Train Loss: 0.351950487, Train Accuracy: 86.57%\n",
            "[On Validation] ==> Val loss: 0.372684860, Total Time: 391.57s\n",
            "Epoch 12/100\n",
            "[On Training] ==> Time: 35.52s, Train Loss: 0.337367657, Train Accuracy: 87.20%\n",
            "[On Validation] ==> Val loss: 0.363915421, Total Time: 427.09s\n",
            "Epoch 13/100\n",
            "[On Training] ==> Time: 35.87s, Train Loss: 0.324141324, Train Accuracy: 87.76%\n",
            "[On Validation] ==> Val loss: 0.356437844, Total Time: 462.96s\n",
            "Epoch 14/100\n",
            "[On Training] ==> Time: 35.51s, Train Loss: 0.312068333, Train Accuracy: 88.29%\n",
            "[On Validation] ==> Val loss: 0.350026695, Total Time: 498.47s\n",
            "Epoch 15/100\n",
            "[On Training] ==> Time: 35.54s, Train Loss: 0.300997864, Train Accuracy: 88.86%\n",
            "[On Validation] ==> Val loss: 0.344512812, Total Time: 534.00s\n",
            "Epoch 16/100\n",
            "[On Training] ==> Time: 35.57s, Train Loss: 0.290811350, Train Accuracy: 89.38%\n",
            "[On Validation] ==> Val loss: 0.339760476, Total Time: 569.58s\n",
            "Epoch 17/100\n",
            "[On Training] ==> Time: 35.57s, Train Loss: 0.281406703, Train Accuracy: 89.77%\n",
            "[On Validation] ==> Val loss: 0.335659795, Total Time: 605.15s\n",
            "Epoch 18/100\n",
            "[On Training] ==> Time: 35.52s, Train Loss: 0.272694676, Train Accuracy: 90.14%\n",
            "[On Validation] ==> Val loss: 0.332113912, Total Time: 640.67s\n",
            "Epoch 19/100\n",
            "[On Training] ==> Time: 35.55s, Train Loss: 0.264596656, Train Accuracy: 90.53%\n",
            "[On Validation] ==> Val loss: 0.329039255, Total Time: 676.21s\n",
            "Epoch 20/100\n",
            "[On Training] ==> Time: 35.68s, Train Loss: 0.257041688, Train Accuracy: 90.89%\n",
            "[On Validation] ==> Val loss: 0.326370403, Total Time: 711.89s\n",
            "Epoch 21/100\n",
            "[On Training] ==> Time: 35.65s, Train Loss: 0.249969745, Train Accuracy: 91.20%\n",
            "[On Validation] ==> Val loss: 0.324047224, Total Time: 747.54s\n",
            "Epoch 22/100\n",
            "[On Training] ==> Time: 35.63s, Train Loss: 0.243327400, Train Accuracy: 91.45%\n",
            "[On Validation] ==> Val loss: 0.322025661, Total Time: 783.17s\n",
            "Epoch 23/100\n",
            "[On Training] ==> Time: 35.57s, Train Loss: 0.237068106, Train Accuracy: 91.65%\n",
            "[On Validation] ==> Val loss: 0.320263619, Total Time: 818.74s\n",
            "Epoch 24/100\n",
            "[On Training] ==> Time: 35.45s, Train Loss: 0.231152366, Train Accuracy: 92.00%\n",
            "[On Validation] ==> Val loss: 0.318728884, Total Time: 854.19s\n",
            "Epoch 25/100\n",
            "[On Training] ==> Time: 35.52s, Train Loss: 0.225545178, Train Accuracy: 92.23%\n",
            "[On Validation] ==> Val loss: 0.317396925, Total Time: 889.71s\n",
            "Epoch 26/100\n",
            "[On Training] ==> Time: 35.49s, Train Loss: 0.220217112, Train Accuracy: 92.49%\n",
            "[On Validation] ==> Val loss: 0.316245120, Total Time: 925.20s\n",
            "Epoch 27/100\n",
            "[On Training] ==> Time: 35.61s, Train Loss: 0.215141866, Train Accuracy: 92.77%\n",
            "[On Validation] ==> Val loss: 0.315255220, Total Time: 960.81s\n",
            "Epoch 28/100\n",
            "[On Training] ==> Time: 35.79s, Train Loss: 0.210296201, Train Accuracy: 92.96%\n",
            "[On Validation] ==> Val loss: 0.314409377, Total Time: 996.60s\n",
            "Epoch 29/100\n",
            "[On Training] ==> Time: 35.69s, Train Loss: 0.205660249, Train Accuracy: 93.16%\n",
            "[On Validation] ==> Val loss: 0.313695899, Total Time: 1032.29s\n",
            "Epoch 30/100\n",
            "[On Training] ==> Time: 35.89s, Train Loss: 0.201216467, Train Accuracy: 93.34%\n",
            "[On Validation] ==> Val loss: 0.313103224, Total Time: 1068.18s\n",
            "Epoch 31/100\n",
            "[On Training] ==> Time: 35.60s, Train Loss: 0.196949343, Train Accuracy: 93.43%\n",
            "[On Validation] ==> Val loss: 0.312620125, Total Time: 1103.78s\n",
            "Epoch 32/100\n",
            "[On Training] ==> Time: 35.58s, Train Loss: 0.192845252, Train Accuracy: 93.64%\n",
            "[On Validation] ==> Val loss: 0.312238425, Total Time: 1139.37s\n",
            "Epoch 33/100\n",
            "[On Training] ==> Time: 35.66s, Train Loss: 0.188891991, Train Accuracy: 93.79%\n",
            "[On Validation] ==> Val loss: 0.311950468, Total Time: 1175.03s\n",
            "Epoch 34/100\n",
            "[On Training] ==> Time: 35.54s, Train Loss: 0.185078775, Train Accuracy: 93.92%\n",
            "[On Validation] ==> Val loss: 0.311749529, Total Time: 1210.58s\n",
            "Epoch 35/100\n",
            "[On Training] ==> Time: 35.59s, Train Loss: 0.181396316, Train Accuracy: 94.08%\n",
            "[On Validation] ==> Val loss: 0.311630471, Total Time: 1246.16s\n",
            "Epoch 36/100\n",
            "[On Training] ==> Time: 35.59s, Train Loss: 0.177835974, Train Accuracy: 94.22%\n",
            "[On Validation] ==> Val loss: 0.311586912, Total Time: 1281.76s\n",
            "--- AFTER TRAINING ---\tModel: Task1C.mdl\n",
            "[On Testing] Time: 2.410961151123047, Test Accuracy: 88.34%\n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "[On Training] ==> Time: 64.51s, Train Loss: 0.683359743, Train Accuracy: 63.23%\n",
            "[On Validation] ==> Val loss: 0.671039734, Total Time: 64.51s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 64.34s, Train Loss: 0.654407062, Train Accuracy: 72.26%\n",
            "[On Validation] ==> Val loss: 0.634157508, Total Time: 128.84s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 64.29s, Train Loss: 0.611103557, Train Accuracy: 75.84%\n",
            "[On Validation] ==> Val loss: 0.585556224, Total Time: 193.13s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 64.34s, Train Loss: 0.560084138, Train Accuracy: 78.40%\n",
            "[On Validation] ==> Val loss: 0.535529579, Total Time: 257.47s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 64.42s, Train Loss: 0.511505365, Train Accuracy: 80.31%\n",
            "[On Validation] ==> Val loss: 0.492687842, Total Time: 321.89s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 64.50s, Train Loss: 0.470868496, Train Accuracy: 81.85%\n",
            "[On Validation] ==> Val loss: 0.459190496, Total Time: 386.39s\n",
            "Epoch 7/100\n",
            "[On Training] ==> Time: 64.34s, Train Loss: 0.438346377, Train Accuracy: 83.13%\n",
            "[On Validation] ==> Val loss: 0.433569430, Total Time: 450.72s\n",
            "Epoch 8/100\n",
            "[On Training] ==> Time: 64.33s, Train Loss: 0.412065749, Train Accuracy: 84.22%\n",
            "[On Validation] ==> Val loss: 0.413708716, Total Time: 515.05s\n",
            "Epoch 9/100\n",
            "[On Training] ==> Time: 64.32s, Train Loss: 0.390158251, Train Accuracy: 85.15%\n",
            "[On Validation] ==> Val loss: 0.397906499, Total Time: 579.38s\n",
            "Epoch 10/100\n",
            "[On Training] ==> Time: 64.41s, Train Loss: 0.371308439, Train Accuracy: 85.86%\n",
            "[On Validation] ==> Val loss: 0.385016412, Total Time: 643.78s\n",
            "Epoch 11/100\n",
            "[On Training] ==> Time: 64.43s, Train Loss: 0.354686549, Train Accuracy: 86.43%\n",
            "[On Validation] ==> Val loss: 0.374284911, Total Time: 708.21s\n",
            "Epoch 12/100\n",
            "[On Training] ==> Time: 64.34s, Train Loss: 0.339786693, Train Accuracy: 87.07%\n",
            "[On Validation] ==> Val loss: 0.365222878, Total Time: 772.55s\n",
            "Epoch 13/100\n",
            "[On Training] ==> Time: 64.32s, Train Loss: 0.326290288, Train Accuracy: 87.64%\n",
            "[On Validation] ==> Val loss: 0.357499909, Total Time: 836.87s\n",
            "Epoch 14/100\n",
            "[On Training] ==> Time: 64.41s, Train Loss: 0.313985446, Train Accuracy: 88.22%\n",
            "[On Validation] ==> Val loss: 0.350878173, Total Time: 901.28s\n",
            "Epoch 15/100\n",
            "[On Training] ==> Time: 64.37s, Train Loss: 0.302712641, Train Accuracy: 88.71%\n",
            "[On Validation] ==> Val loss: 0.345180658, Total Time: 965.66s\n",
            "Epoch 16/100\n",
            "[On Training] ==> Time: 64.48s, Train Loss: 0.292346344, Train Accuracy: 89.21%\n",
            "[On Validation] ==> Val loss: 0.340267534, Total Time: 1030.13s\n",
            "Epoch 17/100\n",
            "[On Training] ==> Time: 64.35s, Train Loss: 0.282784526, Train Accuracy: 89.79%\n",
            "[On Validation] ==> Val loss: 0.336021856, Total Time: 1094.48s\n",
            "Epoch 18/100\n",
            "[On Training] ==> Time: 64.35s, Train Loss: 0.273933555, Train Accuracy: 90.18%\n",
            "[On Validation] ==> Val loss: 0.332347399, Total Time: 1158.83s\n",
            "Epoch 19/100\n",
            "[On Training] ==> Time: 64.41s, Train Loss: 0.265712813, Train Accuracy: 90.52%\n",
            "[On Validation] ==> Val loss: 0.329162111, Total Time: 1223.23s\n",
            "Epoch 20/100\n",
            "[On Training] ==> Time: 64.36s, Train Loss: 0.258051296, Train Accuracy: 90.95%\n",
            "[On Validation] ==> Val loss: 0.326396650, Total Time: 1287.59s\n",
            "Epoch 21/100\n",
            "[On Training] ==> Time: 64.46s, Train Loss: 0.250885461, Train Accuracy: 91.23%\n",
            "[On Validation] ==> Val loss: 0.323989860, Total Time: 1352.05s\n",
            "Epoch 22/100\n",
            "[On Training] ==> Time: 64.34s, Train Loss: 0.244161421, Train Accuracy: 91.47%\n",
            "[On Validation] ==> Val loss: 0.321891829, Total Time: 1416.39s\n",
            "Epoch 23/100\n",
            "[On Training] ==> Time: 64.31s, Train Loss: 0.237831055, Train Accuracy: 91.68%\n",
            "[On Validation] ==> Val loss: 0.320062278, Total Time: 1480.70s\n",
            "Epoch 24/100\n",
            "[On Training] ==> Time: 64.45s, Train Loss: 0.231852781, Train Accuracy: 91.97%\n",
            "[On Validation] ==> Val loss: 0.318465646, Total Time: 1545.15s\n",
            "Epoch 25/100\n",
            "[On Training] ==> Time: 64.48s, Train Loss: 0.226189982, Train Accuracy: 92.25%\n",
            "[On Validation] ==> Val loss: 0.317076909, Total Time: 1609.63s\n",
            "Epoch 26/100\n",
            "[On Training] ==> Time: 64.39s, Train Loss: 0.220812041, Train Accuracy: 92.52%\n",
            "[On Validation] ==> Val loss: 0.315870237, Total Time: 1674.02s\n",
            "Epoch 27/100\n",
            "[On Training] ==> Time: 64.34s, Train Loss: 0.215691490, Train Accuracy: 92.72%\n",
            "[On Validation] ==> Val loss: 0.314827446, Total Time: 1738.36s\n",
            "Epoch 28/100\n",
            "[On Training] ==> Time: 64.39s, Train Loss: 0.210804493, Train Accuracy: 92.94%\n",
            "[On Validation] ==> Val loss: 0.313932165, Total Time: 1802.76s\n",
            "Epoch 29/100\n",
            "[On Training] ==> Time: 64.40s, Train Loss: 0.206129953, Train Accuracy: 93.07%\n",
            "[On Validation] ==> Val loss: 0.313168766, Total Time: 1867.15s\n",
            "Epoch 30/100\n",
            "[On Training] ==> Time: 64.51s, Train Loss: 0.201650148, Train Accuracy: 93.25%\n",
            "[On Validation] ==> Val loss: 0.312527690, Total Time: 1931.66s\n",
            "Epoch 31/100\n",
            "[On Training] ==> Time: 64.33s, Train Loss: 0.197349040, Train Accuracy: 93.48%\n",
            "[On Validation] ==> Val loss: 0.311998681, Total Time: 1995.99s\n",
            "Epoch 32/100\n",
            "[On Training] ==> Time: 64.39s, Train Loss: 0.193212711, Train Accuracy: 93.64%\n",
            "[On Validation] ==> Val loss: 0.311571958, Total Time: 2060.38s\n",
            "Epoch 33/100\n",
            "[On Training] ==> Time: 64.40s, Train Loss: 0.189228875, Train Accuracy: 93.80%\n",
            "[On Validation] ==> Val loss: 0.311240252, Total Time: 2124.78s\n",
            "Epoch 34/100\n",
            "[On Training] ==> Time: 64.35s, Train Loss: 0.185386971, Train Accuracy: 93.97%\n",
            "[On Validation] ==> Val loss: 0.310998909, Total Time: 2189.13s\n",
            "Epoch 35/100\n",
            "[On Training] ==> Time: 64.45s, Train Loss: 0.181676852, Train Accuracy: 94.10%\n",
            "[On Validation] ==> Val loss: 0.310839692, Total Time: 2253.58s\n",
            "Epoch 36/100\n",
            "[On Training] ==> Time: 64.33s, Train Loss: 0.178090023, Train Accuracy: 94.18%\n",
            "[On Validation] ==> Val loss: 0.310758296, Total Time: 2317.91s\n",
            "Epoch 37/100\n",
            "[On Training] ==> Time: 64.35s, Train Loss: 0.174618355, Train Accuracy: 94.34%\n",
            "[On Validation] ==> Val loss: 0.310748634, Total Time: 2382.25s\n",
            "--- AFTER TRAINING ---\tModel: Task1D.mdl\n",
            "[On Testing] Time: 3.031783103942871, Test Accuracy: 88.54%\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4XoHF1M8IVdf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download('Task1A.mdl')\n",
        "files.download('Task1B.mdl')\n",
        "files.download('Task1C.mdl')\n",
        "files.download('Task1D.mdl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FxO_e5P1mX3I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Task-2"
      ]
    },
    {
      "metadata": {
        "id": "lpUB7Obs197T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "task2 = [(10, 10, 'Task2A.mdl'), (20, 10, 'Task2B.mdl'), (30, 30, 'Task2C.mdl'), (50, 50, 'Task2D.mdl'), (100, 50, 'Task2E.mdl')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UiqcS_4XmXbd",
        "colab_type": "code",
        "outputId": "88b97af2-7186-4de2-d063-34b0e68b6fa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5562
        }
      },
      "cell_type": "code",
      "source": [
        "for num_of_hidden1, num_of_hidden2, task_name in task2:\n",
        "    class BOWClassifier(nn.Module):\n",
        "        def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
        "            super(BOWClassifier, self).__init__()\n",
        "            SEED = 42\n",
        "            torch.manual_seed(SEED)\n",
        "            torch.cuda.manual_seed(SEED)\n",
        "            self.i2h = nn.Linear(input_size, hidden_size1) # initialises weights and biases i2h\n",
        "            self.h2h = nn.Linear(hidden_size1, hidden_size2) # initialises weights and biases h2h\n",
        "            self.h2o = nn.Linear(hidden_size2, output_size) # initialises weights and biases h2o\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = torch.relu(self.i2h(x)) # relu activation @ hidden layer\n",
        "            x = torch.relu(self.h2h(x)) # relu activation @ hidden layer\n",
        "            x = torch.sigmoid(self.h2o(x)) # sigmoid activation @ output layer\n",
        "            return x\n",
        "\n",
        "    num_of_input = VOCAB_SIZE # Bag word restriction has to be equal to vocabulary\n",
        "    num_of_output = 1 # binary sentiment classes (+ve, -ve)\n",
        "\n",
        "    bow = BOWClassifier(num_of_input, num_of_hidden1, num_of_hidden2, num_of_output).to('cuda:0') # initialises weights and biases\n",
        "\n",
        "    # define a loss function and an optimizer\n",
        "    loss_function = nn.BCELoss()\n",
        "    opt = torch.optim.SGD(bow.parameters(), lr = 0.0001)\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    # Train The Model\n",
        "    epochs = 100\n",
        "    # the training loop\n",
        "    total_time = 0.0\n",
        "    prev_val_loss = float('inf')\n",
        "    val_loss = 0\n",
        "    early_stop_bow = 0\n",
        "\n",
        "    for e in range(epochs):\n",
        "        tic = time.time() # start the timer\n",
        "        correct = 0\n",
        "        cumulative_loss = 0\n",
        "        incorrect = 0\n",
        "\n",
        "        for i, instance in enumerate(train_data): # train_text_reviews \n",
        "            # get the training data\n",
        "            label = train_text_labels[i] # get the label of the corresponding instace\n",
        "            label = Variable(torch.FloatTensor([label])).resize_((1,1)).to('cuda:0') # wraps a tensor for label\n",
        "\n",
        "            bow.zero_grad() # reset the gradient for each instance\n",
        "\n",
        "            pred = bow.forward(instance)\n",
        "\n",
        "            loss = loss_function(pred, label) # compute the loss\n",
        "            loss.backward() # backprop the loss\n",
        "            opt.step() # performs parameter updation based on the current gradient\n",
        "\n",
        "            cumulative_loss += float(loss.item()) # accumulate the loss over whole training sample\n",
        "\n",
        "            pred_class = 1 if pred.item() > 0.5 else 0 # as sigmoid activated\n",
        "            if(int(label) == pred_class): # counting correct prediction in each epoch\n",
        "                correct += 1\n",
        "\n",
        "        train_loss = float(cumulative_loss)/num_train_data\n",
        "        train_accuracy = correct*float(100)/num_train_data\n",
        "\n",
        "#         val_loss, val_accuracy, p, r, f = evaluate_validation_accuracy(val_data, bow) # test val-data-set on currently trained model\n",
        "        val_loss = evaluate_validation_accuracy(val_data, bow) # test val-data-set on currently trained model\n",
        "        \n",
        "        if(prev_val_loss-val_loss < 0.0): # early stop if nearly no change\n",
        "            bow = copy.deepcopy(early_stop_bow)\n",
        "            break\n",
        "\n",
        "        early_stop_bow = copy.deepcopy(bow)\n",
        "\n",
        "        prev_val_loss = val_loss\n",
        "\n",
        "        toc = time.time() # final time\n",
        "        total_time += (toc-tic)\n",
        "\n",
        "        print(\"Epoch {}/{}\\n[On Training] ==> Time: {:.2f}s, Train Loss: {:.9f}, Train Accuracy: {:.2f}%\".format(e+1, epochs, (toc-tic), train_loss, train_accuracy))\n",
        "#         print(\"[On Validation] ==> Precision: {:.3f}, Recall: {:.3f}, F-Score: {:.3f}, Val loss: {:.9f}, Val Accuracy: {:.2f}% Total Time: {:.2f}s\".format(p, r, f, val_loss, val_accuracy, total_time))\n",
        "        print(\"[On Validation] ==> Val loss: {:.9f}, Total Time: {:.2f}s\".format(val_loss, total_time))\n",
        "\n",
        "    torch.save(bow, task_name)\n",
        "\n",
        "    print('--- AFTER TRAINING ---\\tModel: {}'.format(task_name))\n",
        "    correct = 0\n",
        "    tic = time.time()\n",
        "    for i, instance in enumerate(data_test):\n",
        "        label = test_text_labels[i] # get the label of the corresponding instace\n",
        "        label = Variable(torch.FloatTensor([label])).resize_((1,1)).to('cuda:0') # wraps a tensor for label\n",
        "\n",
        "        pred = bow.forward(instance)\n",
        "        pred_class = 1 if pred.item() > 0.5 else 0 # sigmoid activated\n",
        "\n",
        "        if(int(label) == pred_class):\n",
        "            correct += 1\n",
        "    toc = time.time()\n",
        "    print(\"[On Testing] Time: {}, Test Accuracy: {:.2f}%\".format((toc-tic), correct*100/len(data_test)))\n",
        "    \n",
        "    print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "[On Training] ==> Time: 26.33s, Train Loss: 0.696559569, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.692734756, Total Time: 26.33s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 26.25s, Train Loss: 0.688986655, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.684673949, Total Time: 52.58s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 26.17s, Train Loss: 0.678799666, Train Accuracy: 52.53%\n",
            "[On Validation] ==> Val loss: 0.671896277, Total Time: 78.75s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 26.20s, Train Loss: 0.662443024, Train Accuracy: 68.51%\n",
            "[On Validation] ==> Val loss: 0.651179718, Total Time: 104.95s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 26.07s, Train Loss: 0.635187637, Train Accuracy: 76.31%\n",
            "[On Validation] ==> Val loss: 0.615992655, Total Time: 131.02s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 26.16s, Train Loss: 0.590748855, Train Accuracy: 79.36%\n",
            "[On Validation] ==> Val loss: 0.562680017, Total Time: 157.18s\n",
            "Epoch 7/100\n",
            "[On Training] ==> Time: 26.16s, Train Loss: 0.530256079, Train Accuracy: 81.52%\n",
            "[On Validation] ==> Val loss: 0.500231635, Total Time: 183.34s\n",
            "Epoch 8/100\n",
            "[On Training] ==> Time: 26.57s, Train Loss: 0.467574091, Train Accuracy: 83.07%\n",
            "[On Validation] ==> Val loss: 0.446350091, Total Time: 209.91s\n",
            "Epoch 9/100\n",
            "[On Training] ==> Time: 26.18s, Train Loss: 0.415966298, Train Accuracy: 84.54%\n",
            "[On Validation] ==> Val loss: 0.408045043, Total Time: 236.09s\n",
            "Epoch 10/100\n",
            "[On Training] ==> Time: 26.02s, Train Loss: 0.376331809, Train Accuracy: 85.73%\n",
            "[On Validation] ==> Val loss: 0.381465853, Total Time: 262.11s\n",
            "Epoch 11/100\n",
            "[On Training] ==> Time: 26.17s, Train Loss: 0.344469143, Train Accuracy: 87.00%\n",
            "[On Validation] ==> Val loss: 0.362372456, Total Time: 288.28s\n",
            "Epoch 12/100\n",
            "[On Training] ==> Time: 26.68s, Train Loss: 0.317480172, Train Accuracy: 88.07%\n",
            "[On Validation] ==> Val loss: 0.348326876, Total Time: 314.97s\n",
            "Epoch 13/100\n",
            "[On Training] ==> Time: 26.23s, Train Loss: 0.294190155, Train Accuracy: 88.90%\n",
            "[On Validation] ==> Val loss: 0.337834259, Total Time: 341.20s\n",
            "Epoch 14/100\n",
            "[On Training] ==> Time: 26.11s, Train Loss: 0.273973762, Train Accuracy: 89.94%\n",
            "[On Validation] ==> Val loss: 0.330152716, Total Time: 367.32s\n",
            "Epoch 15/100\n",
            "[On Training] ==> Time: 26.10s, Train Loss: 0.256272020, Train Accuracy: 90.71%\n",
            "[On Validation] ==> Val loss: 0.324559724, Total Time: 393.41s\n",
            "Epoch 16/100\n",
            "[On Training] ==> Time: 26.18s, Train Loss: 0.240544853, Train Accuracy: 91.41%\n",
            "[On Validation] ==> Val loss: 0.320465224, Total Time: 419.59s\n",
            "Epoch 17/100\n",
            "[On Training] ==> Time: 26.20s, Train Loss: 0.226455850, Train Accuracy: 92.06%\n",
            "[On Validation] ==> Val loss: 0.317692915, Total Time: 445.79s\n",
            "Epoch 18/100\n",
            "[On Training] ==> Time: 26.12s, Train Loss: 0.213749210, Train Accuracy: 92.65%\n",
            "[On Validation] ==> Val loss: 0.315951909, Total Time: 471.91s\n",
            "Epoch 19/100\n",
            "[On Training] ==> Time: 26.43s, Train Loss: 0.202168438, Train Accuracy: 93.14%\n",
            "[On Validation] ==> Val loss: 0.314992706, Total Time: 498.35s\n",
            "Epoch 20/100\n",
            "[On Training] ==> Time: 26.09s, Train Loss: 0.191495954, Train Accuracy: 93.54%\n",
            "[On Validation] ==> Val loss: 0.314753440, Total Time: 524.44s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type BOWClassifier. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--- AFTER TRAINING ---\tModel: Task2A.mdl\n",
            "[On Testing] Time: 2.347750663757324, Test Accuracy: 88.34%\n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "[On Training] ==> Time: 26.53s, Train Loss: 0.697163920, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.692895690, Total Time: 26.53s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 25.94s, Train Loss: 0.688107316, Train Accuracy: 50.91%\n",
            "[On Validation] ==> Val loss: 0.682609272, Total Time: 52.47s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 26.39s, Train Loss: 0.676529645, Train Accuracy: 64.41%\n",
            "[On Validation] ==> Val loss: 0.669844118, Total Time: 78.86s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 26.25s, Train Loss: 0.661661082, Train Accuracy: 76.71%\n",
            "[On Validation] ==> Val loss: 0.652425888, Total Time: 105.11s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 25.98s, Train Loss: 0.640017168, Train Accuracy: 77.10%\n",
            "[On Validation] ==> Val loss: 0.625674039, Total Time: 131.10s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 26.47s, Train Loss: 0.601781989, Train Accuracy: 78.69%\n",
            "[On Validation] ==> Val loss: 0.577262559, Total Time: 157.56s\n",
            "Epoch 7/100\n",
            "[On Training] ==> Time: 26.03s, Train Loss: 0.546384485, Train Accuracy: 80.58%\n",
            "[On Validation] ==> Val loss: 0.519811252, Total Time: 183.59s\n",
            "Epoch 8/100\n",
            "[On Training] ==> Time: 26.58s, Train Loss: 0.486560819, Train Accuracy: 82.32%\n",
            "[On Validation] ==> Val loss: 0.464902663, Total Time: 210.17s\n",
            "Epoch 9/100\n",
            "[On Training] ==> Time: 26.17s, Train Loss: 0.433503584, Train Accuracy: 83.89%\n",
            "[On Validation] ==> Val loss: 0.422291366, Total Time: 236.34s\n",
            "Epoch 10/100\n",
            "[On Training] ==> Time: 26.26s, Train Loss: 0.391265949, Train Accuracy: 85.36%\n",
            "[On Validation] ==> Val loss: 0.392009945, Total Time: 262.60s\n",
            "Epoch 11/100\n",
            "[On Training] ==> Time: 26.15s, Train Loss: 0.357546118, Train Accuracy: 86.60%\n",
            "[On Validation] ==> Val loss: 0.370425927, Total Time: 288.75s\n",
            "Epoch 12/100\n",
            "[On Training] ==> Time: 25.77s, Train Loss: 0.329428360, Train Accuracy: 87.58%\n",
            "[On Validation] ==> Val loss: 0.354632487, Total Time: 314.53s\n",
            "Epoch 13/100\n",
            "[On Training] ==> Time: 26.18s, Train Loss: 0.305203011, Train Accuracy: 88.58%\n",
            "[On Validation] ==> Val loss: 0.342827499, Total Time: 340.70s\n",
            "Epoch 14/100\n",
            "[On Training] ==> Time: 25.79s, Train Loss: 0.284077425, Train Accuracy: 89.50%\n",
            "[On Validation] ==> Val loss: 0.334102217, Total Time: 366.50s\n",
            "Epoch 15/100\n",
            "[On Training] ==> Time: 26.96s, Train Loss: 0.265527118, Train Accuracy: 90.41%\n",
            "[On Validation] ==> Val loss: 0.327661403, Total Time: 393.45s\n",
            "Epoch 16/100\n",
            "[On Training] ==> Time: 25.97s, Train Loss: 0.249111157, Train Accuracy: 90.94%\n",
            "[On Validation] ==> Val loss: 0.323045344, Total Time: 419.43s\n",
            "Epoch 17/100\n",
            "[On Training] ==> Time: 26.63s, Train Loss: 0.234486982, Train Accuracy: 91.62%\n",
            "[On Validation] ==> Val loss: 0.319749271, Total Time: 446.06s\n",
            "Epoch 18/100\n",
            "[On Training] ==> Time: 26.14s, Train Loss: 0.221310894, Train Accuracy: 92.26%\n",
            "[On Validation] ==> Val loss: 0.317477872, Total Time: 472.20s\n",
            "Epoch 19/100\n",
            "[On Training] ==> Time: 26.16s, Train Loss: 0.209322367, Train Accuracy: 92.80%\n",
            "[On Validation] ==> Val loss: 0.316164460, Total Time: 498.36s\n",
            "Epoch 20/100\n",
            "[On Training] ==> Time: 26.27s, Train Loss: 0.198323401, Train Accuracy: 93.28%\n",
            "[On Validation] ==> Val loss: 0.315550853, Total Time: 524.63s\n",
            "--- AFTER TRAINING ---\tModel: Task2B.mdl\n",
            "[On Testing] Time: 2.4323575496673584, Test Accuracy: 88.44%\n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "[On Training] ==> Time: 26.34s, Train Loss: 0.692641562, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.689663194, Total Time: 26.34s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 26.25s, Train Loss: 0.686562054, Train Accuracy: 51.36%\n",
            "[On Validation] ==> Val loss: 0.682685288, Total Time: 52.59s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 27.23s, Train Loss: 0.678325810, Train Accuracy: 65.20%\n",
            "[On Validation] ==> Val loss: 0.672515627, Total Time: 79.82s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 26.49s, Train Loss: 0.665160870, Train Accuracy: 74.65%\n",
            "[On Validation] ==> Val loss: 0.655358400, Total Time: 106.31s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 26.41s, Train Loss: 0.642194177, Train Accuracy: 76.92%\n",
            "[On Validation] ==> Val loss: 0.625314525, Total Time: 132.72s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 27.06s, Train Loss: 0.603146976, Train Accuracy: 78.68%\n",
            "[On Validation] ==> Val loss: 0.576870178, Total Time: 159.78s\n",
            "Epoch 7/100\n",
            "[On Training] ==> Time: 26.16s, Train Loss: 0.546162732, Train Accuracy: 80.59%\n",
            "[On Validation] ==> Val loss: 0.515284227, Total Time: 185.94s\n",
            "Epoch 8/100\n",
            "[On Training] ==> Time: 27.08s, Train Loss: 0.483548245, Train Accuracy: 82.19%\n",
            "[On Validation] ==> Val loss: 0.459261382, Total Time: 213.02s\n",
            "Epoch 9/100\n",
            "[On Training] ==> Time: 26.34s, Train Loss: 0.430476484, Train Accuracy: 83.89%\n",
            "[On Validation] ==> Val loss: 0.418227725, Total Time: 239.36s\n",
            "Epoch 10/100\n",
            "[On Training] ==> Time: 26.15s, Train Loss: 0.389542028, Train Accuracy: 85.29%\n",
            "[On Validation] ==> Val loss: 0.389622834, Total Time: 265.52s\n",
            "Epoch 11/100\n",
            "[On Training] ==> Time: 26.51s, Train Loss: 0.356829128, Train Accuracy: 86.42%\n",
            "[On Validation] ==> Val loss: 0.369043231, Total Time: 292.03s\n",
            "Epoch 12/100\n",
            "[On Training] ==> Time: 26.53s, Train Loss: 0.329182746, Train Accuracy: 87.47%\n",
            "[On Validation] ==> Val loss: 0.353616677, Total Time: 318.56s\n",
            "Epoch 13/100\n",
            "[On Training] ==> Time: 26.97s, Train Loss: 0.305112053, Train Accuracy: 88.50%\n",
            "[On Validation] ==> Val loss: 0.341954520, Total Time: 345.54s\n",
            "Epoch 14/100\n",
            "[On Training] ==> Time: 26.30s, Train Loss: 0.284053463, Train Accuracy: 89.49%\n",
            "[On Validation] ==> Val loss: 0.333252531, Total Time: 371.83s\n",
            "Epoch 15/100\n",
            "[On Training] ==> Time: 27.29s, Train Loss: 0.265568422, Train Accuracy: 90.30%\n",
            "[On Validation] ==> Val loss: 0.326920024, Total Time: 399.12s\n",
            "Epoch 16/100\n",
            "[On Training] ==> Time: 26.33s, Train Loss: 0.249200857, Train Accuracy: 90.94%\n",
            "[On Validation] ==> Val loss: 0.322201603, Total Time: 425.45s\n",
            "Epoch 17/100\n",
            "[On Training] ==> Time: 26.93s, Train Loss: 0.234573691, Train Accuracy: 91.59%\n",
            "[On Validation] ==> Val loss: 0.318848902, Total Time: 452.38s\n",
            "Epoch 18/100\n",
            "[On Training] ==> Time: 26.75s, Train Loss: 0.221388907, Train Accuracy: 92.20%\n",
            "[On Validation] ==> Val loss: 0.316574565, Total Time: 479.13s\n",
            "Epoch 19/100\n",
            "[On Training] ==> Time: 26.37s, Train Loss: 0.209399221, Train Accuracy: 92.76%\n",
            "[On Validation] ==> Val loss: 0.315205962, Total Time: 505.51s\n",
            "Epoch 20/100\n",
            "[On Training] ==> Time: 27.13s, Train Loss: 0.198391803, Train Accuracy: 93.28%\n",
            "[On Validation] ==> Val loss: 0.314568482, Total Time: 532.63s\n",
            "--- AFTER TRAINING ---\tModel: Task2C.mdl\n",
            "[On Testing] Time: 2.4894840717315674, Test Accuracy: 88.32%\n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "[On Training] ==> Time: 25.51s, Train Loss: 0.692143227, Train Accuracy: 56.16%\n",
            "[On Validation] ==> Val loss: 0.690746614, Total Time: 25.51s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 26.50s, Train Loss: 0.688463344, Train Accuracy: 66.11%\n",
            "[On Validation] ==> Val loss: 0.685180905, Total Time: 52.01s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 27.62s, Train Loss: 0.680227176, Train Accuracy: 70.61%\n",
            "[On Validation] ==> Val loss: 0.673760463, Total Time: 79.63s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 27.92s, Train Loss: 0.664574093, Train Accuracy: 73.45%\n",
            "[On Validation] ==> Val loss: 0.652760673, Total Time: 107.54s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 27.56s, Train Loss: 0.636072845, Train Accuracy: 76.21%\n",
            "[On Validation] ==> Val loss: 0.615596404, Total Time: 135.11s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 28.22s, Train Loss: 0.589107276, Train Accuracy: 78.41%\n",
            "[On Validation] ==> Val loss: 0.560019676, Total Time: 163.33s\n",
            "Epoch 7/100\n",
            "[On Training] ==> Time: 27.90s, Train Loss: 0.527463273, Train Accuracy: 80.65%\n",
            "[On Validation] ==> Val loss: 0.498149896, Total Time: 191.24s\n",
            "Epoch 8/100\n",
            "[On Training] ==> Time: 27.57s, Train Loss: 0.466663279, Train Accuracy: 82.41%\n",
            "[On Validation] ==> Val loss: 0.446684667, Total Time: 218.80s\n",
            "Epoch 9/100\n",
            "[On Training] ==> Time: 27.93s, Train Loss: 0.417511107, Train Accuracy: 84.23%\n",
            "[On Validation] ==> Val loss: 0.409951717, Total Time: 246.73s\n",
            "Epoch 10/100\n",
            "[On Training] ==> Time: 27.25s, Train Loss: 0.379392849, Train Accuracy: 85.52%\n",
            "[On Validation] ==> Val loss: 0.384150042, Total Time: 273.98s\n",
            "Epoch 11/100\n",
            "[On Training] ==> Time: 28.35s, Train Loss: 0.348374894, Train Accuracy: 86.68%\n",
            "[On Validation] ==> Val loss: 0.365266136, Total Time: 302.33s\n",
            "Epoch 12/100\n",
            "[On Training] ==> Time: 27.55s, Train Loss: 0.321898284, Train Accuracy: 87.68%\n",
            "[On Validation] ==> Val loss: 0.351011727, Total Time: 329.88s\n",
            "Epoch 13/100\n",
            "[On Training] ==> Time: 28.68s, Train Loss: 0.298810387, Train Accuracy: 88.75%\n",
            "[On Validation] ==> Val loss: 0.340234403, Total Time: 358.56s\n",
            "Epoch 14/100\n",
            "[On Training] ==> Time: 27.30s, Train Loss: 0.278583337, Train Accuracy: 89.55%\n",
            "[On Validation] ==> Val loss: 0.332156581, Total Time: 385.86s\n",
            "Epoch 15/100\n",
            "[On Training] ==> Time: 27.90s, Train Loss: 0.260772272, Train Accuracy: 90.46%\n",
            "[On Validation] ==> Val loss: 0.326116525, Total Time: 413.76s\n",
            "Epoch 16/100\n",
            "[On Training] ==> Time: 27.30s, Train Loss: 0.244962298, Train Accuracy: 91.16%\n",
            "[On Validation] ==> Val loss: 0.321655733, Total Time: 441.06s\n",
            "Epoch 17/100\n",
            "[On Training] ==> Time: 28.30s, Train Loss: 0.230799133, Train Accuracy: 91.76%\n",
            "[On Validation] ==> Val loss: 0.318486834, Total Time: 469.36s\n",
            "Epoch 18/100\n",
            "[On Training] ==> Time: 27.38s, Train Loss: 0.218002715, Train Accuracy: 92.37%\n",
            "[On Validation] ==> Val loss: 0.316378424, Total Time: 496.74s\n",
            "Epoch 19/100\n",
            "[On Training] ==> Time: 27.92s, Train Loss: 0.206325878, Train Accuracy: 92.85%\n",
            "[On Validation] ==> Val loss: 0.315096052, Total Time: 524.67s\n",
            "Epoch 20/100\n",
            "[On Training] ==> Time: 27.94s, Train Loss: 0.195574263, Train Accuracy: 93.39%\n",
            "[On Validation] ==> Val loss: 0.314549437, Total Time: 552.61s\n",
            "--- AFTER TRAINING ---\tModel: Task2D.mdl\n",
            "[On Testing] Time: 2.4960849285125732, Test Accuracy: 88.34%\n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "[On Training] ==> Time: 29.89s, Train Loss: 0.692096797, Train Accuracy: 50.44%\n",
            "[On Validation] ==> Val loss: 0.690036945, Total Time: 29.89s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 29.87s, Train Loss: 0.687625172, Train Accuracy: 58.90%\n",
            "[On Validation] ==> Val loss: 0.684454286, Total Time: 59.76s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 30.04s, Train Loss: 0.680218043, Train Accuracy: 68.40%\n",
            "[On Validation] ==> Val loss: 0.674285434, Total Time: 89.80s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 29.82s, Train Loss: 0.666457728, Train Accuracy: 72.81%\n",
            "[On Validation] ==> Val loss: 0.655669258, Total Time: 119.62s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 29.81s, Train Loss: 0.641718720, Train Accuracy: 75.66%\n",
            "[On Validation] ==> Val loss: 0.623315065, Total Time: 149.43s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 29.87s, Train Loss: 0.600437044, Train Accuracy: 77.77%\n",
            "[On Validation] ==> Val loss: 0.573035782, Total Time: 179.29s\n",
            "Epoch 7/100\n",
            "[On Training] ==> Time: 29.70s, Train Loss: 0.542968059, Train Accuracy: 80.10%\n",
            "[On Validation] ==> Val loss: 0.512446245, Total Time: 208.99s\n",
            "Epoch 8/100\n",
            "[On Training] ==> Time: 29.96s, Train Loss: 0.481865996, Train Accuracy: 81.94%\n",
            "[On Validation] ==> Val loss: 0.458243882, Total Time: 238.95s\n",
            "Epoch 9/100\n",
            "[On Training] ==> Time: 29.70s, Train Loss: 0.430215695, Train Accuracy: 83.77%\n",
            "[On Validation] ==> Val loss: 0.418337432, Total Time: 268.65s\n",
            "Epoch 10/100\n",
            "[On Training] ==> Time: 29.76s, Train Loss: 0.390041394, Train Accuracy: 85.05%\n",
            "[On Validation] ==> Val loss: 0.390272633, Total Time: 298.40s\n",
            "Epoch 11/100\n",
            "[On Training] ==> Time: 29.74s, Train Loss: 0.357748223, Train Accuracy: 86.23%\n",
            "[On Validation] ==> Val loss: 0.369888804, Total Time: 328.15s\n",
            "Epoch 12/100\n",
            "[On Training] ==> Time: 29.69s, Train Loss: 0.330391148, Train Accuracy: 87.26%\n",
            "[On Validation] ==> Val loss: 0.354576701, Total Time: 357.84s\n",
            "Epoch 13/100\n",
            "[On Training] ==> Time: 29.83s, Train Loss: 0.306562074, Train Accuracy: 88.37%\n",
            "[On Validation] ==> Val loss: 0.342948652, Total Time: 387.67s\n",
            "Epoch 14/100\n",
            "[On Training] ==> Time: 29.68s, Train Loss: 0.285616776, Train Accuracy: 89.29%\n",
            "[On Validation] ==> Val loss: 0.334177576, Total Time: 417.35s\n",
            "Epoch 15/100\n",
            "[On Training] ==> Time: 29.79s, Train Loss: 0.267149737, Train Accuracy: 90.31%\n",
            "[On Validation] ==> Val loss: 0.327635400, Total Time: 447.14s\n",
            "Epoch 16/100\n",
            "[On Training] ==> Time: 29.69s, Train Loss: 0.250768300, Train Accuracy: 90.86%\n",
            "[On Validation] ==> Val loss: 0.322840315, Total Time: 476.83s\n",
            "Epoch 17/100\n",
            "[On Training] ==> Time: 29.61s, Train Loss: 0.236119016, Train Accuracy: 91.56%\n",
            "[On Validation] ==> Val loss: 0.319397226, Total Time: 506.43s\n",
            "Epoch 18/100\n",
            "[On Training] ==> Time: 29.71s, Train Loss: 0.222892387, Train Accuracy: 92.08%\n",
            "[On Validation] ==> Val loss: 0.317050074, Total Time: 536.14s\n",
            "Epoch 19/100\n",
            "[On Training] ==> Time: 29.96s, Train Loss: 0.210851645, Train Accuracy: 92.68%\n",
            "[On Validation] ==> Val loss: 0.315581829, Total Time: 566.10s\n",
            "Epoch 20/100\n",
            "[On Training] ==> Time: 29.68s, Train Loss: 0.199788187, Train Accuracy: 93.23%\n",
            "[On Validation] ==> Val loss: 0.314854371, Total Time: 595.79s\n",
            "Epoch 21/100\n",
            "[On Training] ==> Time: 29.56s, Train Loss: 0.189532776, Train Accuracy: 93.62%\n",
            "[On Validation] ==> Val loss: 0.314770065, Total Time: 625.35s\n",
            "--- AFTER TRAINING ---\tModel: Task2E.mdl\n",
            "[On Testing] Time: 2.473707675933838, Test Accuracy: 88.24%\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eMMAv_S1WLNF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download('Task2A.mdl')\n",
        "files.download('Task2B.mdl')\n",
        "files.download('Task2C.mdl')\n",
        "files.download('Task2D.mdl')\n",
        "files.download('Task2E.mdl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n2JByG_xnrr1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Task-3"
      ]
    },
    {
      "metadata": {
        "id": "vtAGGeuU2ect",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "task3 = [(100, 50, 10, 'Task3A.mdl'), (200, 100, 10, 'Task3B.mdl')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rz1rm-runrPt",
        "colab_type": "code",
        "outputId": "88f3b906-20e3-4be8-91a2-068e367e6b6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1992
        }
      },
      "cell_type": "code",
      "source": [
        "for num_of_hidden1, num_of_hidden2, num_of_hidden3, task_name in task3:\n",
        "    class BOWClassifier(nn.Module):\n",
        "        def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
        "            super(BOWClassifier, self).__init__()\n",
        "            SEED = 42\n",
        "            torch.manual_seed(SEED)\n",
        "            torch.cuda.manual_seed(SEED)\n",
        "            self.i2h1 = nn.Linear(input_size, hidden_size1) # initialises weights and biases i2h1\n",
        "            self.h12h2 = nn.Linear(hidden_size1, hidden_size2) # initialises weights and biases h12h2\n",
        "            self.h22h3 = nn.Linear(hidden_size2, hidden_size3) # initialises weights and biases h22h3\n",
        "            self.h32o = nn.Linear(hidden_size3, output_size) # initialises weights and biases h32o\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = torch.relu(self.i2h1(x)) # relu activation @ hidden layer\n",
        "            x = torch.relu(self.h12h2(x)) # relu activation @ hidden layer\n",
        "            x = torch.relu(self.h22h3(x)) # relu activation @ hidden layer\n",
        "            x = torch.sigmoid(self.h32o(x)) # sigmoid activation @ output layer\n",
        "            return x\n",
        "\n",
        "    num_of_input = VOCAB_SIZE # Bag word restriction has to be equal to vocabulary\n",
        "    num_of_output = 1 # binary sentiment classes (+ve, -ve)\n",
        "\n",
        "    bow = BOWClassifier(num_of_input, num_of_hidden1, num_of_hidden2, num_of_hidden3, num_of_output).to('cuda:0') # initialises weights and biases\n",
        "\n",
        "    # define a loss function and an optimizer\n",
        "    loss_function = nn.BCELoss()\n",
        "    opt = torch.optim.SGD(bow.parameters(), lr = 0.0001)\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    # Train The Model\n",
        "    epochs = 100\n",
        "    # the training loop\n",
        "    total_time = 0.0\n",
        "    prev_val_loss = float('inf')\n",
        "    val_loss = 0\n",
        "    early_stop_bow = 0\n",
        "\n",
        "    for e in range(epochs):\n",
        "        tic = time.time() # start the timer\n",
        "        correct = 0\n",
        "        cumulative_loss = 0\n",
        "        incorrect = 0\n",
        "\n",
        "        for i, instance in enumerate(train_data): # train_text_reviews \n",
        "            # get the training data\n",
        "            label = train_text_labels[i] # get the label of the corresponding instace\n",
        "            label = Variable(torch.FloatTensor([label])).resize_((1,1)).to('cuda:0') # wraps a tensor for label\n",
        "\n",
        "            bow.zero_grad() # reset the gradient for each instance\n",
        "\n",
        "            pred = bow.forward(instance)\n",
        "\n",
        "            loss = loss_function(pred, label) # compute the loss\n",
        "            loss.backward() # backprop the loss\n",
        "            opt.step() # performs parameter updation based on the current gradient\n",
        "\n",
        "            cumulative_loss += float(loss.item()) # accumulate the loss over whole training sample\n",
        "\n",
        "            pred_class = 1 if pred.item() > 0.5 else 0 # as sigmoid activated\n",
        "            if(int(label) == pred_class): # counting correct prediction in each epoch\n",
        "                correct += 1\n",
        "\n",
        "        train_loss = float(cumulative_loss)/num_train_data\n",
        "        train_accuracy = correct*float(100)/num_train_data\n",
        "\n",
        "#         val_loss, val_accuracy, p, r, f = evaluate_validation_accuracy(val_data, bow) # test val-data-set on currently trained model\n",
        "        val_loss = evaluate_validation_accuracy(val_data, bow) # test val-data-set on currently trained model\n",
        "\n",
        "        if(prev_val_loss-val_loss < 0.0): # early stop if nearly no change\n",
        "            bow = copy.deepcopy(early_stop_bow)\n",
        "            break\n",
        "\n",
        "        early_stop_bow = copy.deepcopy(bow)\n",
        "\n",
        "        prev_val_loss = val_loss\n",
        "\n",
        "        toc = time.time() # final time\n",
        "        total_time += (toc-tic)\n",
        "\n",
        "        print(\"Epoch {}/{}\\n[On Training] ==> Time: {:.2f}s, Train Loss: {:.9f}, Train Accuracy: {:.2f}%\".format(e+1, epochs, (toc-tic), train_loss, train_accuracy))\n",
        "#         print(\"[On Validation] ==> Precision: {:.3f}, Recall: {:.3f}, F-Score: {:.3f}, Val loss: {:.9f}, Val Accuracy: {:.2f}% Total Time: {:.2f}s\".format(p, r, f, val_loss, val_accuracy, total_time))\n",
        "        print(\"[On Validation] ==> Val loss: {:.9f}, Total Time: {:.2f}s\".format(val_loss, total_time))\n",
        "    \n",
        "    torch.save(bow, task_name)\n",
        "    \n",
        "    print('--- AFTER TRAINING ---\\tModel: {}'.format(task_name))\n",
        "    correct = 0\n",
        "    tic = time.time()\n",
        "    for i, instance in enumerate(data_test):\n",
        "        label = test_text_labels[i] # get the label of the corresponding instace\n",
        "        label = Variable(torch.FloatTensor([label])).resize_((1,1)).to('cuda:0') # wraps a tensor for label\n",
        "\n",
        "        pred = bow.forward(instance)\n",
        "        pred_class = 1 if pred.item() > 0.5 else 0 # sigmoid activated\n",
        "\n",
        "        if(int(label) == pred_class):\n",
        "            correct += 1\n",
        "    toc = time.time()\n",
        "    print(\"[On Testing] Time: {}, Test Accuracy: {:.2f}%\".format((toc-tic), correct*100/len(data_test)))\n",
        "\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "[On Training] ==> Time: 34.33s, Train Loss: 0.692630532, Train Accuracy: 50.27%\n",
            "[On Validation] ==> Val loss: 0.691402558, Total Time: 34.33s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 33.96s, Train Loss: 0.690193089, Train Accuracy: 56.99%\n",
            "[On Validation] ==> Val loss: 0.688726793, Total Time: 68.29s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 33.90s, Train Loss: 0.686891386, Train Accuracy: 65.47%\n",
            "[On Validation] ==> Val loss: 0.684602229, Total Time: 102.19s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 33.98s, Train Loss: 0.681620428, Train Accuracy: 69.19%\n",
            "[On Validation] ==> Val loss: 0.677938429, Total Time: 136.17s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 33.97s, Train Loss: 0.672985672, Train Accuracy: 71.72%\n",
            "[On Validation] ==> Val loss: 0.666855308, Total Time: 170.14s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 33.95s, Train Loss: 0.658400188, Train Accuracy: 73.76%\n",
            "[On Validation] ==> Val loss: 0.648114574, Total Time: 204.09s\n",
            "Epoch 7/100\n",
            "[On Training] ==> Time: 33.91s, Train Loss: 0.634059845, Train Accuracy: 75.91%\n",
            "[On Validation] ==> Val loss: 0.617541231, Total Time: 238.00s\n",
            "Epoch 8/100\n",
            "[On Training] ==> Time: 33.85s, Train Loss: 0.595491574, Train Accuracy: 78.69%\n",
            "[On Validation] ==> Val loss: 0.571270672, Total Time: 271.85s\n",
            "Epoch 9/100\n",
            "[On Training] ==> Time: 33.88s, Train Loss: 0.540047978, Train Accuracy: 81.07%\n",
            "[On Validation] ==> Val loss: 0.510100367, Total Time: 305.73s\n",
            "Epoch 10/100\n",
            "[On Training] ==> Time: 33.94s, Train Loss: 0.473570894, Train Accuracy: 83.21%\n",
            "[On Validation] ==> Val loss: 0.447963744, Total Time: 339.67s\n",
            "Epoch 11/100\n",
            "[On Training] ==> Time: 33.79s, Train Loss: 0.411423843, Train Accuracy: 84.87%\n",
            "[On Validation] ==> Val loss: 0.401082950, Total Time: 373.46s\n",
            "Epoch 12/100\n",
            "[On Training] ==> Time: 33.88s, Train Loss: 0.362022896, Train Accuracy: 86.39%\n",
            "[On Validation] ==> Val loss: 0.370274018, Total Time: 407.34s\n",
            "Epoch 13/100\n",
            "[On Training] ==> Time: 33.97s, Train Loss: 0.322599990, Train Accuracy: 87.97%\n",
            "[On Validation] ==> Val loss: 0.349605353, Total Time: 441.30s\n",
            "Epoch 14/100\n",
            "[On Training] ==> Time: 33.92s, Train Loss: 0.289684464, Train Accuracy: 89.29%\n",
            "[On Validation] ==> Val loss: 0.336039628, Total Time: 475.22s\n",
            "Epoch 15/100\n",
            "[On Training] ==> Time: 33.95s, Train Loss: 0.261932703, Train Accuracy: 90.54%\n",
            "[On Validation] ==> Val loss: 0.327792532, Total Time: 509.17s\n",
            "Epoch 16/100\n",
            "[On Training] ==> Time: 33.95s, Train Loss: 0.238276913, Train Accuracy: 91.40%\n",
            "[On Validation] ==> Val loss: 0.323179218, Total Time: 543.12s\n",
            "Epoch 17/100\n",
            "[On Training] ==> Time: 33.90s, Train Loss: 0.217836265, Train Accuracy: 92.41%\n",
            "[On Validation] ==> Val loss: 0.321243302, Total Time: 577.02s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type BOWClassifier. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--- AFTER TRAINING ---\tModel: Task3A.mdl\n",
            "[On Testing] Time: 2.637869119644165, Test Accuracy: 88.38%\n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "[On Training] ==> Time: 41.33s, Train Loss: 0.694334867, Train Accuracy: 50.03%\n",
            "[On Validation] ==> Val loss: 0.692519313, Total Time: 41.33s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 41.28s, Train Loss: 0.691208783, Train Accuracy: 50.61%\n",
            "[On Validation] ==> Val loss: 0.689922050, Total Time: 82.61s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 41.32s, Train Loss: 0.688374524, Train Accuracy: 57.25%\n",
            "[On Validation] ==> Val loss: 0.686649844, Total Time: 123.93s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 41.29s, Train Loss: 0.684441089, Train Accuracy: 64.94%\n",
            "[On Validation] ==> Val loss: 0.681825521, Total Time: 165.22s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 41.48s, Train Loss: 0.678350999, Train Accuracy: 69.53%\n",
            "[On Validation] ==> Val loss: 0.674077965, Total Time: 206.70s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 41.33s, Train Loss: 0.668159923, Train Accuracy: 72.51%\n",
            "[On Validation] ==> Val loss: 0.660859317, Total Time: 248.03s\n",
            "Epoch 7/100\n",
            "[On Training] ==> Time: 41.32s, Train Loss: 0.650420290, Train Accuracy: 74.98%\n",
            "[On Validation] ==> Val loss: 0.637653554, Total Time: 289.35s\n",
            "Epoch 8/100\n",
            "[On Training] ==> Time: 41.40s, Train Loss: 0.619316395, Train Accuracy: 77.57%\n",
            "[On Validation] ==> Val loss: 0.597587033, Total Time: 330.76s\n",
            "Epoch 9/100\n",
            "[On Training] ==> Time: 41.30s, Train Loss: 0.568020845, Train Accuracy: 79.91%\n",
            "[On Validation] ==> Val loss: 0.536246359, Total Time: 372.05s\n",
            "Epoch 10/100\n",
            "[On Training] ==> Time: 41.19s, Train Loss: 0.498216709, Train Accuracy: 82.15%\n",
            "[On Validation] ==> Val loss: 0.465920298, Total Time: 413.25s\n",
            "Epoch 11/100\n",
            "[On Training] ==> Time: 41.31s, Train Loss: 0.429144973, Train Accuracy: 83.98%\n",
            "[On Validation] ==> Val loss: 0.410889066, Total Time: 454.56s\n",
            "Epoch 12/100\n",
            "[On Training] ==> Time: 41.51s, Train Loss: 0.375356094, Train Accuracy: 85.73%\n",
            "[On Validation] ==> Val loss: 0.375686118, Total Time: 496.07s\n",
            "Epoch 13/100\n",
            "[On Training] ==> Time: 41.24s, Train Loss: 0.333681911, Train Accuracy: 87.39%\n",
            "[On Validation] ==> Val loss: 0.352677731, Total Time: 537.31s\n",
            "Epoch 14/100\n",
            "[On Training] ==> Time: 41.23s, Train Loss: 0.299051857, Train Accuracy: 88.80%\n",
            "[On Validation] ==> Val loss: 0.337361870, Total Time: 578.53s\n",
            "Epoch 15/100\n",
            "[On Training] ==> Time: 41.46s, Train Loss: 0.269679017, Train Accuracy: 90.13%\n",
            "[On Validation] ==> Val loss: 0.327806500, Total Time: 619.99s\n",
            "Epoch 16/100\n",
            "[On Training] ==> Time: 41.37s, Train Loss: 0.244792910, Train Accuracy: 91.10%\n",
            "[On Validation] ==> Val loss: 0.322479163, Total Time: 661.36s\n",
            "Epoch 17/100\n",
            "[On Training] ==> Time: 41.27s, Train Loss: 0.223433999, Train Accuracy: 92.03%\n",
            "[On Validation] ==> Val loss: 0.319962659, Total Time: 702.63s\n",
            "Epoch 18/100\n",
            "[On Training] ==> Time: 41.30s, Train Loss: 0.204695133, Train Accuracy: 92.84%\n",
            "[On Validation] ==> Val loss: 0.319456448, Total Time: 743.93s\n",
            "--- AFTER TRAINING ---\tModel: Task3B.mdl\n",
            "[On Testing] Time: 2.6196510791778564, Test Accuracy: 88.38%\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vGf7IdbRdbk7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download('Task3A.mdl')\n",
        "files.download('Task3B.mdl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-MYVmtv9jn7n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "quMcuqqyjqP2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## TASK-4"
      ]
    },
    {
      "metadata": {
        "id": "FznXcSrri_l6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "task4 = [(30, 20, 10, 'Task4A.mdl'), (100, 100, 0, 'Task4B.mdl'), (100, 10, 0, 'Task4C.mdl')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TMTr6ZqQjoZA",
        "colab_type": "code",
        "outputId": "b21a434e-d322-4d35-e840-69b10b4dde6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3080
        }
      },
      "cell_type": "code",
      "source": [
        "k = 1\n",
        "for num_of_hidden1, num_of_hidden2, num_of_hidden3, task_name in task4:\n",
        "    if(k == 1):\n",
        "        k = 2\n",
        "        class BOWClassifier(nn.Module):\n",
        "            def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
        "                super(BOWClassifier, self).__init__()\n",
        "                SEED = 42\n",
        "                torch.manual_seed(SEED)\n",
        "                torch.cuda.manual_seed(SEED)\n",
        "                \n",
        "                self.drop_out1 = nn.Dropout(p=0.1) # Adding dropout layer\n",
        "                self.drop_out2 = nn.Dropout(p=0.1) # Adding dropout layer\n",
        "                self.drop_out3 = nn.Dropout(p=0.0) # Adding dropout layer\n",
        "                \n",
        "                self.i2h1 = nn.Linear(input_size, hidden_size1) # initialises weights and biases i2h1\n",
        "                self.h12h2 = nn.Linear(hidden_size1, hidden_size2) # initialises weights and biases h12h2\n",
        "                self.h22h3 = nn.Linear(hidden_size2, hidden_size3) # initialises weights and biases h22h3\n",
        "                self.h32o = nn.Linear(hidden_size3, output_size) # initialises weights and biases h32o\n",
        "\n",
        "            def forward(self, x):\n",
        "                x = torch.relu(self.drop_out1(self.i2h1(x))) # relu activation @ hidden layer\n",
        "                x = torch.relu(self.drop_out2(self.h12h2(x))) # relu activation @ hidden layer\n",
        "                x = torch.relu(self.drop_out3(self.h22h3(x))) # relu activation @ hidden layer\n",
        "                x = torch.sigmoid(self.h32o(x)) # sigmoid activation @ output layer\n",
        "                return x\n",
        "\n",
        "        num_of_input = VOCAB_SIZE # Bag word restriction has to be equal to vocabulary\n",
        "        num_of_output = 1 # binary sentiment classes (+ve, -ve)\n",
        "\n",
        "        bow = BOWClassifier(num_of_input, num_of_hidden1, num_of_hidden2, num_of_hidden3, num_of_output).to('cuda:0') # initialises weights and biases\n",
        "    elif(k == 2):\n",
        "        k = 2\n",
        "        class BOWClassifier(nn.Module):\n",
        "            def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
        "                super(BOWClassifier, self).__init__()\n",
        "                SEED = 42\n",
        "                torch.manual_seed(SEED)\n",
        "                torch.cuda.manual_seed(SEED)\n",
        "                \n",
        "                self.drop_out1 = nn.Dropout(p=0.1) # Adding dropout layer\n",
        "                self.drop_out2 = nn.Dropout(p=0.1) # Adding dropout layer\n",
        "                \n",
        "                self.i2h = nn.Linear(input_size, hidden_size1) # initialises weights and biases i2h\n",
        "                self.h2h = nn.Linear(hidden_size1, hidden_size2) # initialises weights and biases i2h\n",
        "                self.h2o = nn.Linear(hidden_size2, output_size) # initialises weights and biases h2o\n",
        "\n",
        "            def forward(self, x):\n",
        "                x = torch.relu(self.drop_out1(self.i2h(x))) # relu activation @ hidden layer\n",
        "                x = torch.relu(self.drop_out2(self.h2h(x))) # relu activation @ hidden layer\n",
        "                x = torch.sigmoid(self.h2o(x)) # sigmoid activation @ output layer\n",
        "                return x\n",
        "\n",
        "        num_of_input = VOCAB_SIZE # Bag word restriction has to be equal to vocabulary\n",
        "        num_of_output = 1 # binary sentiment classes (+ve, -ve)\n",
        "\n",
        "        bow = BOWClassifier(num_of_input, num_of_hidden1, num_of_hidden2, num_of_output).to('cuda:0') # initialises weights and biases\n",
        "\n",
        "    # define a loss function and an optimizer\n",
        "    loss_function = nn.BCELoss()\n",
        "    opt = torch.optim.SGD(bow.parameters(), lr = 0.0001)\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    # Train The Model\n",
        "    epochs = 100\n",
        "    # the training loop\n",
        "    total_time = 0.0\n",
        "    prev_val_loss = float('inf')\n",
        "    val_loss = 0\n",
        "    early_stop_bow = 0\n",
        "\n",
        "    for e in range(epochs):\n",
        "        tic = time.time() # start the timer\n",
        "        correct = 0\n",
        "        cumulative_loss = 0\n",
        "        incorrect = 0\n",
        "\n",
        "        for i, instance in enumerate(train_data): # train_text_reviews \n",
        "            # get the training data\n",
        "            label = train_text_labels[i] # get the label of the corresponding instace\n",
        "            label = Variable(torch.FloatTensor([label])).resize_((1,1)).to('cuda:0') # wraps a tensor for label\n",
        "\n",
        "            bow.zero_grad() # reset the gradient for each instance\n",
        "\n",
        "            pred = bow.forward(instance)\n",
        "\n",
        "            loss = loss_function(pred, label) # compute the loss\n",
        "            loss.backward() # backprop the loss\n",
        "            opt.step() # performs parameter updation based on the current gradient\n",
        "\n",
        "            cumulative_loss += float(loss.item()) # accumulate the loss over whole training sample\n",
        "\n",
        "            pred_class = 1 if pred.item() > 0.5 else 0 # as sigmoid activated\n",
        "            if(int(label) == pred_class): # counting correct prediction in each epoch\n",
        "                correct += 1\n",
        "\n",
        "        train_loss = float(cumulative_loss)/num_train_data\n",
        "        train_accuracy = correct*float(100)/num_train_data\n",
        "\n",
        "#         val_loss, val_accuracy, p, r, f = evaluate_validation_accuracy(val_data, bow) # test val-data-set on currently trained model\n",
        "        val_loss = evaluate_validation_accuracy(val_data, bow) # test val-data-set on currently trained model\n",
        "    \n",
        "        if(prev_val_loss-val_loss < 0.0): # early stop if nearly no change\n",
        "            bow = copy.deepcopy(early_stop_bow)\n",
        "            break\n",
        "\n",
        "        early_stop_bow = copy.deepcopy(bow)\n",
        "\n",
        "        prev_val_loss = val_loss\n",
        "\n",
        "        toc = time.time() # final time\n",
        "        total_time += (toc-tic)\n",
        "\n",
        "        print(\"Epoch {}/{}\\n[On Training] ==> Time: {:.2f}s, Train Loss: {:.9f}, Train Accuracy: {:.2f}%\".format(e+1, epochs, (toc-tic), train_loss, train_accuracy))\n",
        "#         print(\"[On Validation] ==> Precision: {:.3f}, Recall: {:.3f}, F-Score: {:.3f}, Val loss: {:.9f}, Val Accuracy: {:.2f}% Total Time: {:.2f}s\".format(p, r, f, val_loss, val_accuracy, total_time))\n",
        "        print(\"[On Validation] ==> Val loss: {:.9f}, Total Time: {:.2f}s\".format(val_loss, total_time))\n",
        "\n",
        "    torch.save(bow, task_name)\n",
        "    \n",
        "    print('--- AFTER TRAINING ---\\tModel: {}'.format(task_name))\n",
        "    correct = 0\n",
        "    tic = time.time()\n",
        "    for i, instance in enumerate(data_test):\n",
        "        label = test_text_labels[i] # get the label of the corresponding instace\n",
        "        label = Variable(torch.FloatTensor([label])).resize_((1,1)).to('cuda:0') # wraps a tensor for label\n",
        "\n",
        "        pred = bow.forward(instance)\n",
        "        pred_class = 1 if pred.item() > 0.5 else 0 # sigmoid activated\n",
        "\n",
        "        if(int(label) == pred_class):\n",
        "            correct += 1\n",
        "    toc = time.time()\n",
        "    print(\"[On Testing] Time: {}, Test Accuracy: {:.2f}%\".format((toc-tic), correct*100/len(data_test)))\n",
        "\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "[On Training] ==> Time: 32.41s, Train Loss: 0.691956612, Train Accuracy: 57.37%\n",
            "[On Validation] ==> Val loss: 0.690812181, Total Time: 32.41s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 32.61s, Train Loss: 0.688779979, Train Accuracy: 66.11%\n",
            "[On Validation] ==> Val loss: 0.686643423, Total Time: 65.03s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 32.39s, Train Loss: 0.684310919, Train Accuracy: 70.87%\n",
            "[On Validation] ==> Val loss: 0.681621355, Total Time: 97.42s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 32.20s, Train Loss: 0.678987091, Train Accuracy: 72.14%\n",
            "[On Validation] ==> Val loss: 0.675607632, Total Time: 129.62s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 32.19s, Train Loss: 0.671137377, Train Accuracy: 74.06%\n",
            "[On Validation] ==> Val loss: 0.666980039, Total Time: 161.80s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 32.64s, Train Loss: 0.659777985, Train Accuracy: 75.70%\n",
            "[On Validation] ==> Val loss: 0.651878908, Total Time: 194.45s\n",
            "Epoch 7/100\n",
            "[On Training] ==> Time: 32.91s, Train Loss: 0.641804253, Train Accuracy: 77.37%\n",
            "[On Validation] ==> Val loss: 0.629889480, Total Time: 227.35s\n",
            "Epoch 8/100\n",
            "[On Training] ==> Time: 32.90s, Train Loss: 0.612679697, Train Accuracy: 78.25%\n",
            "[On Validation] ==> Val loss: 0.595531322, Total Time: 260.25s\n",
            "Epoch 9/100\n",
            "[On Training] ==> Time: 32.97s, Train Loss: 0.567654939, Train Accuracy: 79.90%\n",
            "[On Validation] ==> Val loss: 0.543272061, Total Time: 293.22s\n",
            "Epoch 10/100\n",
            "[On Training] ==> Time: 32.80s, Train Loss: 0.507202321, Train Accuracy: 81.44%\n",
            "[On Validation] ==> Val loss: 0.476844808, Total Time: 326.02s\n",
            "Epoch 11/100\n",
            "[On Training] ==> Time: 32.54s, Train Loss: 0.447646662, Train Accuracy: 82.97%\n",
            "[On Validation] ==> Val loss: 0.436323833, Total Time: 358.56s\n",
            "Epoch 12/100\n",
            "[On Training] ==> Time: 33.02s, Train Loss: 0.399006546, Train Accuracy: 84.57%\n",
            "[On Validation] ==> Val loss: 0.410168646, Total Time: 391.58s\n",
            "Epoch 13/100\n",
            "[On Training] ==> Time: 32.87s, Train Loss: 0.359220055, Train Accuracy: 86.28%\n",
            "[On Validation] ==> Val loss: 0.377428829, Total Time: 424.44s\n",
            "Epoch 14/100\n",
            "[On Training] ==> Time: 32.75s, Train Loss: 0.329084670, Train Accuracy: 87.61%\n",
            "[On Validation] ==> Val loss: 0.367022825, Total Time: 457.19s\n",
            "Epoch 15/100\n",
            "[On Training] ==> Time: 32.53s, Train Loss: 0.299627462, Train Accuracy: 88.91%\n",
            "[On Validation] ==> Val loss: 0.346482286, Total Time: 489.72s\n",
            "Epoch 16/100\n",
            "[On Training] ==> Time: 33.22s, Train Loss: 0.274341534, Train Accuracy: 89.83%\n",
            "[On Validation] ==> Val loss: 0.339751292, Total Time: 522.94s\n",
            "Epoch 17/100\n",
            "[On Training] ==> Time: 32.92s, Train Loss: 0.253017328, Train Accuracy: 90.86%\n",
            "[On Validation] ==> Val loss: 0.338958980, Total Time: 555.86s\n",
            "Epoch 18/100\n",
            "[On Training] ==> Time: 32.01s, Train Loss: 0.234126147, Train Accuracy: 91.84%\n",
            "[On Validation] ==> Val loss: 0.333139089, Total Time: 587.86s\n",
            "Epoch 19/100\n",
            "[On Training] ==> Time: 32.47s, Train Loss: 0.218058473, Train Accuracy: 92.54%\n",
            "[On Validation] ==> Val loss: 0.323233687, Total Time: 620.33s\n",
            "Epoch 20/100\n",
            "[On Training] ==> Time: 32.70s, Train Loss: 0.201244482, Train Accuracy: 93.02%\n",
            "[On Validation] ==> Val loss: 0.318901244, Total Time: 653.03s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type BOWClassifier. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--- AFTER TRAINING ---\tModel: Task4A.mdl\n",
            "[On Testing] Time: 3.0457141399383545, Test Accuracy: 87.68%\n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "[On Training] ==> Time: 32.66s, Train Loss: 0.691899299, Train Accuracy: 50.89%\n",
            "[On Validation] ==> Val loss: 0.689207148, Total Time: 32.66s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 32.57s, Train Loss: 0.686522725, Train Accuracy: 60.35%\n",
            "[On Validation] ==> Val loss: 0.683110623, Total Time: 65.24s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 32.80s, Train Loss: 0.678141074, Train Accuracy: 67.93%\n",
            "[On Validation] ==> Val loss: 0.671698995, Total Time: 98.04s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 32.92s, Train Loss: 0.662649004, Train Accuracy: 72.03%\n",
            "[On Validation] ==> Val loss: 0.650117480, Total Time: 130.96s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 33.72s, Train Loss: 0.635433326, Train Accuracy: 74.81%\n",
            "[On Validation] ==> Val loss: 0.616078615, Total Time: 164.68s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 33.27s, Train Loss: 0.591664840, Train Accuracy: 77.49%\n",
            "[On Validation] ==> Val loss: 0.563409770, Total Time: 197.96s\n",
            "Epoch 7/100\n",
            "[On Training] ==> Time: 33.34s, Train Loss: 0.534481590, Train Accuracy: 79.99%\n",
            "[On Validation] ==> Val loss: 0.506239093, Total Time: 231.30s\n",
            "Epoch 8/100\n",
            "[On Training] ==> Time: 33.35s, Train Loss: 0.476691583, Train Accuracy: 81.60%\n",
            "[On Validation] ==> Val loss: 0.459279812, Total Time: 264.65s\n",
            "Epoch 9/100\n",
            "[On Training] ==> Time: 33.31s, Train Loss: 0.429842728, Train Accuracy: 83.32%\n",
            "[On Validation] ==> Val loss: 0.421508363, Total Time: 297.96s\n",
            "Epoch 10/100\n",
            "[On Training] ==> Time: 33.11s, Train Loss: 0.393644209, Train Accuracy: 84.76%\n",
            "[On Validation] ==> Val loss: 0.392664407, Total Time: 331.07s\n",
            "Epoch 11/100\n",
            "[On Training] ==> Time: 32.98s, Train Loss: 0.364609307, Train Accuracy: 85.73%\n",
            "[On Validation] ==> Val loss: 0.381162134, Total Time: 364.05s\n",
            "Epoch 12/100\n",
            "[On Training] ==> Time: 32.98s, Train Loss: 0.338409651, Train Accuracy: 87.18%\n",
            "[On Validation] ==> Val loss: 0.360411127, Total Time: 397.04s\n",
            "Epoch 13/100\n",
            "[On Training] ==> Time: 32.96s, Train Loss: 0.313990468, Train Accuracy: 88.07%\n",
            "[On Validation] ==> Val loss: 0.352303149, Total Time: 430.00s\n",
            "Epoch 14/100\n",
            "[On Training] ==> Time: 33.17s, Train Loss: 0.293853218, Train Accuracy: 88.96%\n",
            "[On Validation] ==> Val loss: 0.343941128, Total Time: 463.17s\n",
            "Epoch 15/100\n",
            "[On Training] ==> Time: 32.91s, Train Loss: 0.276533180, Train Accuracy: 89.67%\n",
            "[On Validation] ==> Val loss: 0.343433441, Total Time: 496.08s\n",
            "Epoch 16/100\n",
            "[On Training] ==> Time: 32.97s, Train Loss: 0.261321915, Train Accuracy: 90.51%\n",
            "[On Validation] ==> Val loss: 0.337938544, Total Time: 529.05s\n",
            "Epoch 17/100\n",
            "[On Training] ==> Time: 33.06s, Train Loss: 0.245244539, Train Accuracy: 91.07%\n",
            "[On Validation] ==> Val loss: 0.328689681, Total Time: 562.11s\n",
            "--- AFTER TRAINING ---\tModel: Task4B.mdl\n",
            "[On Testing] Time: 2.6005632877349854, Test Accuracy: 87.60%\n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "[On Training] ==> Time: 32.05s, Train Loss: 0.690785398, Train Accuracy: 58.54%\n",
            "[On Validation] ==> Val loss: 0.688038361, Total Time: 32.05s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 32.41s, Train Loss: 0.682983635, Train Accuracy: 66.35%\n",
            "[On Validation] ==> Val loss: 0.677022469, Total Time: 64.46s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 32.57s, Train Loss: 0.667787762, Train Accuracy: 70.98%\n",
            "[On Validation] ==> Val loss: 0.658238406, Total Time: 97.03s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 32.71s, Train Loss: 0.643768916, Train Accuracy: 73.94%\n",
            "[On Validation] ==> Val loss: 0.627698490, Total Time: 129.74s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 32.91s, Train Loss: 0.608202971, Train Accuracy: 75.77%\n",
            "[On Validation] ==> Val loss: 0.588026294, Total Time: 162.65s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 33.33s, Train Loss: 0.559753318, Train Accuracy: 78.24%\n",
            "[On Validation] ==> Val loss: 0.533162564, Total Time: 195.98s\n",
            "Epoch 7/100\n",
            "[On Training] ==> Time: 33.32s, Train Loss: 0.507669462, Train Accuracy: 80.07%\n",
            "[On Validation] ==> Val loss: 0.484807824, Total Time: 229.29s\n",
            "Epoch 8/100\n",
            "[On Training] ==> Time: 33.33s, Train Loss: 0.461074485, Train Accuracy: 81.66%\n",
            "[On Validation] ==> Val loss: 0.454291511, Total Time: 262.62s\n",
            "Epoch 9/100\n",
            "[On Training] ==> Time: 33.13s, Train Loss: 0.426850257, Train Accuracy: 82.87%\n",
            "[On Validation] ==> Val loss: 0.427985458, Total Time: 295.75s\n",
            "Epoch 10/100\n",
            "[On Training] ==> Time: 33.23s, Train Loss: 0.394030078, Train Accuracy: 84.73%\n",
            "[On Validation] ==> Val loss: 0.402892148, Total Time: 328.98s\n",
            "Epoch 11/100\n",
            "[On Training] ==> Time: 33.39s, Train Loss: 0.369809106, Train Accuracy: 85.45%\n",
            "[On Validation] ==> Val loss: 0.391573302, Total Time: 362.37s\n",
            "Epoch 12/100\n",
            "[On Training] ==> Time: 33.23s, Train Loss: 0.347647398, Train Accuracy: 86.51%\n",
            "[On Validation] ==> Val loss: 0.373814179, Total Time: 395.60s\n",
            "Epoch 13/100\n",
            "[On Training] ==> Time: 32.94s, Train Loss: 0.324873141, Train Accuracy: 87.63%\n",
            "[On Validation] ==> Val loss: 0.361298509, Total Time: 428.55s\n",
            "Epoch 14/100\n",
            "[On Training] ==> Time: 33.06s, Train Loss: 0.307075120, Train Accuracy: 88.43%\n",
            "[On Validation] ==> Val loss: 0.358812729, Total Time: 461.60s\n",
            "Epoch 15/100\n",
            "[On Training] ==> Time: 33.46s, Train Loss: 0.290214609, Train Accuracy: 89.12%\n",
            "[On Validation] ==> Val loss: 0.347074569, Total Time: 495.06s\n",
            "Epoch 16/100\n",
            "[On Training] ==> Time: 33.29s, Train Loss: 0.271596296, Train Accuracy: 90.26%\n",
            "[On Validation] ==> Val loss: 0.342931460, Total Time: 528.35s\n",
            "Epoch 17/100\n",
            "[On Training] ==> Time: 33.10s, Train Loss: 0.257919157, Train Accuracy: 90.55%\n",
            "[On Validation] ==> Val loss: 0.341919485, Total Time: 561.45s\n",
            "Epoch 18/100\n",
            "[On Training] ==> Time: 33.18s, Train Loss: 0.246722357, Train Accuracy: 91.18%\n",
            "[On Validation] ==> Val loss: 0.334267204, Total Time: 594.64s\n",
            "--- AFTER TRAINING ---\tModel: Task4C.mdl\n",
            "[On Testing] Time: 2.6776630878448486, Test Accuracy: 86.80%\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qD-kkwEKW0iM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download('Task4A.mdl')\n",
        "files.download('Task4B.mdl')\n",
        "files.download('Task4C.mdl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "12KQYM1RYuiH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Task-5"
      ]
    },
    {
      "metadata": {
        "id": "cd6rex2FYwmp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "task5 = [('relu', 'Task5A.mdl'), ('tanh', 'Task5B.mdl'), ('sigmoid', 'Task5C.mdl')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-lCj7dx6ZB-Y",
        "colab_type": "code",
        "outputId": "357f5120-b502-4a27-b4c1-e3a839e6f0e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2468
        }
      },
      "cell_type": "code",
      "source": [
        "for activation, task_name in task5:\n",
        "    if(activation == 'relu'):\n",
        "        class BOWClassifier(nn.Module):\n",
        "            def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
        "                super(BOWClassifier, self).__init__()\n",
        "                SEED = 42\n",
        "                torch.manual_seed(SEED)\n",
        "                torch.cuda.manual_seed(SEED)\n",
        "                \n",
        "                self.drop_out1 = nn.Dropout(p=0.1) # Adding dropout layer\n",
        "                self.drop_out2 = nn.Dropout(p=0.1) # Adding dropout layer\n",
        "                self.drop_out3 = nn.Dropout(p=0.0) # Adding dropout layer\n",
        "                \n",
        "                self.i2h1 = nn.Linear(input_size, hidden_size1) # initialises weights and biases i2h1\n",
        "                self.h12h2 = nn.Linear(hidden_size1, hidden_size2) # initialises weights and biases h12h2\n",
        "                self.h22h3 = nn.Linear(hidden_size2, hidden_size3) # initialises weights and biases h22h3\n",
        "                self.h32o = nn.Linear(hidden_size3, output_size) # initialises weights and biases h32o\n",
        "\n",
        "            def forward(self, x):\n",
        "                x = torch.relu(self.drop_out1(self.i2h1(x))) # relu activation @ hidden layer\n",
        "                x = torch.relu(self.drop_out2(self.h12h2(x))) # relu activation @ hidden layer\n",
        "                x = torch.relu(self.drop_out3(self.h22h3(x))) # relu activation @ hidden layer\n",
        "                x = torch.sigmoid(self.h32o(x)) # sigmoid activation @ output layer\n",
        "                return x\n",
        "\n",
        "        num_of_input = VOCAB_SIZE # Bag word restriction has to be equal to vocabulary\n",
        "        num_of_hidden1 = 30\n",
        "        num_of_hidden2 = 20\n",
        "        num_of_hidden3 = 10\n",
        "        num_of_output = 1 # binary sentiment classes (+ve, -ve)\n",
        "\n",
        "        bow = BOWClassifier(num_of_input, num_of_hidden1, num_of_hidden2, num_of_hidden3, num_of_output).to('cuda:0') # initialises weights and biases\n",
        "        \n",
        "    if(activation == 'tanh'):\n",
        "        class BOWClassifier(nn.Module):\n",
        "            def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
        "                super(BOWClassifier, self).__init__()\n",
        "                SEED = 42\n",
        "                torch.manual_seed(SEED)\n",
        "                torch.cuda.manual_seed(SEED)\n",
        "                \n",
        "                self.drop_out1 = nn.Dropout(p=0.1) # Adding dropout layer\n",
        "                self.drop_out2 = nn.Dropout(p=0.1) # Adding dropout layer\n",
        "                self.drop_out3 = nn.Dropout(p=0.0) # Adding dropout layer\n",
        "                \n",
        "                self.i2h1 = nn.Linear(input_size, hidden_size1) # initialises weights and biases i2h1\n",
        "                self.h12h2 = nn.Linear(hidden_size1, hidden_size2) # initialises weights and biases h12h2\n",
        "                self.h22h3 = nn.Linear(hidden_size2, hidden_size3) # initialises weights and biases h22h3\n",
        "                self.h32o = nn.Linear(hidden_size3, output_size) # initialises weights and biases h32o\n",
        "\n",
        "            def forward(self, x):\n",
        "                x = torch.tanh(self.drop_out1(self.i2h1(x))) # tanh activation @ hidden layer\n",
        "                x = torch.tanh(self.drop_out2(self.h12h2(x))) # tanh activation @ hidden layer\n",
        "                x = torch.tanh(self.drop_out3(self.h22h3(x))) # tanh activation @ hidden layer\n",
        "                x = torch.sigmoid(self.h32o(x)) # sigmoid activation @ output layer\n",
        "                return x\n",
        "\n",
        "        num_of_input = VOCAB_SIZE # Bag word restriction has to be equal to vocabulary\n",
        "        num_of_hidden1 = 30\n",
        "        num_of_hidden2 = 20\n",
        "        num_of_hidden3 = 10\n",
        "        num_of_output = 1 # binary sentiment classes (+ve, -ve)\n",
        "\n",
        "        bow = BOWClassifier(num_of_input, num_of_hidden1, num_of_hidden2, num_of_hidden3, num_of_output).to('cuda:0') # initialises weights and biases\n",
        "        \n",
        "    if(activation == 'sigmoid'):\n",
        "        class BOWClassifier(nn.Module):\n",
        "            def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
        "                super(BOWClassifier, self).__init__()\n",
        "                SEED = 42\n",
        "                torch.manual_seed(SEED)\n",
        "                torch.cuda.manual_seed(SEED)\n",
        "                \n",
        "                self.drop_out1 = nn.Dropout(p=0.1) # Adding dropout layer\n",
        "                self.drop_out2 = nn.Dropout(p=0.1) # Adding dropout layer\n",
        "                self.drop_out3 = nn.Dropout(p=0.0) # Adding dropout layer\n",
        "                \n",
        "                self.i2h1 = nn.Linear(input_size, hidden_size1) # initialises weights and biases i2h1\n",
        "                self.h12h2 = nn.Linear(hidden_size1, hidden_size2) # initialises weights and biases h12h2\n",
        "                self.h22h3 = nn.Linear(hidden_size2, hidden_size3) # initialises weights and biases h22h3\n",
        "                self.h32o = nn.Linear(hidden_size3, output_size) # initialises weights and biases h32o\n",
        "\n",
        "            def forward(self, x):\n",
        "                x = torch.sigmoid(self.drop_out1(self.i2h1(x))) # sigmoid activation @ hidden layer\n",
        "                x = torch.sigmoid(self.drop_out2(self.h12h2(x))) # sigmoid activation @ hidden layer\n",
        "                x = torch.sigmoid(self.drop_out3(self.h22h3(x))) # sigmoid activation @ hidden layer\n",
        "                x = torch.sigmoid(self.h32o(x)) # sigmoid activation @ output layer\n",
        "                return x\n",
        "\n",
        "        num_of_input = VOCAB_SIZE # Bag word restriction has to be equal to vocabulary\n",
        "        num_of_hidden1 = 30\n",
        "        num_of_hidden2 = 20\n",
        "        num_of_hidden3 = 10\n",
        "        num_of_output = 1 # binary sentiment classes (+ve, -ve)\n",
        "\n",
        "        bow = BOWClassifier(num_of_input, num_of_hidden1, num_of_hidden2, num_of_hidden3, num_of_output).to('cuda:0') # initialises weights and biases\n",
        "        \n",
        "    # define a loss function and an optimizer\n",
        "    loss_function = nn.BCELoss()\n",
        "    if(activation == 'sigmoid'):\n",
        "        opt = torch.optim.SGD(bow.parameters(), lr = 0.007) ## sigmoid activation converges slowly\n",
        "    else:\n",
        "        opt = torch.optim.SGD(bow.parameters(), lr = 0.0001)\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    # Train The Model\n",
        "    epochs = 100\n",
        "    # the training loop\n",
        "    total_time = 0.0\n",
        "    prev_val_loss = float('inf')\n",
        "    val_loss = 0\n",
        "    early_stop_bow = 0\n",
        "\n",
        "    for e in range(epochs):\n",
        "        tic = time.time() # start the timer\n",
        "        correct = 0\n",
        "        cumulative_loss = 0\n",
        "        incorrect = 0\n",
        "\n",
        "        for i, instance in enumerate(train_data): # train_text_reviews \n",
        "            # get the training data\n",
        "            label = train_text_labels[i] # get the label of the corresponding instace\n",
        "            label = Variable(torch.FloatTensor([label])).resize_((1,1)).to('cuda:0') # wraps a tensor for label\n",
        "\n",
        "            bow.zero_grad() # reset the gradient for each instance\n",
        "\n",
        "            pred = bow.forward(instance)\n",
        "\n",
        "            loss = loss_function(pred, label) # compute the loss\n",
        "            loss.backward() # backprop the loss\n",
        "            opt.step() # performs parameter updation based on the current gradient\n",
        "\n",
        "            cumulative_loss += float(loss.item()) # accumulate the loss over whole training sample\n",
        "\n",
        "            pred_class = 1 if pred.item() > 0.5 else 0 # as sigmoid activated\n",
        "            if(int(label) == pred_class): # counting correct prediction in each epoch\n",
        "                correct += 1\n",
        "\n",
        "        train_loss = float(cumulative_loss)/num_train_data\n",
        "        train_accuracy = correct*float(100)/num_train_data\n",
        "\n",
        "#         val_loss, val_accuracy, p, r, f = evaluate_validation_accuracy(val_data, bow) # test val-data-set on currently trained model\n",
        "        val_loss = evaluate_validation_accuracy(val_data, bow) # test val-data-set on currently trained model\n",
        "\n",
        "        if(prev_val_loss-val_loss < 0.0): # early stop if nearly no change\n",
        "            bow = copy.deepcopy(early_stop_bow)\n",
        "            break\n",
        "\n",
        "        early_stop_bow = copy.deepcopy(bow)\n",
        "\n",
        "        prev_val_loss = val_loss\n",
        "\n",
        "        toc = time.time() # final time\n",
        "        total_time += (toc-tic)\n",
        "\n",
        "        print(\"Epoch {}/{}\\n[On Training] ==> Time: {:.2f}s, Train Loss: {:.9f}, Train Accuracy: {:.2f}%\".format(e+1, epochs, (toc-tic), train_loss, train_accuracy))\n",
        "#         print(\"[On Validation] ==> Precision: {:.3f}, Recall: {:.3f}, F-Score: {:.3f}, Val loss: {:.9f}, Val Accuracy: {:.2f}% Total Time: {:.2f}s\".format(p, r, f, val_loss, val_accuracy, total_time))\n",
        "        print(\"[On Validation] ==> Val loss: {:.9f}, Total Time: {:.2f}s\".format(val_loss, total_time))\n",
        "\n",
        "    torch.save(bow, task_name)\n",
        "    \n",
        "    print('--- AFTER TRAINING ---\\tModel: {}'.format(task_name))\n",
        "    correct = 0\n",
        "    tic = time.time()\n",
        "    for i, instance in enumerate(data_test):\n",
        "        label = test_text_labels[i] # get the label of the corresponding instace\n",
        "        label = Variable(torch.FloatTensor([label])).resize_((1,1)).to('cuda:0') # wraps a tensor for label\n",
        "\n",
        "        pred = bow.forward(instance)\n",
        "        pred_class = 1 if pred.item() > 0.5 else 0 # sigmoid activated\n",
        "\n",
        "        if(int(label) == pred_class):\n",
        "            correct += 1\n",
        "    toc = time.time()\n",
        "    print(\"[On Testing] Time: {}, Test Accuracy: {:.2f}%\".format((toc-tic), correct*100/len(data_test)))\n",
        "\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "[On Training] ==> Time: 31.26s, Train Loss: 0.691956612, Train Accuracy: 57.37%\n",
            "[On Validation] ==> Val loss: 0.690812181, Total Time: 31.26s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 31.16s, Train Loss: 0.688779979, Train Accuracy: 66.11%\n",
            "[On Validation] ==> Val loss: 0.686643423, Total Time: 62.42s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 30.68s, Train Loss: 0.684310919, Train Accuracy: 70.87%\n",
            "[On Validation] ==> Val loss: 0.681621355, Total Time: 93.11s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 30.35s, Train Loss: 0.678987091, Train Accuracy: 72.14%\n",
            "[On Validation] ==> Val loss: 0.675607632, Total Time: 123.45s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 31.22s, Train Loss: 0.671137377, Train Accuracy: 74.06%\n",
            "[On Validation] ==> Val loss: 0.666980039, Total Time: 154.68s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 30.87s, Train Loss: 0.659777985, Train Accuracy: 75.70%\n",
            "[On Validation] ==> Val loss: 0.651878908, Total Time: 185.54s\n",
            "Epoch 7/100\n",
            "[On Training] ==> Time: 30.40s, Train Loss: 0.641804253, Train Accuracy: 77.37%\n",
            "[On Validation] ==> Val loss: 0.629889480, Total Time: 215.94s\n",
            "Epoch 8/100\n",
            "[On Training] ==> Time: 31.56s, Train Loss: 0.612679697, Train Accuracy: 78.25%\n",
            "[On Validation] ==> Val loss: 0.595531322, Total Time: 247.50s\n",
            "Epoch 9/100\n",
            "[On Training] ==> Time: 31.59s, Train Loss: 0.567654939, Train Accuracy: 79.90%\n",
            "[On Validation] ==> Val loss: 0.543272061, Total Time: 279.09s\n",
            "Epoch 10/100\n",
            "[On Training] ==> Time: 30.46s, Train Loss: 0.507202321, Train Accuracy: 81.44%\n",
            "[On Validation] ==> Val loss: 0.476844808, Total Time: 309.55s\n",
            "Epoch 11/100\n",
            "[On Training] ==> Time: 31.22s, Train Loss: 0.447646662, Train Accuracy: 82.97%\n",
            "[On Validation] ==> Val loss: 0.436323833, Total Time: 340.76s\n",
            "Epoch 12/100\n",
            "[On Training] ==> Time: 30.92s, Train Loss: 0.399006546, Train Accuracy: 84.57%\n",
            "[On Validation] ==> Val loss: 0.410168646, Total Time: 371.68s\n",
            "Epoch 13/100\n",
            "[On Training] ==> Time: 30.64s, Train Loss: 0.359220055, Train Accuracy: 86.28%\n",
            "[On Validation] ==> Val loss: 0.377428829, Total Time: 402.31s\n",
            "Epoch 14/100\n",
            "[On Training] ==> Time: 31.15s, Train Loss: 0.329084670, Train Accuracy: 87.61%\n",
            "[On Validation] ==> Val loss: 0.367022825, Total Time: 433.46s\n",
            "Epoch 15/100\n",
            "[On Training] ==> Time: 30.68s, Train Loss: 0.299627462, Train Accuracy: 88.91%\n",
            "[On Validation] ==> Val loss: 0.346482286, Total Time: 464.15s\n",
            "Epoch 16/100\n",
            "[On Training] ==> Time: 30.35s, Train Loss: 0.274341534, Train Accuracy: 89.83%\n",
            "[On Validation] ==> Val loss: 0.339751292, Total Time: 494.49s\n",
            "Epoch 17/100\n",
            "[On Training] ==> Time: 31.05s, Train Loss: 0.253017328, Train Accuracy: 90.86%\n",
            "[On Validation] ==> Val loss: 0.338958980, Total Time: 525.55s\n",
            "Epoch 18/100\n",
            "[On Training] ==> Time: 31.28s, Train Loss: 0.234126147, Train Accuracy: 91.84%\n",
            "[On Validation] ==> Val loss: 0.333139089, Total Time: 556.83s\n",
            "Epoch 19/100\n",
            "[On Training] ==> Time: 31.06s, Train Loss: 0.218058473, Train Accuracy: 92.54%\n",
            "[On Validation] ==> Val loss: 0.323233687, Total Time: 587.88s\n",
            "Epoch 20/100\n",
            "[On Training] ==> Time: 31.16s, Train Loss: 0.201244482, Train Accuracy: 93.02%\n",
            "[On Validation] ==> Val loss: 0.318901244, Total Time: 619.04s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type BOWClassifier. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--- AFTER TRAINING ---\tModel: Task5A.mdl\n",
            "[On Testing] Time: 2.8744962215423584, Test Accuracy: 87.68%\n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "[On Training] ==> Time: 29.95s, Train Loss: 0.687514751, Train Accuracy: 61.52%\n",
            "[On Validation] ==> Val loss: 0.682310814, Total Time: 29.95s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 31.47s, Train Loss: 0.675133548, Train Accuracy: 69.26%\n",
            "[On Validation] ==> Val loss: 0.667065554, Total Time: 61.42s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 31.34s, Train Loss: 0.655745639, Train Accuracy: 72.60%\n",
            "[On Validation] ==> Val loss: 0.641544085, Total Time: 92.76s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 30.99s, Train Loss: 0.622425545, Train Accuracy: 75.11%\n",
            "[On Validation] ==> Val loss: 0.598505884, Total Time: 123.76s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 31.44s, Train Loss: 0.567249865, Train Accuracy: 78.38%\n",
            "[On Validation] ==> Val loss: 0.534629848, Total Time: 155.20s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 30.80s, Train Loss: 0.498269229, Train Accuracy: 80.74%\n",
            "[On Validation] ==> Val loss: 0.468169713, Total Time: 185.99s\n",
            "Epoch 7/100\n",
            "[On Training] ==> Time: 30.99s, Train Loss: 0.434764820, Train Accuracy: 83.15%\n",
            "[On Validation] ==> Val loss: 0.415832258, Total Time: 216.98s\n",
            "Epoch 8/100\n",
            "[On Training] ==> Time: 32.13s, Train Loss: 0.385756828, Train Accuracy: 84.95%\n",
            "[On Validation] ==> Val loss: 0.381259687, Total Time: 249.12s\n",
            "Epoch 9/100\n",
            "[On Training] ==> Time: 31.09s, Train Loss: 0.347215066, Train Accuracy: 86.56%\n",
            "[On Validation] ==> Val loss: 0.358421436, Total Time: 280.21s\n",
            "Epoch 10/100\n",
            "[On Training] ==> Time: 31.25s, Train Loss: 0.313664456, Train Accuracy: 88.21%\n",
            "[On Validation] ==> Val loss: 0.341921071, Total Time: 311.46s\n",
            "Epoch 11/100\n",
            "[On Training] ==> Time: 31.82s, Train Loss: 0.285609157, Train Accuracy: 89.41%\n",
            "[On Validation] ==> Val loss: 0.328870285, Total Time: 343.28s\n",
            "Epoch 12/100\n",
            "[On Training] ==> Time: 31.62s, Train Loss: 0.260597799, Train Accuracy: 90.71%\n",
            "[On Validation] ==> Val loss: 0.322399414, Total Time: 374.90s\n",
            "Epoch 13/100\n",
            "[On Training] ==> Time: 31.14s, Train Loss: 0.239333802, Train Accuracy: 91.67%\n",
            "[On Validation] ==> Val loss: 0.316891547, Total Time: 406.04s\n",
            "Epoch 14/100\n",
            "[On Training] ==> Time: 31.19s, Train Loss: 0.220315052, Train Accuracy: 92.42%\n",
            "[On Validation] ==> Val loss: 0.313163760, Total Time: 437.23s\n",
            "Epoch 15/100\n",
            "[On Training] ==> Time: 31.56s, Train Loss: 0.202548057, Train Accuracy: 93.19%\n",
            "[On Validation] ==> Val loss: 0.311971316, Total Time: 468.79s\n",
            "Epoch 16/100\n",
            "[On Training] ==> Time: 31.06s, Train Loss: 0.187634261, Train Accuracy: 93.79%\n",
            "[On Validation] ==> Val loss: 0.306851833, Total Time: 499.85s\n",
            "--- AFTER TRAINING ---\tModel: Task5B.mdl\n",
            "[On Testing] Time: 2.9175961017608643, Test Accuracy: 88.50%\n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "[On Training] ==> Time: 32.08s, Train Loss: 0.694528942, Train Accuracy: 50.41%\n",
            "[On Validation] ==> Val loss: 0.694842055, Total Time: 32.08s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 31.17s, Train Loss: 0.694182141, Train Accuracy: 50.45%\n",
            "[On Validation] ==> Val loss: 0.694518362, Total Time: 63.25s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 30.73s, Train Loss: 0.693649169, Train Accuracy: 50.79%\n",
            "[On Validation] ==> Val loss: 0.693650937, Total Time: 93.98s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 31.34s, Train Loss: 0.691804109, Train Accuracy: 52.17%\n",
            "[On Validation] ==> Val loss: 0.689556982, Total Time: 125.32s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 31.03s, Train Loss: 0.657672781, Train Accuracy: 63.49%\n",
            "[On Validation] ==> Val loss: 0.546454152, Total Time: 156.35s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 31.07s, Train Loss: 0.389206879, Train Accuracy: 83.62%\n",
            "[On Validation] ==> Val loss: 0.329917067, Total Time: 187.42s\n",
            "Epoch 7/100\n",
            "[On Training] ==> Time: 31.61s, Train Loss: 0.266469304, Train Accuracy: 89.52%\n",
            "[On Validation] ==> Val loss: 0.326456559, Total Time: 219.03s\n",
            "--- AFTER TRAINING ---\tModel: Task5C.mdl\n",
            "[On Testing] Time: 2.9045286178588867, Test Accuracy: 87.62%\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lB4CfOgdF6l4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download('Task5A.mdl')\n",
        "files.download('Task5B.mdl')\n",
        "files.download('Task5C.mdl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-gTTJ8yE1o9F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Task-6"
      ]
    },
    {
      "metadata": {
        "id": "3EitRXhg_Q4p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def bigram_build_vocabulary(sentences):\n",
        "    # Build bigram vocabulary\n",
        "    bigrams_dictWordCount = {}\n",
        "    for sent in sentences:\n",
        "        for i, word in enumerate(sent.split()):\n",
        "            if (i < len(sent.split())-1):\n",
        "                bigrams_dictWordCount[word+' '+sent.split()[i+1]] = 0\n",
        "\n",
        "    for sent in sentences:\n",
        "        for i, word in enumerate(sent.split()):\n",
        "            if (i < len(sent.split())-1):\n",
        "                bigrams_dictWordCount[word+' '+sent.split()[i+1]] += 1\n",
        "    max = 0\n",
        "    temp = bigrams_dictWordCount.copy()\n",
        "    for key, val in temp.items():\n",
        "        if(bigrams_dictWordCount[key] <= 8):\n",
        "            del bigrams_dictWordCount[key]\n",
        "\n",
        "    # Mapping from index to word\n",
        "    vocabulary_inv = sorted(bigrams_dictWordCount, key=bigrams_dictWordCount.__getitem__, reverse=True)\n",
        "    \n",
        "    # Mapping from word to index\n",
        "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
        "    return vocabulary, vocabulary_inv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t2ywCp1R_WlD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text = []\n",
        "for row in train_text_reviews+val_text_reviews+test_text_reviews:\n",
        "    text.append(' '.join(row))\n",
        "\n",
        "bigram_word_to_ix, bigram_ix_to_word = bigram_build_vocabulary(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7J2XQOoF_a9g",
        "colab_type": "code",
        "outputId": "79b2913d-14a7-4139-e723-4e22ded6db68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "BIGRAM_VOCAB_SIZE = len(bigram_word_to_ix)\n",
        "bigram_ix_to_word[bigram_word_to_ix['worth watching']]=='worth watching', bigram_word_to_ix['worth watching'], BIGRAM_VOCAB_SIZE"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, 82, 26519)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "W6xGAkhM_cHp",
        "colab_type": "code",
        "outputId": "94c3e14d-bb00-410c-9774-0836eaaf0840",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1689"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "metadata": {
        "id": "hmF5hYmn1ome",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def bigram_make_bow_vector(sentence, bigram_word_to_ix):\n",
        "    # create a vector of zeros of vocab size = len(word_to_idx)\n",
        "    vec = torch.zeros(len(bigram_word_to_ix)).to('cuda:0') #, device=device) # make 1D vector of len = vocab size\n",
        "    for word in sentence:\n",
        "        if word not in bigram_word_to_ix:\n",
        "#             raise ValueError('Word',word,' not present in the dictionary. Sorry!')\n",
        "            pass\n",
        "        else:\n",
        "            vec[bigram_word_to_ix[word]]+=1 # count the number of occurance of same word in a sentences\n",
        "            \n",
        "    return vec.view(1, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "40_81ABnQ4vQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Prepare train bigrams"
      ]
    },
    {
      "metadata": {
        "id": "1g9masLZO4Nq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "for row in train_text_reviews:\n",
        "    sentences.append(' '.join(row))\n",
        "\n",
        "train_bigram = [[word+' '+sent.split()[i+1] for i, word in enumerate(sent.split()) if i < len(sent.split())-1] for sent in sentences]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vMMQlUk4TAth",
        "colab_type": "code",
        "outputId": "28e21040-cf58-4cf5-9520-261dacc71f52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# store the bag of word vectors for each sentences and wraping to tensor of torch type\n",
        "tic = time.time()\n",
        "train_data_bigram = [Variable(bigram_make_bow_vector(instance, bigram_word_to_ix)).to('cuda:0') for instance in train_bigram]\n",
        "num_train_data_bigram = len(train_bigram)\n",
        "toc = time.time()\n",
        "num_train_data_bigram, (toc-tic)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16198, 25.883325338363647)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "metadata": {
        "id": "2QZs8ymoQ-Ja",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Prepare validation bigrams"
      ]
    },
    {
      "metadata": {
        "id": "1ZegD4DJQ9fM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "for row in val_text_reviews:\n",
        "    sentences.append(' '.join(row))\n",
        "\n",
        "val_bigram = [[word+' '+sent.split()[i+1] for i, word in enumerate(sent.split()) if i < len(sent.split())-1] for sent in sentences]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "konOkGFXR5j6",
        "colab_type": "code",
        "outputId": "48d36089-8224-4baf-e8d1-eedbaa7d9f4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# store the bag of word vectors for each sentences and wraping to tensor of torch type\n",
        "tic = time.time()\n",
        "val_data_bigram = [Variable(bigram_make_bow_vector(instance, bigram_word_to_ix)).to('cuda:0') for instance in val_bigram]\n",
        "num_val_data_bigram = len(val_bigram)\n",
        "toc = time.time()\n",
        "num_val_data_bigram, (toc-tic)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1801, 2.9647693634033203)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "metadata": {
        "id": "fadh-MUnRCs6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Prepare test bigrams"
      ]
    },
    {
      "metadata": {
        "id": "gFlO2awDOPkr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "for row in test_text_reviews:\n",
        "    sentences.append(' '.join(row))\n",
        "\n",
        "test_bigrams = [[word+' '+sent.split()[i+1] for i, word in enumerate(sent.split()) if i < len(sent.split())-1] for sent in sentences]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ip_DcyJ8TQ0y",
        "colab_type": "code",
        "outputId": "81efc131-91a5-4817-f212-c58e1f7a5a53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# store the bag of word vectors for each sentences and wraping to tensor of torch type\n",
        "tic = time.time()\n",
        "test_data_bigram = [Variable(bigram_make_bow_vector(instance, bigram_word_to_ix)).to('cuda:0') for instance in test_bigrams]\n",
        "num_test_data_bigram = len(test_bigrams)\n",
        "toc = time.time()\n",
        "num_test_data_bigram, (toc-tic)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 8.128740310668945)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "metadata": {
        "id": "PUx2DyMOfJ7L",
        "colab_type": "code",
        "outputId": "79038e25-9417-4c37-b009-c79c040485b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "metadata": {
        "id": "wMR3gc2TTSHh",
        "colab_type": "code",
        "outputId": "41749367-6df6-43a5-d7c0-a39f4d453e13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "cell_type": "code",
      "source": [
        "task_name = 'Task6.mdl'\n",
        "class BOWClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
        "        super(BOWClassifier, self).__init__()\n",
        "        SEED = 42\n",
        "        torch.manual_seed(SEED)\n",
        "        torch.cuda.manual_seed(SEED)\n",
        "        \n",
        "        self.drop_out1 = nn.Dropout(p=0.1) # Adding dropout layer\n",
        "        self.drop_out2 = nn.Dropout(p=0.1) # Adding dropout layer\n",
        "        self.drop_out3 = nn.Dropout(p=0.0) # Adding dropout layer\n",
        "        \n",
        "        self.i2h1 = nn.Linear(input_size, hidden_size1) # initialises weights and biases i2h1\n",
        "        self.h12h2 = nn.Linear(hidden_size1, hidden_size2) # initialises weights and biases h12h2\n",
        "        self.h22h3 = nn.Linear(hidden_size2, hidden_size3) # initialises weights and biases h22h3\n",
        "        self.h32o = nn.Linear(hidden_size3, output_size) # initialises weights and biases h32o\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.drop_out1(self.i2h1(x))) # relu activation @ hidden layer\n",
        "        x = torch.relu(self.drop_out2(self.h12h2(x))) # relu activation @ hidden layer\n",
        "        x = torch.relu(self.drop_out3(self.h22h3(x))) # relu activation @ hidden layer\n",
        "        x = torch.sigmoid(self.h32o(x)) # sigmoid activation @ output layer\n",
        "        return x\n",
        "\n",
        "num_of_input = BIGRAM_VOCAB_SIZE # Bag word restriction has to be equal to vocabulary\n",
        "num_of_hidden1 = 30\n",
        "num_of_hidden2 = 20\n",
        "num_of_hidden3 = 10\n",
        "num_of_output = 1 # binary sentiment classes (+ve, -ve)\n",
        "\n",
        "bow = BOWClassifier(num_of_input, num_of_hidden1, num_of_hidden2, num_of_hidden3, num_of_output).to('cuda:0') # initialises weights and biases\n",
        "# define a loss function and an optimizer\n",
        "loss_function = nn.BCELoss()\n",
        "opt = torch.optim.SGD(bow.parameters(), lr = 0.001)\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "# Train The Model\n",
        "epochs = 100\n",
        "# the training loop\n",
        "total_time = 0.0\n",
        "prev_val_loss = float('inf')\n",
        "val_loss = 0\n",
        "early_stop_bow = 0\n",
        "\n",
        "for e in range(epochs):\n",
        "    tic = time.time() # start the timer\n",
        "    correct = 0\n",
        "    cumulative_loss = 0\n",
        "    incorrect = 0\n",
        "\n",
        "    for i, instance in enumerate(train_data_bigram): # train_text_reviews \n",
        "        # get the training data\n",
        "        label = train_text_labels[i] # get the label of the corresponding instace\n",
        "        label = Variable(torch.FloatTensor([label])).resize_((1,1)).to('cuda:0') # wraps a tensor for label\n",
        "\n",
        "        bow.zero_grad() # reset the gradient for each instance\n",
        "\n",
        "        pred = bow.forward(instance)\n",
        "\n",
        "        loss = loss_function(pred, label) # compute the loss\n",
        "        loss.backward() # backprop the loss\n",
        "        opt.step() # performs parameter updation based on the current gradient\n",
        "\n",
        "        cumulative_loss += float(loss.item()) # accumulate the loss over whole training sample\n",
        "\n",
        "        pred_class = 1 if pred.item() > 0.5 else 0 # as sigmoid activated\n",
        "        if(int(label) == pred_class): # counting correct prediction in each epoch\n",
        "            correct += 1\n",
        "\n",
        "    train_loss = float(cumulative_loss)/num_train_data_bigram\n",
        "    train_accuracy = correct*float(100)/num_train_data_bigram\n",
        "\n",
        "#         val_loss, val_accuracy, p, r, f = evaluate_validation_accuracy(val_data, bow) # test val-data-set on currently trained model\n",
        "    val_loss = evaluate_validation_accuracy(val_data_bigram, bow) # test val-data-set on currently trained model\n",
        "\n",
        "    if(prev_val_loss-val_loss < 0.0): # early stop if nearly no change\n",
        "        bow = copy.deepcopy(early_stop_bow)\n",
        "        break\n",
        "\n",
        "    early_stop_bow = copy.deepcopy(bow)\n",
        "\n",
        "    prev_val_loss = val_loss\n",
        "\n",
        "    toc = time.time() # final time\n",
        "    total_time += (toc-tic)\n",
        "\n",
        "    print(\"Epoch {}/{}\\n[On Training] ==> Time: {:.2f}s, Train Loss: {:.9f}, Train Accuracy: {:.2f}%\".format(e+1, epochs, (toc-tic), train_loss, train_accuracy))\n",
        "#         print(\"[On Validation] ==> Precision: {:.3f}, Recall: {:.3f}, F-Score: {:.3f}, Val loss: {:.9f}, Val Accuracy: {:.2f}% Total Time: {:.2f}s\".format(p, r, f, val_loss, val_accuracy, total_time))\n",
        "    print(\"[On Validation] ==> Val loss: {:.9f}, Total Time: {:.2f}s\".format(val_loss, total_time))\n",
        "\n",
        "torch.save(bow, task_name)\n",
        "\n",
        "print('--- AFTER TRAINING ---\\tModel: {}'.format(task_name))\n",
        "correct = 0\n",
        "tic = time.time()\n",
        "for i, instance in enumerate(test_data_bigram):\n",
        "    label = test_text_labels[i] # get the label of the corresponding instace\n",
        "    label = Variable(torch.FloatTensor([label])).resize_((1,1)).to('cuda:0') # wraps a tensor for label\n",
        "\n",
        "    pred = bow.forward(instance)\n",
        "    pred_class = 1 if pred.item() > 0.5 else 0 # sigmoid activated\n",
        "\n",
        "    if(int(label) == pred_class):\n",
        "        correct += 1\n",
        "\n",
        "toc = time.time()\n",
        "print(\"[On Testing] Time: {}, Test Accuracy: {:.2f}%\".format((toc-tic), correct*100/len(data_test)))\n",
        "\n",
        "print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "[On Training] ==> Time: 33.23s, Train Loss: 0.694220629, Train Accuracy: 50.05%\n",
            "[On Validation] ==> Val loss: 0.693000334, Total Time: 33.23s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 33.16s, Train Loss: 0.692630878, Train Accuracy: 52.07%\n",
            "[On Validation] ==> Val loss: 0.692187153, Total Time: 66.39s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 32.71s, Train Loss: 0.690838117, Train Accuracy: 55.86%\n",
            "[On Validation] ==> Val loss: 0.689141003, Total Time: 99.09s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 32.35s, Train Loss: 0.683250456, Train Accuracy: 65.16%\n",
            "[On Validation] ==> Val loss: 0.673008703, Total Time: 131.44s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 32.54s, Train Loss: 0.614814503, Train Accuracy: 74.14%\n",
            "[On Validation] ==> Val loss: 0.521644103, Total Time: 163.99s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 32.96s, Train Loss: 0.393266442, Train Accuracy: 83.21%\n",
            "[On Validation] ==> Val loss: 0.374230266, Total Time: 196.95s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type BOWClassifier. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--- AFTER TRAINING ---\tModel: Task6.mdl\n",
            "[On Testing] Time: 3.017899751663208, Test Accuracy: 82.04%\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pzFRIWc2OPVS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download('Task6.mdl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vP_IAo2PXbjX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.save('bigram_vocab.npy', bigram_word_to_ix)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"bigram_vocab.npy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3b0G-yVN1sGe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Task-7"
      ]
    },
    {
      "metadata": {
        "id": "aZ-uxYYw1t8z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "task7 = [('ce', 'Task7A.mdl'), ('mse', 'Task7B.mdl'), ('hinge', 'Task7C.mdl')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jWoJ_I333ACf",
        "colab_type": "code",
        "outputId": "33d9e1aa-b1ec-4c5b-d80e-73c1c9d22a16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7058
        }
      },
      "cell_type": "code",
      "source": [
        "for loss_fun, task_name in task7:\n",
        "    if(loss_fun == 'ce'):\n",
        "        class BOWClassifier(nn.Module):\n",
        "            def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
        "                super(BOWClassifier, self).__init__()\n",
        "                SEED = 42\n",
        "                torch.manual_seed(SEED)\n",
        "                torch.cuda.manual_seed(SEED)\n",
        "                \n",
        "                self.i2h1 = nn.Linear(input_size, hidden_size1) # initialises weights and biases i2h1\n",
        "                self.h12h2 = nn.Linear(hidden_size1, hidden_size2) # initialises weights and biases h12h2\n",
        "                self.h22h3 = nn.Linear(hidden_size2, hidden_size3) # initialises weights and biases h22h3\n",
        "                self.h32o = nn.Linear(hidden_size3, output_size) # initialises weights and biases h32o\n",
        "\n",
        "            def forward(self, x):\n",
        "                x = torch.relu(self.i2h1(x)) # relu activation @ hidden layer\n",
        "                x = torch.relu(self.h12h2(x)) # relu activation @ hidden layer\n",
        "                x = torch.relu(self.h22h3(x)) # relu activation @ hidden layer\n",
        "                x = torch.sigmoid(self.h32o(x)) # sigmoid activation @ output layer\n",
        "                return x\n",
        "\n",
        "        num_of_input = VOCAB_SIZE # Bag word restriction has to be equal to vocabulary\n",
        "        num_of_hidden1 = 30\n",
        "        num_of_hidden2 = 20\n",
        "        num_of_hidden3 = 10\n",
        "        num_of_output = 1 # binary sentiment classes (+ve, -ve)\n",
        "\n",
        "        bow = BOWClassifier(num_of_input, num_of_hidden1, num_of_hidden2, num_of_hidden3, num_of_output).to('cuda:0') # initialises weights and biases\n",
        "        \n",
        "        # define a loss function and \n",
        "        loss_function = nn.BCELoss()\n",
        "        \n",
        "    if(loss_fun == 'mse'):\n",
        "        class BOWClassifier(nn.Module):\n",
        "            def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
        "                super(BOWClassifier, self).__init__()\n",
        "                SEED = 42\n",
        "                torch.manual_seed(SEED)\n",
        "                torch.cuda.manual_seed(SEED)\n",
        "                \n",
        "                self.i2h1 = nn.Linear(input_size, hidden_size1) # initialises weights and biases i2h1\n",
        "                self.h12h2 = nn.Linear(hidden_size1, hidden_size2) # initialises weights and biases h12h2\n",
        "                self.h22h3 = nn.Linear(hidden_size2, hidden_size3) # initialises weights and biases h22h3\n",
        "                self.h32o = nn.Linear(hidden_size3, output_size) # initialises weights and biases h32o\n",
        "\n",
        "            def forward(self, x):\n",
        "                x = torch.relu(self.i2h1(x)) # relu activation @ hidden layer\n",
        "                x = torch.relu(self.h12h2(x)) # relu activation @ hidden layer\n",
        "                x = torch.relu(self.h22h3(x)) # relu activation @ hidden layer\n",
        "                x = torch.sigmoid(self.h32o(x)) # sigmoid activation @ output layer\n",
        "                return x\n",
        "\n",
        "        num_of_input = VOCAB_SIZE # Bag word restriction has to be equal to vocabulary\n",
        "        num_of_hidden1 = 30\n",
        "        num_of_hidden2 = 20\n",
        "        num_of_hidden3 = 10\n",
        "        num_of_output = 1 # binary sentiment classes (+ve, -ve)\n",
        "\n",
        "        bow = BOWClassifier(num_of_input, num_of_hidden1, num_of_hidden2, num_of_hidden3, num_of_output).to('cuda:0') # initialises weights and biases\n",
        "        \n",
        "        # define a loss function and \n",
        "        loss_function = nn.MSELoss()\n",
        "        \n",
        "    if(loss_fun == 'hinge'):\n",
        "        class BOWClassifier(nn.Module):\n",
        "            def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
        "                super(BOWClassifier, self).__init__()\n",
        "                SEED = 42\n",
        "                torch.manual_seed(SEED)\n",
        "                torch.cuda.manual_seed(SEED)\n",
        "                \n",
        "                self.i2h1 = nn.Linear(input_size, hidden_size1) # initialises weights and biases i2h1\n",
        "                self.h12h2 = nn.Linear(hidden_size1, hidden_size2) # initialises weights and biases h12h2\n",
        "                self.h22h3 = nn.Linear(hidden_size2, hidden_size3) # initialises weights and biases h22h3\n",
        "                self.h32o = nn.Linear(hidden_size3, output_size) # initialises weights and biases h32o\n",
        "\n",
        "            def forward(self, x):\n",
        "                x = torch.relu(self.i2h1(x)) # relu activation @ hidden layer\n",
        "                x = torch.relu(self.h12h2(x)) # relu activation @ hidden layer\n",
        "                x = torch.relu(self.h22h3(x)) # relu activation @ hidden layer\n",
        "                x = torch.sigmoid(self.h32o(x)) # sigmoid activation @ output layer\n",
        "                return x\n",
        "\n",
        "        num_of_input = VOCAB_SIZE # Bag word restriction has to be equal to vocabulary\n",
        "        num_of_hidden1 = 30\n",
        "        num_of_hidden2 = 20\n",
        "        num_of_hidden3 = 10\n",
        "        num_of_output = 1 # binary sentiment classes (+ve, -ve)\n",
        "\n",
        "        bow = BOWClassifier(num_of_input, num_of_hidden1, num_of_hidden2, num_of_hidden3, num_of_output).to('cuda:0') # initialises weights and biases\n",
        "        \n",
        "        # define a loss function and \n",
        "        loss_function = nn.HingeEmbeddingLoss()\n",
        "        \n",
        "    # define an optimizer\n",
        "    if(loss_fun == 'ce'):\n",
        "        opt = torch.optim.SGD(bow.parameters(), lr = 0.0001)\n",
        "    elif(loss_fun == 'mse'):\n",
        "        opt = torch.optim.SGD(bow.parameters(), lr = 0.0003)\n",
        "    elif(loss_fun == 'hinge'):\n",
        "        opt = torch.optim.SGD(bow.parameters(), lr = 0.01)\n",
        "    gc.collect()\n",
        "\n",
        "    # Train The Model\n",
        "    epochs = 100\n",
        "    # the training loop\n",
        "    total_time = 0.0\n",
        "    prev_val_loss = float('inf')\n",
        "    val_loss = 0\n",
        "    early_stop_bow = 0\n",
        "\n",
        "    for e in range(epochs):\n",
        "        tic = time.time() # start the timer\n",
        "        correct = 0\n",
        "        cumulative_loss = 0\n",
        "        incorrect = 0\n",
        "\n",
        "        for i, instance in enumerate(train_data): # train_text_reviews \n",
        "            # get the training data\n",
        "            label = train_text_labels[i] # get the label of the corresponding instace\n",
        "            label = Variable(torch.FloatTensor([label])).resize_((1,1)).to('cuda:0') # wraps a tensor for label\n",
        "\n",
        "            bow.zero_grad() # reset the gradient for each instance\n",
        "\n",
        "            pred = bow.forward(instance)\n",
        "\n",
        "            loss = loss_function(pred, label) # compute the loss\n",
        "            loss.backward() # backprop the loss\n",
        "            opt.step() # performs parameter updation based on the current gradient\n",
        "\n",
        "            cumulative_loss += float(loss.item()) # accumulate the loss over whole training sample\n",
        "\n",
        "            pred_class = 1 if pred.item() > 0.5 else 0 # as sigmoid activated\n",
        "            if(int(label) == pred_class): # counting correct prediction in each epoch\n",
        "                correct += 1\n",
        "\n",
        "        train_loss = float(cumulative_loss)/num_train_data\n",
        "        train_accuracy = correct*float(100)/num_train_data\n",
        "\n",
        "#         val_loss, val_accuracy, p, r, f = evaluate_validation_accuracy(val_data, bow) # test val-data-set on currently trained model\n",
        "        val_loss = evaluate_validation_accuracy(val_data, bow) # test val-data-set on currently trained model\n",
        "\n",
        "        if(prev_val_loss-val_loss < 0.0): # early stop if nearly no change\n",
        "            bow = copy.deepcopy(early_stop_bow)\n",
        "            break\n",
        "\n",
        "        early_stop_bow = copy.deepcopy(bow)\n",
        "\n",
        "        prev_val_loss = val_loss\n",
        "\n",
        "        toc = time.time() # final time\n",
        "        total_time += (toc-tic)\n",
        "\n",
        "        print(\"Epoch {}/{}\\n[On Training] ==> Time: {:.2f}s, Train Loss: {:.9f}, Train Accuracy: {:.2f}%\".format(e+1, epochs, (toc-tic), train_loss, train_accuracy))\n",
        "#         print(\"[On Validation] ==> Precision: {:.3f}, Recall: {:.3f}, F-Score: {:.3f}, Val loss: {:.9f}, Val Accuracy: {:.2f}% Total Time: {:.2f}s\".format(p, r, f, val_loss, val_accuracy, total_time))\n",
        "        print(\"[On Validation] ==> Val loss: {:.9f}, Total Time: {:.2f}s\".format(val_loss, total_time))\n",
        "\n",
        "    torch.save(bow, task_name)\n",
        "    \n",
        "    print('--- AFTER TRAINING ---\\tModel: {}'.format(task_name))\n",
        "    correct = 0\n",
        "    tic = time.time()\n",
        "    for i, instance in enumerate(data_test):\n",
        "        label = test_text_labels[i] # get the label of the corresponding instace\n",
        "        label = Variable(torch.FloatTensor([label])).resize_((1,1)).to('cuda:0') # wraps a tensor for label\n",
        "\n",
        "        pred = bow.forward(instance)\n",
        "        pred_class = 1 if pred.item() > 0.5 else 0 # sigmoid activated\n",
        "\n",
        "        if(int(label) == pred_class):\n",
        "            correct += 1\n",
        "    toc = time.time()\n",
        "    print(\"[On Testing] Time: {}, Test Accuracy: {:.2f}%\".format((toc-tic), correct*100/len(data_test)))\n",
        "\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "[On Training] ==> Time: 29.76s, Train Loss: 0.691906127, Train Accuracy: 58.86%\n",
            "[On Validation] ==> Val loss: 0.690454737, Total Time: 29.76s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 30.48s, Train Loss: 0.688493054, Train Accuracy: 68.54%\n",
            "[On Validation] ==> Val loss: 0.686511477, Total Time: 60.24s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 29.72s, Train Loss: 0.683754277, Train Accuracy: 72.19%\n",
            "[On Validation] ==> Val loss: 0.680960536, Total Time: 89.96s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 29.62s, Train Loss: 0.677575701, Train Accuracy: 74.35%\n",
            "[On Validation] ==> Val loss: 0.674137801, Total Time: 119.59s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 30.13s, Train Loss: 0.669296890, Train Accuracy: 76.08%\n",
            "[On Validation] ==> Val loss: 0.664292754, Total Time: 149.72s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 29.71s, Train Loss: 0.656674199, Train Accuracy: 77.51%\n",
            "[On Validation] ==> Val loss: 0.648659062, Total Time: 179.43s\n",
            "Epoch 7/100\n",
            "[On Training] ==> Time: 29.75s, Train Loss: 0.636101073, Train Accuracy: 78.89%\n",
            "[On Validation] ==> Val loss: 0.622593309, Total Time: 209.17s\n",
            "Epoch 8/100\n",
            "[On Training] ==> Time: 29.68s, Train Loss: 0.601431664, Train Accuracy: 80.39%\n",
            "[On Validation] ==> Val loss: 0.578808622, Total Time: 238.85s\n",
            "Epoch 9/100\n",
            "[On Training] ==> Time: 29.72s, Train Loss: 0.545218395, Train Accuracy: 81.91%\n",
            "[On Validation] ==> Val loss: 0.512500974, Total Time: 268.57s\n",
            "Epoch 10/100\n",
            "[On Training] ==> Time: 30.17s, Train Loss: 0.470913620, Train Accuracy: 83.65%\n",
            "[On Validation] ==> Val loss: 0.441420767, Total Time: 298.74s\n",
            "Epoch 11/100\n",
            "[On Training] ==> Time: 30.99s, Train Loss: 0.402053860, Train Accuracy: 85.35%\n",
            "[On Validation] ==> Val loss: 0.391170602, Total Time: 329.72s\n",
            "Epoch 12/100\n",
            "[On Training] ==> Time: 30.27s, Train Loss: 0.350874941, Train Accuracy: 86.86%\n",
            "[On Validation] ==> Val loss: 0.360573441, Total Time: 360.00s\n",
            "Epoch 13/100\n",
            "[On Training] ==> Time: 29.94s, Train Loss: 0.311711769, Train Accuracy: 88.30%\n",
            "[On Validation] ==> Val loss: 0.341414308, Total Time: 389.93s\n",
            "Epoch 14/100\n",
            "[On Training] ==> Time: 29.65s, Train Loss: 0.279643598, Train Accuracy: 89.73%\n",
            "[On Validation] ==> Val loss: 0.329507230, Total Time: 419.58s\n",
            "Epoch 15/100\n",
            "[On Training] ==> Time: 29.90s, Train Loss: 0.252850710, Train Accuracy: 90.82%\n",
            "[On Validation] ==> Val loss: 0.322555599, Total Time: 449.48s\n",
            "Epoch 16/100\n",
            "[On Training] ==> Time: 29.98s, Train Loss: 0.230239891, Train Accuracy: 91.83%\n",
            "[On Validation] ==> Val loss: 0.318995657, Total Time: 479.47s\n",
            "Epoch 17/100\n",
            "[On Training] ==> Time: 29.47s, Train Loss: 0.210790504, Train Accuracy: 92.69%\n",
            "[On Validation] ==> Val loss: 0.317762711, Total Time: 508.94s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type BOWClassifier. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--- AFTER TRAINING ---\tModel: Task7A.mdl\n",
            "[On Testing] Time: 2.7122726440429688, Test Accuracy: 88.10%\n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "[On Training] ==> Time: 29.67s, Train Loss: 0.249003833, Train Accuracy: 60.96%\n",
            "[On Validation] ==> Val loss: 0.247761235, Total Time: 29.67s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 30.18s, Train Loss: 0.245982860, Train Accuracy: 71.26%\n",
            "[On Validation] ==> Val loss: 0.243954085, Total Time: 59.85s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 31.28s, Train Loss: 0.241345093, Train Accuracy: 74.56%\n",
            "[On Validation] ==> Val loss: 0.238403651, Total Time: 91.13s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 31.33s, Train Loss: 0.233724919, Train Accuracy: 77.00%\n",
            "[On Validation] ==> Val loss: 0.228189957, Total Time: 122.46s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 30.59s, Train Loss: 0.218657703, Train Accuracy: 79.18%\n",
            "[On Validation] ==> Val loss: 0.207310625, Total Time: 153.05s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 30.41s, Train Loss: 0.189368987, Train Accuracy: 81.44%\n",
            "[On Validation] ==> Val loss: 0.170642444, Total Time: 183.46s\n",
            "Epoch 7/100\n",
            "[On Training] ==> Time: 30.33s, Train Loss: 0.149805608, Train Accuracy: 83.90%\n",
            "[On Validation] ==> Val loss: 0.135894928, Total Time: 213.79s\n",
            "Epoch 8/100\n",
            "[On Training] ==> Time: 30.48s, Train Loss: 0.119812681, Train Accuracy: 85.97%\n",
            "[On Validation] ==> Val loss: 0.117102675, Total Time: 244.27s\n",
            "Epoch 9/100\n",
            "[On Training] ==> Time: 30.20s, Train Loss: 0.100953748, Train Accuracy: 87.78%\n",
            "[On Validation] ==> Val loss: 0.107051461, Total Time: 274.47s\n",
            "Epoch 10/100\n",
            "[On Training] ==> Time: 30.30s, Train Loss: 0.087535812, Train Accuracy: 89.47%\n",
            "[On Validation] ==> Val loss: 0.101165885, Total Time: 304.77s\n",
            "Epoch 11/100\n",
            "[On Training] ==> Time: 30.65s, Train Loss: 0.077158792, Train Accuracy: 90.84%\n",
            "[On Validation] ==> Val loss: 0.097566588, Total Time: 335.42s\n",
            "Epoch 12/100\n",
            "[On Training] ==> Time: 30.57s, Train Loss: 0.068788458, Train Accuracy: 91.89%\n",
            "[On Validation] ==> Val loss: 0.095245639, Total Time: 365.99s\n",
            "Epoch 13/100\n",
            "[On Training] ==> Time: 30.77s, Train Loss: 0.061811835, Train Accuracy: 92.90%\n",
            "[On Validation] ==> Val loss: 0.093595222, Total Time: 396.76s\n",
            "Epoch 14/100\n",
            "[On Training] ==> Time: 31.89s, Train Loss: 0.055835773, Train Accuracy: 93.86%\n",
            "[On Validation] ==> Val loss: 0.092499902, Total Time: 428.65s\n",
            "Epoch 15/100\n",
            "[On Training] ==> Time: 30.58s, Train Loss: 0.050625865, Train Accuracy: 94.47%\n",
            "[On Validation] ==> Val loss: 0.091948250, Total Time: 459.23s\n",
            "Epoch 16/100\n",
            "[On Training] ==> Time: 30.60s, Train Loss: 0.045972971, Train Accuracy: 95.07%\n",
            "[On Validation] ==> Val loss: 0.091821296, Total Time: 489.83s\n",
            "--- AFTER TRAINING ---\tModel: Task7B.mdl\n",
            "[On Testing] Time: 2.8071727752685547, Test Accuracy: 88.26%\n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "[On Training] ==> Time: 35.65s, Train Loss: 0.508536553, Train Accuracy: 49.96%\n",
            "[On Validation] ==> Val loss: 0.499885119, Total Time: 35.65s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 35.55s, Train Loss: 0.499784795, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499794739, Total Time: 71.19s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 35.63s, Train Loss: 0.499739855, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499768060, Total Time: 106.82s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 35.63s, Train Loss: 0.499723579, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499755473, Total Time: 142.45s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 35.46s, Train Loss: 0.499715247, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499748195, Total Time: 177.91s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 37.21s, Train Loss: 0.499710218, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499743472, Total Time: 215.13s\n",
            "Epoch 7/100\n",
            "[On Training] ==> Time: 35.34s, Train Loss: 0.499706870, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499740168, Total Time: 250.47s\n",
            "Epoch 8/100\n",
            "[On Training] ==> Time: 35.33s, Train Loss: 0.499704489, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499737733, Total Time: 285.80s\n",
            "Epoch 9/100\n",
            "[On Training] ==> Time: 35.27s, Train Loss: 0.499702713, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499735866, Total Time: 321.07s\n",
            "Epoch 10/100\n",
            "[On Training] ==> Time: 35.38s, Train Loss: 0.499701341, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499734391, Total Time: 356.45s\n",
            "Epoch 11/100\n",
            "[On Training] ==> Time: 35.25s, Train Loss: 0.499700250, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499733197, Total Time: 391.71s\n",
            "Epoch 12/100\n",
            "[On Training] ==> Time: 35.28s, Train Loss: 0.499699364, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499732212, Total Time: 426.99s\n",
            "Epoch 13/100\n",
            "[On Training] ==> Time: 35.27s, Train Loss: 0.499698630, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499731385, Total Time: 462.25s\n",
            "Epoch 14/100\n",
            "[On Training] ==> Time: 36.26s, Train Loss: 0.499698013, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499730683, Total Time: 498.51s\n",
            "Epoch 15/100\n",
            "[On Training] ==> Time: 35.62s, Train Loss: 0.499697487, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499730078, Total Time: 534.13s\n",
            "Epoch 16/100\n",
            "[On Training] ==> Time: 35.38s, Train Loss: 0.499697035, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499729553, Total Time: 569.51s\n",
            "Epoch 17/100\n",
            "[On Training] ==> Time: 35.20s, Train Loss: 0.499696641, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499729092, Total Time: 604.71s\n",
            "Epoch 18/100\n",
            "[On Training] ==> Time: 35.23s, Train Loss: 0.499696295, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499728685, Total Time: 639.94s\n",
            "Epoch 19/100\n",
            "[On Training] ==> Time: 35.18s, Train Loss: 0.499695990, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499728323, Total Time: 675.12s\n",
            "Epoch 20/100\n",
            "[On Training] ==> Time: 35.24s, Train Loss: 0.499695718, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499727999, Total Time: 710.36s\n",
            "Epoch 21/100\n",
            "[On Training] ==> Time: 35.43s, Train Loss: 0.499695475, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499727707, Total Time: 745.79s\n",
            "Epoch 22/100\n",
            "[On Training] ==> Time: 35.24s, Train Loss: 0.499695256, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499727443, Total Time: 781.04s\n",
            "Epoch 23/100\n",
            "[On Training] ==> Time: 36.80s, Train Loss: 0.499695057, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499727202, Total Time: 817.84s\n",
            "Epoch 24/100\n",
            "[On Training] ==> Time: 35.59s, Train Loss: 0.499694877, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499726983, Total Time: 853.43s\n",
            "Epoch 25/100\n",
            "[On Training] ==> Time: 35.64s, Train Loss: 0.499694713, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499726782, Total Time: 889.07s\n",
            "Epoch 26/100\n",
            "[On Training] ==> Time: 35.41s, Train Loss: 0.499694562, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499726598, Total Time: 924.48s\n",
            "Epoch 27/100\n",
            "[On Training] ==> Time: 35.40s, Train Loss: 0.499694424, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499726427, Total Time: 959.88s\n",
            "Epoch 28/100\n",
            "[On Training] ==> Time: 35.42s, Train Loss: 0.499694296, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499726270, Total Time: 995.30s\n",
            "Epoch 29/100\n",
            "[On Training] ==> Time: 35.37s, Train Loss: 0.499694178, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499726124, Total Time: 1030.67s\n",
            "Epoch 30/100\n",
            "[On Training] ==> Time: 35.30s, Train Loss: 0.499694068, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499725988, Total Time: 1065.97s\n",
            "Epoch 31/100\n",
            "[On Training] ==> Time: 35.41s, Train Loss: 0.499693966, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499725861, Total Time: 1101.37s\n",
            "Epoch 32/100\n",
            "[On Training] ==> Time: 37.14s, Train Loss: 0.499693871, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499725742, Total Time: 1138.51s\n",
            "Epoch 33/100\n",
            "[On Training] ==> Time: 36.19s, Train Loss: 0.499693783, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499725631, Total Time: 1174.70s\n",
            "Epoch 34/100\n",
            "[On Training] ==> Time: 36.27s, Train Loss: 0.499693700, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499725527, Total Time: 1210.97s\n",
            "Epoch 35/100\n",
            "[On Training] ==> Time: 35.93s, Train Loss: 0.499693622, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499725429, Total Time: 1246.90s\n",
            "Epoch 36/100\n",
            "[On Training] ==> Time: 35.82s, Train Loss: 0.499693548, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499725336, Total Time: 1282.72s\n",
            "Epoch 37/100\n",
            "[On Training] ==> Time: 35.93s, Train Loss: 0.499693480, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499725249, Total Time: 1318.65s\n",
            "Epoch 38/100\n",
            "[On Training] ==> Time: 35.67s, Train Loss: 0.499693415, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499725167, Total Time: 1354.32s\n",
            "Epoch 39/100\n",
            "[On Training] ==> Time: 35.70s, Train Loss: 0.499693353, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499725089, Total Time: 1390.02s\n",
            "Epoch 40/100\n",
            "[On Training] ==> Time: 36.79s, Train Loss: 0.499693295, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499725015, Total Time: 1426.81s\n",
            "Epoch 41/100\n",
            "[On Training] ==> Time: 36.23s, Train Loss: 0.499693240, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499724945, Total Time: 1463.04s\n",
            "Epoch 42/100\n",
            "[On Training] ==> Time: 35.54s, Train Loss: 0.499693188, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499724879, Total Time: 1498.58s\n",
            "Epoch 43/100\n",
            "[On Training] ==> Time: 36.41s, Train Loss: 0.499693138, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499724816, Total Time: 1534.99s\n",
            "Epoch 44/100\n",
            "[On Training] ==> Time: 36.04s, Train Loss: 0.499693091, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499724755, Total Time: 1571.03s\n",
            "Epoch 45/100\n",
            "[On Training] ==> Time: 36.12s, Train Loss: 0.499693046, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499724698, Total Time: 1607.15s\n",
            "Epoch 46/100\n",
            "[On Training] ==> Time: 35.89s, Train Loss: 0.499693004, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499724643, Total Time: 1643.04s\n",
            "Epoch 47/100\n",
            "[On Training] ==> Time: 36.19s, Train Loss: 0.499692963, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499724590, Total Time: 1679.23s\n",
            "Epoch 48/100\n",
            "[On Training] ==> Time: 35.71s, Train Loss: 0.499692924, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499724540, Total Time: 1714.94s\n",
            "Epoch 49/100\n",
            "[On Training] ==> Time: 37.33s, Train Loss: 0.499692887, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499724492, Total Time: 1752.27s\n",
            "Epoch 50/100\n",
            "[On Training] ==> Time: 35.59s, Train Loss: 0.499692851, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499724446, Total Time: 1787.86s\n",
            "Epoch 51/100\n",
            "[On Training] ==> Time: 35.80s, Train Loss: 0.499692817, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499724402, Total Time: 1823.66s\n",
            "Epoch 52/100\n",
            "[On Training] ==> Time: 35.39s, Train Loss: 0.499692784, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499724360, Total Time: 1859.05s\n",
            "Epoch 53/100\n",
            "[On Training] ==> Time: 35.59s, Train Loss: 0.499692753, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499724319, Total Time: 1894.64s\n",
            "Epoch 54/100\n",
            "[On Training] ==> Time: 35.10s, Train Loss: 0.499692723, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499724280, Total Time: 1929.74s\n",
            "Epoch 55/100\n",
            "[On Training] ==> Time: 35.07s, Train Loss: 0.499692694, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499724242, Total Time: 1964.82s\n",
            "Epoch 56/100\n",
            "[On Training] ==> Time: 35.24s, Train Loss: 0.499692666, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499724206, Total Time: 2000.06s\n",
            "Epoch 57/100\n",
            "[On Training] ==> Time: 35.85s, Train Loss: 0.499692639, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499724171, Total Time: 2035.90s\n",
            "Epoch 58/100\n",
            "[On Training] ==> Time: 36.41s, Train Loss: 0.499692613, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499724138, Total Time: 2072.32s\n",
            "Epoch 59/100\n",
            "[On Training] ==> Time: 34.90s, Train Loss: 0.499692589, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499724105, Total Time: 2107.22s\n",
            "Epoch 60/100\n",
            "[On Training] ==> Time: 34.81s, Train Loss: 0.499692565, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499724074, Total Time: 2142.02s\n",
            "Epoch 61/100\n",
            "[On Training] ==> Time: 35.01s, Train Loss: 0.499692542, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499724044, Total Time: 2177.04s\n",
            "Epoch 62/100\n",
            "[On Training] ==> Time: 35.51s, Train Loss: 0.499692519, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499724015, Total Time: 2212.55s\n",
            "Epoch 63/100\n",
            "[On Training] ==> Time: 35.59s, Train Loss: 0.499692498, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723986, Total Time: 2248.14s\n",
            "Epoch 64/100\n",
            "[On Training] ==> Time: 35.52s, Train Loss: 0.499692477, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723959, Total Time: 2283.66s\n",
            "Epoch 65/100\n",
            "[On Training] ==> Time: 35.62s, Train Loss: 0.499692457, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723933, Total Time: 2319.28s\n",
            "Epoch 66/100\n",
            "[On Training] ==> Time: 36.39s, Train Loss: 0.499692437, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723907, Total Time: 2355.67s\n",
            "Epoch 67/100\n",
            "[On Training] ==> Time: 36.34s, Train Loss: 0.499692418, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723882, Total Time: 2392.01s\n",
            "Epoch 68/100\n",
            "[On Training] ==> Time: 35.98s, Train Loss: 0.499692400, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723858, Total Time: 2427.99s\n",
            "Epoch 69/100\n",
            "[On Training] ==> Time: 35.77s, Train Loss: 0.499692383, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723835, Total Time: 2463.76s\n",
            "Epoch 70/100\n",
            "[On Training] ==> Time: 35.81s, Train Loss: 0.499692365, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723813, Total Time: 2499.57s\n",
            "Epoch 71/100\n",
            "[On Training] ==> Time: 35.83s, Train Loss: 0.499692349, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723791, Total Time: 2535.40s\n",
            "Epoch 72/100\n",
            "[On Training] ==> Time: 35.90s, Train Loss: 0.499692333, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723769, Total Time: 2571.29s\n",
            "Epoch 73/100\n",
            "[On Training] ==> Time: 35.86s, Train Loss: 0.499692317, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723749, Total Time: 2607.15s\n",
            "Epoch 74/100\n",
            "[On Training] ==> Time: 36.39s, Train Loss: 0.499692302, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723729, Total Time: 2643.54s\n",
            "Epoch 75/100\n",
            "[On Training] ==> Time: 36.40s, Train Loss: 0.499692287, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723709, Total Time: 2679.95s\n",
            "Epoch 76/100\n",
            "[On Training] ==> Time: 34.92s, Train Loss: 0.499692273, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723690, Total Time: 2714.86s\n",
            "Epoch 77/100\n",
            "[On Training] ==> Time: 34.84s, Train Loss: 0.499692259, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723672, Total Time: 2749.70s\n",
            "Epoch 78/100\n",
            "[On Training] ==> Time: 34.88s, Train Loss: 0.499692245, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723654, Total Time: 2784.57s\n",
            "Epoch 79/100\n",
            "[On Training] ==> Time: 34.85s, Train Loss: 0.499692232, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723636, Total Time: 2819.42s\n",
            "Epoch 80/100\n",
            "[On Training] ==> Time: 35.48s, Train Loss: 0.499692219, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723619, Total Time: 2854.90s\n",
            "Epoch 81/100\n",
            "[On Training] ==> Time: 35.62s, Train Loss: 0.499692207, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723603, Total Time: 2890.52s\n",
            "Epoch 82/100\n",
            "[On Training] ==> Time: 35.81s, Train Loss: 0.499692195, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723586, Total Time: 2926.32s\n",
            "Epoch 83/100\n",
            "[On Training] ==> Time: 36.41s, Train Loss: 0.499692183, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723571, Total Time: 2962.73s\n",
            "Epoch 84/100\n",
            "[On Training] ==> Time: 36.73s, Train Loss: 0.499692171, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723555, Total Time: 2999.46s\n",
            "Epoch 85/100\n",
            "[On Training] ==> Time: 35.52s, Train Loss: 0.499692160, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723540, Total Time: 3034.97s\n",
            "Epoch 86/100\n",
            "[On Training] ==> Time: 35.58s, Train Loss: 0.499692149, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723526, Total Time: 3070.55s\n",
            "Epoch 87/100\n",
            "[On Training] ==> Time: 35.63s, Train Loss: 0.499692138, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723511, Total Time: 3106.18s\n",
            "Epoch 88/100\n",
            "[On Training] ==> Time: 35.56s, Train Loss: 0.499692128, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723497, Total Time: 3141.74s\n",
            "Epoch 89/100\n",
            "[On Training] ==> Time: 35.44s, Train Loss: 0.499692118, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723484, Total Time: 3177.18s\n",
            "Epoch 90/100\n",
            "[On Training] ==> Time: 35.68s, Train Loss: 0.499692108, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723471, Total Time: 3212.86s\n",
            "Epoch 91/100\n",
            "[On Training] ==> Time: 35.70s, Train Loss: 0.499692098, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723458, Total Time: 3248.56s\n",
            "Epoch 92/100\n",
            "[On Training] ==> Time: 36.26s, Train Loss: 0.499692088, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723445, Total Time: 3284.82s\n",
            "Epoch 93/100\n",
            "[On Training] ==> Time: 36.63s, Train Loss: 0.499692079, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723432, Total Time: 3321.45s\n",
            "Epoch 94/100\n",
            "[On Training] ==> Time: 35.62s, Train Loss: 0.499692070, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723420, Total Time: 3357.07s\n",
            "Epoch 95/100\n",
            "[On Training] ==> Time: 35.51s, Train Loss: 0.499692061, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723408, Total Time: 3392.58s\n",
            "Epoch 96/100\n",
            "[On Training] ==> Time: 35.53s, Train Loss: 0.499692053, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723397, Total Time: 3428.11s\n",
            "Epoch 97/100\n",
            "[On Training] ==> Time: 35.60s, Train Loss: 0.499692044, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723385, Total Time: 3463.71s\n",
            "Epoch 98/100\n",
            "[On Training] ==> Time: 35.63s, Train Loss: 0.499692036, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723374, Total Time: 3499.33s\n",
            "Epoch 99/100\n",
            "[On Training] ==> Time: 35.64s, Train Loss: 0.499692028, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723363, Total Time: 3534.97s\n",
            "Epoch 100/100\n",
            "[On Training] ==> Time: 36.27s, Train Loss: 0.499692020, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.499723353, Total Time: 3571.24s\n",
            "--- AFTER TRAINING ---\tModel: Task7C.mdl\n",
            "[On Testing] Time: 2.7684483528137207, Test Accuracy: 49.64%\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "00HZEnQGGKYy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download('Task7A.mdl')\n",
        "files.download('Task7B.mdl')\n",
        "files.download('Task7C.mdl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0VkVoMzMUdeh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.save('vocab.npy', word_to_ix)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"vocab.npy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FDUyFfwmXSew",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# plt.figure(\"Image\")\n",
        "# plt.title(\"Loss vs Epoch\")\n",
        "# val_loss_history_plt =  [float(i)/sum(val_loss_history) for i in val_loss_history] # normalised between 0-1\n",
        "# train_loss_history_plt =  [float(i)/sum(train_loss_history) for i in train_loss_history] # normalised between 0-1\n",
        "# f_score_plt = [float(i)/sum(f_score) for i in f_score] # normalised between 0-1\n",
        "# plt.plot(val_loss_history_plt, c=\"red\", label=\"Validation Loss\")\n",
        "# plt.plot(train_loss_history_plt, c=\"blue\", label = \"Training Loss\")\n",
        "# plt.plot(f_score_plt, c=\"green\", label = \"F-Score\")\n",
        "# plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zz5PwYjVudEk",
        "colab_type": "code",
        "outputId": "b827a586-3ef1-4a92-8a95-3bbf68dabc54",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# temp_test = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7063ed6b-ee9a-4382-bd21-300e434055d5\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-7063ed6b-ee9a-4382-bd21-300e434055d5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Task3A.mdl to Task3A.mdl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "29twCfxpuc8r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# _model_ = torch.load(io.BytesIO(temp_test['Task2C.mdl']))\n",
        "# _model_ = torch.load('Task5C.mdl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ixy30WkrH3_-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# for params in _model_.parameters():\n",
        "#     print (params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ffYv5PAmjyp-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# print('--- AFTER TRAINING ---')\n",
        "# correct = 0\n",
        "# tic = time.time()\n",
        "# for i, instance in enumerate(data_test):\n",
        "#     label = test_text_labels[i] # get the label of the corresponding instace\n",
        "#     label = Variable(torch.FloatTensor([label])).resize_((1,1)).to('cuda:0') # wraps a tensor for label\n",
        "    \n",
        "#     pred = _model_.forward(instance)\n",
        "#     pred_class = 1 if pred.item() > 0.5 else 0 # sigmoid activated\n",
        "    \n",
        "#     if(int(label) == pred_class):\n",
        "#         correct += 1\n",
        "# toc = time.time()\n",
        "# print(\"Time: {}, Test Accuracy: {:.2f}%\".format((toc-tic), correct*100/len(data_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hBL6jT8ReP6p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# predict on test dataset\n",
        "# with open(\"Task1A.csv\", \"wb\") as _f:\n",
        "#     writer = csv.writer(_f)\n",
        "#     writer.writerows(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ub0HKgL8H37z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lCraW41hH32S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mhftDt1DH3x6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kE_TcX1KH3tQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T60wqxmoH3Zl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}