{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Assign.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajeshsahu09/CS69002_9A_18CS60R19/blob/master/DL_Assign.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "wuB0HM37teAc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Movie Review Sentiment Analysis"
      ]
    },
    {
      "metadata": {
        "id": "rKlXTA5zfx20",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Run on **GPU**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "_NV6CE-qtvGO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Import Header Files"
      ]
    },
    {
      "metadata": {
        "id": "WE7OEcPOtzJf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v4037ccUuFFp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load the Dataset and Visualise"
      ]
    },
    {
      "metadata": {
        "id": "EhWFepP2t3eA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# import io\n",
        "# df_train = pd.read_csv(io.StringIO(uploaded['Train_20K.csv'].decode('utf-8')), sep='\\t')\n",
        "# df_train.head()\n",
        "url = \"https://raw.githubusercontent.com/binny-mathew/IITKGP_CS69002_Spring_2019/master/Dataset/Train_20K.csv\"\n",
        "df = pd.read_csv(url, sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mvAXyCoDaR3b",
        "colab_type": "code",
        "outputId": "c307b36a-9291-40a9-eb23-1c508122d84e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17999, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "44YtF48vX24a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_1 = df[df['label']==1] # get only label == 1\n",
        "df_0 = df[df['label']==0] # get only label == 0\n",
        "\n",
        "range_1 = int(0.9*len(df_1))\n",
        "range_2 = int(0.9*len(df_0))\n",
        "\n",
        "np.random.seed(42) # fix the seed value\n",
        "df_1 = df_1.iloc[np.random.permutation(len(df_1))] # shuffled the data set of label==1\n",
        "df_0 = df_0.iloc[np.random.permutation(len(df_0))] # shuffled the data set of label==0\n",
        "\n",
        "temp_1_train = df_1.iloc[:range_1]\n",
        "temp_2_train = df_0.iloc[:range_2]\n",
        "df_train = pd.concat([temp_1_train, temp_2_train])\n",
        "\n",
        "temp_1_val = df_1.iloc[range_1:]\n",
        "temp_2_val = df_0.iloc[range_2:]\n",
        "df_val = pd.concat([temp_1_val, temp_2_val])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T9FBTqABuRlO",
        "colab_type": "code",
        "outputId": "3e6d2e6e-73ec-4a3f-edd3-dc0189e5a0b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "df_train = df_train.iloc[np.random.permutation(len(df_train))] # shuffled the data set of label==1\n",
        "df_val = df_val.iloc[np.random.permutation(len(df_val))] # shuffled the data set of label==1\n",
        "len(df_train), len(df_val)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16198, 1801)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "0WFB1TtsxD3F",
        "colab_type": "code",
        "outputId": "9541515d-3f60-437b-da0e-ba5f757a37b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "print('Number of Negative movie reviews', len(df_train[df_train['label']==0]))\n",
        "print('Number of Positive movie reviews', len(df_train[df_train['label']==1]))\n",
        "print('Number of movie reviews', len(df_train['label']))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Negative movie reviews 8094\n",
            "Number of Positive movie reviews 8104\n",
            "Number of movie reviews 16198\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sYtzJk8r3lm5",
        "colab_type": "code",
        "outputId": "dcda8d84-9486-4ce2-d8ca-3a664be4685d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "print('Number of Negative movie reviews', len(df_val[df_val['label']==0]))\n",
        "print('Number of Positive movie reviews', len(df_val[df_val['label']==1]))\n",
        "print('Number of movie reviews', len(df_val['label']))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Negative movie reviews 900\n",
            "Number of Positive movie reviews 901\n",
            "Number of movie reviews 1801\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uuZxOMpkQoka",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/binny-mathew/IITKGP_CS69002_Spring_2019/master/Dataset/Test_5K.csv\"\n",
        "df_test = pd.read_csv(url, sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TVzhPcGGxf2-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data pre-processing"
      ]
    },
    {
      "metadata": {
        "id": "u-8IR64TKH0F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Get Train Data"
      ]
    },
    {
      "metadata": {
        "id": "JprKnOXgxXkE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_raw_text_reviews = df_train['text'].astype(str).tolist()\n",
        "train_text_labels = df_train['label'].astype(int).tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TwU4zROpKLdQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Get Validation Data"
      ]
    },
    {
      "metadata": {
        "id": "7TsL1ACWJ_3V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "val_raw_text_reviews = df_val['text'].astype(str).tolist()\n",
        "val_text_labels = df_val['label'].astype(int).tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YixDzEnXLZN-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Get Test Data"
      ]
    },
    {
      "metadata": {
        "id": "MQHaSpnJJ9eT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_raw_text_reviews = df_test['text'].astype(str).tolist()\n",
        "test_text_labels = df_test['label'].astype(int).tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ui-q6WRqOrB4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Cleaning the raw input data"
      ]
    },
    {
      "metadata": {
        "id": "8_4b4FmShb0u",
        "colab_type": "code",
        "outputId": "76b9d656-4699-432c-b772-ed6ca676b54c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "# import nltk\n",
        "# nltk.download(\"stopwords\")\n",
        "# from nltk.corpus import stopwords\n",
        "\n",
        "# # Finding stop words\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "\n",
        "import spacy\n",
        "print('spaCy Version: %s' % (spacy.__version__))\n",
        "spacy_nlp = spacy.load('en_core_web_sm')\n",
        "# stop word list\n",
        "stop_words = set(spacy.lang.en.stop_words.STOP_WORDS)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spaCy Version: 2.0.18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "s3AxEhD_z1DM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess_document(doc):\n",
        "    # negative sense should not be eleminated + some short representation\n",
        "    CONTRACTIONS = {\"didn't\":\"did not\", \"mayn't\":\"may not\", \"can't\":\"can not\", \"won't\":\"will not\", \"isn't\":\"is not\", \"amn't\":\"am not\",\\\n",
        "                  \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\", \"couldn't\":\"could not\", \\\n",
        "                  \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\\\n",
        "                  \"i'll\":\"i will\", \"you'll\":\"you will\", \"they'll\":\"they will\",\\\n",
        "                  \"may've\":\"may have\", \"can've\":\"can have\", \"will've\":\"will have\", \"you've\":\"you have\", \\\n",
        "                  \"could've\":\"could have\", \"would've\":\"would have\", \"you've\":\"you have\", \"they\":\"they have\",\\\n",
        "                  \"i've\":\"i have\", \"you've\":\"you have\", \"we've\":\"we have\", \"there's\":\"there is\", \"i'm\":\"i am\",\\\n",
        "                  \"it's\":\"it is\", \"what's\":\"what is\", \"where's\":\"where is\", \"how's\":\"how is\", \"i'd\":\"i had\"}\n",
        "    punctuation = string.punctuation + \"\\n\\n\"\n",
        "    punc_replace = ''.join([' ' for s in punctuation]) # required for replacing punctuation with null ('')\n",
        "    doc_clean = doc.replace('-', ' ') # replace - with null str\n",
        "    doc_clean = (doc_clean.encode('ascii', 'ignore')).decode(\"utf-8\")\n",
        "    doc_clean = doc_clean.replace('<br />', '') # replace <br /> with ''\n",
        "    doc_clean = doc_clean.replace(\"’\", \"'\") # replace <br /> with null str\n",
        "    doc_clean = [CONTRACTIONS[word] if word in CONTRACTIONS else word for word in doc_clean.split(' ')] # replacing some common short forms\n",
        "    doc_clean = \" \".join(doc_clean) # list to sentence of strings\n",
        "    doc_clean = re.sub(r'\\W +', ' ', doc_clean) # except [a-zA-Z0-9_]\n",
        "    doc_clean = re.sub(r'\\d+', ' ', doc_clean) # remove numbers [0-9]\n",
        "    trans_table = str.maketrans(punctuation, punc_replace); # replace punctuations with ' '\n",
        "    doc_clean = ' '.join([word.translate(trans_table) for word in doc_clean.split(' ')])\n",
        "    doc_clean = doc_clean.split(' ')\n",
        "    doc_clean = [word for word in doc_clean if len(word) > 0]\n",
        "    # removing the stopwords from a sentence\n",
        "    doc_clean = [word for word in doc_clean if not word.lower() in stop_words or word.lower() == 'not' or word.lower() == 'no']\n",
        "    return doc_clean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rSNuXUopzavX",
        "colab_type": "code",
        "outputId": "68f7814c-4786-477a-8955-00682f315ca9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "train_text_reviews = [preprocess_document(review.lower()) for review in train_raw_text_reviews]\n",
        "print (train_text_reviews[len(train_text_reviews)-2])\n",
        "print (train_text_labels[len(train_text_labels)-2])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['tales', 'told', 'film', 'shot', 'knowledge', 'combined', 'vignette', 'film', 'makers', 'relate', 'vignettes', 'having', 'connected', 'shrink', 'martin', 'kove', 'leads', 'kove', 'vignette', 'sexy', 'vivian', 'schilling', 'woman', 'afraid', 'sun', 'makes', 'adrian', 'monk', 'look', 'brave', 'having', 'paranoia', 'laced', 'evening', 'home', 'literally', 'scream', 'vivian', 'ridiculous', 'things', 'spends', 'majority', 'time', 'nighty', 'shows', 'amazing', 'features', 'film', 'worst', 'not', 'nail', 'biting', 'second', 'vignette', 'owned', 'bill', 'paxton', 'portrays', 'roommate', 'hell', 'geeky', 'roommate', 'allows', 'complete', 'advantage', 'bill', 'vignette', 'funny', 'man', 'fears', 'death', 'moment', 'like', 'pal', 'choked', 'death', 'olive', 'not', 'interesting', 'movie', 'chopped', 'little', 'thought', 'involved', 'bill', 'paxton', 'fans']\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "W6psZ2jxNOoP",
        "colab_type": "code",
        "outputId": "4c1de8c2-fb14-46c1-f85d-b5a6eb38b5cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "val_text_reviews = [preprocess_document(review.lower()) for review in val_raw_text_reviews]\n",
        "print (val_text_reviews[len(val_text_reviews)-2])\n",
        "print (val_text_labels[len(val_text_labels)-2])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['not', 'realize', 'sellers', 'poor', 'health', 'time', 'filming', 'passed', 'away', 'film', 's', 'release', 'fiendish', 'plot', 'dr', 'fu', 'manchu', 'painful', 'viewing', 'supposedly', 'lampoon', 'sax', 'rohmer', 's', 'famous', 'oriental', 'villain', 'lacks', 'focus', 'potential', 'satirical', 'commentary', 'anti', 'oriental', 'overtones', 'rohmer', 's', 'concept', 'ignored', 'movie', 'employs', 'racist', 'insults', 'hardly', 'actual', 'jokes', 'gags', 'actors', 'behaving', 'idiotically', 'spouting', 'dreary', 'lines', 'especially', 'distressing', 'sid', 'caesar', 'forced', 'spout', 'curses', 'racial', 'slurs', 'attempted', 'laughs', 'actors', 'embarrass', 'peter', 'sellers', 'plays', 'dual', 'roles', 'sinister', 'fu', 'manchu', 'trying', 'concoct', 'formula', 'regain', 'youth', 'stalwart', 'british', 'foe', 'nayland', 'smith', 'sellers', 'not', 'cent', 'bad', 'conveys', 'quirky', 'warmth', 'smith', 'discusses', 'fetishistic', 'attachment', 'lawn', 'mower', 's', 'oddly', 'moving', 'manchu', 'expresses', 'love', 'english', 'music', 'hall', 'entertainment', 'time', 'plays', 'roles', 'weary', 'grimness', 'sabotaging', 'comical', 'possibilities', 'sellers', 'routines', 'revitalizes', 'fading', 'strength', 'electric', 'shocks', 'particularly', 'excruciating', 'convincingly', 'agonized', 'funny', 'genuinely', 'witty', 'lines', 'apt', 'slapstick', 'bit', 'burt', 'kwouk', 'cato', 'pink', 'panther', 'films', 'manchu', 's', 'minions', 'helen', 'mirren', 's', 'amusing', 'musical', 'numbers', 'salvage', 'mess', 'wants', 'understand', 'peter', 'sellers', 'considered', 'comedic', 'genius', 'not', 'learn', 'fiendish', 'plot', 'dr', 'fu', 'manchu']\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "REdaGj3VNOEX",
        "colab_type": "code",
        "outputId": "1ca1f715-b82e-4880-bc6d-2208a7741c05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "test_text_reviews = [preprocess_document(review.lower()) for review in test_raw_text_reviews]\n",
        "print (test_text_reviews[len(test_text_reviews)-2])\n",
        "print (test_text_labels[len(test_text_labels)-2])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['shining', 'know', 'weird', 'movie', 'movie', 'people', 'claim', 'not', 'like', 'horror', 'films', 'shining', 'terrific', 'film', 'stanley', 'kubrick', 's', 'classic', 'vision', 'stephen', 'king', 's', 'horror', 'tale', 'madness', 'blood', 'incredible', 'film', 'wither', 'seen', 'not', 'heard', 'know', 'lines', 'know', 'classic', 'images', 'forget', 'jack', 's', 's', 'johnny', 'forget', 'work', 'no', 'play', 'jack', 'dull', 'boy', 'forget', 'chilling', 'ending', 'film', 'unforgettable', 'honestly', 'opinion', 'kubrick', 's', 'best', 'work', 'know', 'lot', 'argument', 'department', 'lot', 'people', 'space', 'odyssey', 'clockwork', 'orange', 'dr', 'strangelove', 'film', 'pioneered', 'film', 'making', 'shining', 'perfected', 'tale', 'isolation', 'madness', 'terrifying', 'images', 'ultimate', 'ghost', 'story', 'crawl', 'underneath', 'skin', 'jack', 'torrance', 'jack', 's', 'son', 'danny', 'jack', 's', 'wife', 'wendy', 'arrive', 'overlook', 'hotel', 'closing', 'day', 'elderly', 'african', 'american', 'chef', 'dick', 'hallorann', 'surprises', 'danny', 'speaking', 'telepathically', 'offering', 'ice', 'cream', 'explains', 'danny', 'grandmother', 'shared', 'gift', 'called', 'communication', 'shining', 'danny', 'asks', 'afraid', 'hotel', 'particularly', 'room', 'dick', 'tells', 'danny', 'hotel', 'certain', 'shine', 'memories', 'not', 'good', 'advises', 'stay', 'room', 'circumstances', 'danny', 's', 'curiosity', 'room', 'finally', 'gets', 'better', 'sees', 'room', 'opened', 'danny', 'shows', 'injured', 'visibly', 'traumatized', 'jack', 'tells', 'wendy', 'loves', 'family', 'seeing', 'wendy', 'thinks', 'jack', 'abusing', 'danny', 'jack', 'wanders', 'hotel', 's', 'gold', 'room', 'meets', 'ghostly', 'bartender', 'named', 'lloyd', 'danny', 'starts', 'calling', 'word', 'redrum', 'frantically', 'scribbling', 'walls', 'goes', 'trance', 'withdraws', 'says', 'tony', 'imaginary', 'friend', 'jack', 'sabotages', 'hotel', 'radio', 'cutting', 'communication', 'outside', 'world', 'hallorann', 'received', 'danny', 's', 'telepathic', 'cry', 'help', 'way', 'wendy', 'discovers', 'jack', 'typing', 'endless', 'pages', 'manuscript', 'repeating', 'work', 'no', 'play', 'makes', 'jack', 'dull', 'boy', 'formatted', 'ways', 'horrified', 'jack', 'threatens', 'knocks', 'unconscious', 'baseball', 'bat', 'locking', 'storage', 'locker', 'kitchen', 'jack', 'converses', 'grady', 'door', 'locker', 'unlocks', 'releasing', 'danny', 'written', 'redrum', 'lipstick', 'door', 'wendy', 's', 'bedroom', 'looks', 'mirror', 'sees', 'murder', 'spelled', 'backwards', 'jack', 'picks', 'axe', 'begins', 'chop', 'door', 'leading', 'family', 's', 'living', 'quarters', 's', 'johnny', 'jack', 's', 'legendary', 'image', 'born', 'shining', 'films', 'seriously', 'time', 'incredible', 'film', 'gives', 'nightmares', 'jack', 'nicholson', 's', 'performance', 'timeless', 'unforgettable', 'feel', 'extremely', 'overlooked', 'shelley', 'duvall', 'scene', 'finding', 'jack', 's', 'rant', 'work', 'incredible', 's', 'look', 'horror', 'fear', 'face', 'realizing', 'husband', 'mad', 'incredible', 'scene', 'jack', 'sees', 'ghost', 'woman', 'bathtub', 'honestly', 'terrifying', 'scenes', 'horror', 'cinema', 'reason', 'film', 'known', 'film', 'perfection', 'simpsons', 'shown', 'films', 'film', 'forever', 'stay', 'trust']\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ydrqMX261kQ-",
        "colab_type": "code",
        "outputId": "e2217f1e-63a1-4b80-9bfe-992d6e0fd2d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "xWfJK2HvwwQn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_vocabulary(sentences):\n",
        "    # Build vocabulary\n",
        "    dictWordCount = {}\n",
        "    for sent in sentences:\n",
        "        for word in sent:\n",
        "            dictWordCount[word] = 0 # initialising the dict value to zero\n",
        "    for sent in sentences:\n",
        "        for word in sent:\n",
        "            dictWordCount[word] += 1 # updating the dictionary count\n",
        "    \n",
        "    temp = dictWordCount.copy()\n",
        "    for key, val in temp.items():\n",
        "        if(dictWordCount[key] <= 10):\n",
        "            del dictWordCount[key]\n",
        "    \n",
        "    # Mapping from index to word\n",
        "    vocabulary_inv = sorted(dictWordCount, key=dictWordCount.__getitem__, reverse=True)\n",
        "    \n",
        "    # Mapping from word to index\n",
        "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
        "    return vocabulary, vocabulary_inv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MKifaS85Dkap",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Creating Tokens and Types"
      ]
    },
    {
      "metadata": {
        "id": "9cpQlJ3TytgZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_to_ix, ix_to_word = build_vocabulary(train_text_reviews+val_text_reviews+test_text_reviews)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PLBgxuf0xDB9",
        "colab_type": "code",
        "outputId": "2ce8f1b1-5645-406e-fcdb-326b4890e48a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = len(word_to_ix)\n",
        "ix_to_word[word_to_ix['kick']]=='kick', word_to_ix['kick'], VOCAB_SIZE"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, 1710, 17746)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "GMbWbZdr1reC",
        "colab_type": "code",
        "outputId": "74727068-e78b-48ff-b4e8-06808e6180d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "403"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "yNUlKuJeNJEu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## PyTorch"
      ]
    },
    {
      "metadata": {
        "id": "SqLOymKPLPug",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable # Automatic gradients are calculated and back-propagated through the computational graph\n",
        "import copy\n",
        "import csv\n",
        "import time\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sWqAwOAI7D54",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generate the BOW Vectors"
      ]
    },
    {
      "metadata": {
        "id": "UZK9TAQI7Fbm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_bow_vector(sentence, word_to_ix):\n",
        "    # create a vector of zeros of vocab size = len(word_to_idx)\n",
        "    vec = torch.zeros(len(word_to_ix)).to('cuda:0') #, device=device) # make 1D vector of len = vocab size\n",
        "    for word in sentence:\n",
        "        if word not in word_to_ix:            \n",
        "#             raise ValueError('Word',word,' not present in the dictionary. Sorry!')\n",
        "            pass\n",
        "        else:\n",
        "            vec[word_to_ix[word]]+=1 # count the number of occurance of same word in a sentences\n",
        "            \n",
        "    return vec.view(1, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6p3T6oo_T99z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Store in input sentences torch vector"
      ]
    },
    {
      "metadata": {
        "id": "W7xoiOiAyZN5",
        "colab_type": "code",
        "outputId": "bdbe376c-9f36-48b2-cf70-8f6d8ac8419f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# store the bag of word vectors for each sentences and wraping to tensor of torch type\n",
        "tic = time.time()\n",
        "train_data = [Variable(make_bow_vector(instance, word_to_ix)).to('cuda:0') for instance in train_text_reviews]\n",
        "num_train_data = len(train_text_reviews)\n",
        "\n",
        "val_data = [Variable(make_bow_vector(instance, word_to_ix)).to('cuda:0') for instance in val_text_reviews]\n",
        "num_val_data = len(val_text_reviews)\n",
        "toc = time.time()\n",
        "num_train_data, num_val_data, (toc-tic)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16198, 1801, 81.76877903938293)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "deO_hz8vT19u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Validation Accuracy Computation"
      ]
    },
    {
      "metadata": {
        "id": "cStQIVFfESWM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate_validation_accuracy(data, net, activation='sigmoid'):\n",
        "    sum_loss = 0\n",
        "    \n",
        "    true_positive = 0\n",
        "    true_negative = 0\n",
        "    false_positive = 0\n",
        "    false_negative = 0\n",
        "    \n",
        "    for i, instance in enumerate(data):\n",
        "        label = val_text_labels[i] # get the label of the corresponding instace\n",
        "        label = Variable(torch.FloatTensor([label])).resize_((1,1)).to('cuda:0') # wraps a tensor for label\n",
        "        \n",
        "#         vec = Variable(make_bow_vector(instance, word_to_ix)).to('cuda:0') # wrap to tensor of torch type for instance\n",
        "        \n",
        "#         prob = net.forward(vec) # forward pass\n",
        "\n",
        "        prob = net.forward(instance)\n",
        "    \n",
        "        if(activation == 'sigmoid'):\n",
        "            _class = 1 if prob.item() > 0.5 else 0 # sigmoid activated\n",
        "        \n",
        "        if(activation == 'tanh'):\n",
        "            _class = 1 if prob.item() > 0.0 else 0 # tanh activated\n",
        "        \n",
        "        if(activation == 'relu'):\n",
        "            _class = np.argmax(prob.data.numpy()) # relu activated\n",
        "        \n",
        "        loss = loss_function(prob, label) # compute the loss\n",
        "        \n",
        "        sum_loss += float(loss.item())\n",
        "        \n",
        "#         if(int(label) == _class and _class == 1):\n",
        "#             true_positive += 1\n",
        "            \n",
        "#         if(int(label) == _class and _class == 0):\n",
        "#             true_negative += 1\n",
        "        \n",
        "#         if(_class == 1 and int(label) == 0):\n",
        "#             false_positive += 1\n",
        "            \n",
        "#         if(_class == 0 and int(label) == 1):\n",
        "#             false_negative += 1\n",
        "    \n",
        "    \n",
        "#     precision = float(true_positive) / (true_positive + false_positive)\n",
        "#     recall = float(true_positive) / (true_positive + false_negative)\n",
        "#     f_score = float(2)*precision*recall / (precision + recall)\n",
        "#     accuracy = float(1)*(true_positive+true_negative)/(true_positive+true_negative+false_positive+false_negative)\n",
        "    \n",
        "    return float(sum_loss)/len(data)#, float(100)*accuracy, precision, recall, f_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SaSwXEBh6565",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Task-1"
      ]
    },
    {
      "metadata": {
        "id": "aCYGwbP7zy8i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "task1 = [(50, 'Task1A.mdl'), (100, 'Task1B.mdl'), (200, 'Task1C.mdl'), (500, 'Task1D.mdl')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PM5I4GP6lFGu",
        "colab_type": "code",
        "outputId": "61221f56-a8b1-48a0-cc11-cf4076385d03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7092
        }
      },
      "cell_type": "code",
      "source": [
        "for num_of_hidden, task_name in task1:\n",
        "    class BOWClassifier(nn.Module):\n",
        "        def __init__(self, input_size, hidden_size, output_size):\n",
        "            super(BOWClassifier, self).__init__()\n",
        "            SEED = 42\n",
        "            torch.manual_seed(SEED)\n",
        "            torch.cuda.manual_seed(SEED)\n",
        "            self.hidden_size = hidden_size\n",
        "            self.i2h = nn.Linear(input_size, hidden_size) # initialises weights and biases i2h\n",
        "            self.h2o = nn.Linear(hidden_size, output_size) # initialises weights and biases h2o\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = torch.relu(self.i2h(x)) # relu activation @ hidden layer\n",
        "            x = torch.sigmoid(self.h2o(x)) # sigmoid activation @ output layer\n",
        "            return x\n",
        "\n",
        "    num_of_input = VOCAB_SIZE # Bag word restriction has to be equal to vocabulary\n",
        "    num_of_output = 1 # binary sentiment classes (+ve, -ve)\n",
        "\n",
        "    bow = BOWClassifier(num_of_input, num_of_hidden, num_of_output).to('cuda:0') # initialises weights and biases\n",
        "    \n",
        "    # define a loss function and an optimizer\n",
        "    loss_function = nn.BCELoss()\n",
        "    opt = torch.optim.SGD(bow.parameters(), lr = 0.0001)\n",
        "    \n",
        "    gc.collect()\n",
        "\n",
        "    # Train The Model\n",
        "    epochs = 100\n",
        "    # the training loop\n",
        "    total_time = 0.0\n",
        "    prev_val_loss = float('inf')\n",
        "    val_loss = 0\n",
        "    early_stop_bow = 0\n",
        "\n",
        "    for e in range(epochs):\n",
        "        tic = time.time() # start the timer\n",
        "        correct = 0\n",
        "        cumulative_loss = 0\n",
        "        incorrect = 0\n",
        "\n",
        "        for i, instance in enumerate(train_data): # train_text_reviews \n",
        "            # get the training data\n",
        "            label = train_text_labels[i] # get the label of the corresponding instace\n",
        "            label = Variable(torch.FloatTensor([label])).resize_((1,1)).to('cuda:0') # wraps a tensor for label\n",
        "\n",
        "            bow.zero_grad() # reset the gradient for each instance\n",
        "\n",
        "            pred = bow.forward(instance)\n",
        "\n",
        "            loss = loss_function(pred, label) # compute the loss\n",
        "            loss.backward() # backprop the loss\n",
        "            opt.step() # performs parameter updation based on the current gradient\n",
        "\n",
        "            cumulative_loss += float(loss.item()) # accumulate the loss over whole training sample\n",
        "\n",
        "            pred_class = 1 if pred.item() > 0.5 else 0 # as sigmoid activated\n",
        "            if(int(label) == pred_class): # counting correct prediction in each epoch\n",
        "                correct += 1\n",
        "\n",
        "        train_loss = float(cumulative_loss)/num_train_data\n",
        "        train_accuracy = correct*float(100)/num_train_data\n",
        "\n",
        "#         val_loss, val_accuracy, p, r, f = evaluate_validation_accuracy(val_data, bow) # test val-data-set on currently trained model\n",
        "        val_loss = evaluate_validation_accuracy(val_data, bow) # test val-data-set on currently trained model\n",
        "        \n",
        "        if(prev_val_loss-val_loss < 0.0): # early stop if nearly no change\n",
        "            bow = copy.deepcopy(early_stop_bow)\n",
        "            break\n",
        "\n",
        "        early_stop_bow = copy.deepcopy(bow)\n",
        "        \n",
        "        prev_val_loss = val_loss\n",
        "\n",
        "        toc = time.time() # final time\n",
        "        total_time += (toc-tic)\n",
        "\n",
        "        print(\"Epoch {}/{}\\n[On Training] ==> Time: {:.2f}s, Train Loss: {:.9f}, Train Accuracy: {:.2f}%\".format(e+1, epochs, (toc-tic), train_loss, train_accuracy))\n",
        "#         print(\"[On Validation] ==> Precision: {:.3f}, Recall: {:.3f}, F-Score: {:.3f}, Val loss: {:.9f}, Val Accuracy: {:.2f}% Total Time: {:.2f}s\".format(p, r, f, val_loss, val_accuracy, total_time))\n",
        "        print(\"[On Validation] ==> Val loss: {:.9f}, Total Time: {:.2f}s\".format(val_loss, total_time))\n",
        "    torch.save(bow, task_name)\n",
        "    \n",
        "    print ('\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "[On Training] ==> Time: 23.06s, Train Loss: 0.681705095, Train Accuracy: 59.83%\n",
            "[On Validation] ==> Val loss: 0.666637056, Total Time: 23.06s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 23.06s, Train Loss: 0.649834897, Train Accuracy: 71.06%\n",
            "[On Validation] ==> Val loss: 0.630639151, Total Time: 46.12s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 23.07s, Train Loss: 0.610761846, Train Accuracy: 74.96%\n",
            "[On Validation] ==> Val loss: 0.589656338, Total Time: 69.18s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 23.29s, Train Loss: 0.567034283, Train Accuracy: 77.96%\n",
            "[On Validation] ==> Val loss: 0.547316584, Total Time: 92.47s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 23.46s, Train Loss: 0.523517547, Train Accuracy: 80.12%\n",
            "[On Validation] ==> Val loss: 0.508655596, Total Time: 115.93s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 23.07s, Train Loss: 0.484684351, Train Accuracy: 81.76%\n",
            "[On Validation] ==> Val loss: 0.476580857, Total Time: 139.00s\n",
            "Epoch 7/100\n",
            "[On Training] ==> Time: 23.01s, Train Loss: 0.452042349, Train Accuracy: 82.96%\n",
            "[On Validation] ==> Val loss: 0.451059309, Total Time: 162.01s\n",
            "Epoch 8/100\n",
            "[On Training] ==> Time: 23.05s, Train Loss: 0.424890700, Train Accuracy: 84.04%\n",
            "[On Validation] ==> Val loss: 0.430740765, Total Time: 185.06s\n",
            "Epoch 9/100\n",
            "[On Training] ==> Time: 22.91s, Train Loss: 0.401949426, Train Accuracy: 84.89%\n",
            "[On Validation] ==> Val loss: 0.414232479, Total Time: 207.96s\n",
            "Epoch 10/100\n",
            "[On Training] ==> Time: 22.96s, Train Loss: 0.382149114, Train Accuracy: 85.59%\n",
            "[On Validation] ==> Val loss: 0.400554505, Total Time: 230.92s\n",
            "Epoch 11/100\n",
            "[On Training] ==> Time: 22.98s, Train Loss: 0.364734929, Train Accuracy: 86.26%\n",
            "[On Validation] ==> Val loss: 0.389032839, Total Time: 253.90s\n",
            "Epoch 12/100\n",
            "[On Training] ==> Time: 23.05s, Train Loss: 0.349214595, Train Accuracy: 87.05%\n",
            "[On Validation] ==> Val loss: 0.379227757, Total Time: 276.95s\n",
            "Epoch 13/100\n",
            "[On Training] ==> Time: 23.09s, Train Loss: 0.335251543, Train Accuracy: 87.62%\n",
            "[On Validation] ==> Val loss: 0.370829720, Total Time: 300.03s\n",
            "Epoch 14/100\n",
            "[On Training] ==> Time: 22.99s, Train Loss: 0.322610657, Train Accuracy: 88.17%\n",
            "[On Validation] ==> Val loss: 0.363629171, Total Time: 323.02s\n",
            "Epoch 15/100\n",
            "[On Training] ==> Time: 22.99s, Train Loss: 0.311110765, Train Accuracy: 88.69%\n",
            "[On Validation] ==> Val loss: 0.357449959, Total Time: 346.01s\n",
            "Epoch 16/100\n",
            "[On Training] ==> Time: 23.04s, Train Loss: 0.300601529, Train Accuracy: 88.95%\n",
            "[On Validation] ==> Val loss: 0.352136252, Total Time: 369.05s\n",
            "Epoch 17/100\n",
            "[On Training] ==> Time: 23.34s, Train Loss: 0.290954471, Train Accuracy: 89.42%\n",
            "[On Validation] ==> Val loss: 0.347570202, Total Time: 392.39s\n",
            "Epoch 18/100\n",
            "[On Training] ==> Time: 22.98s, Train Loss: 0.282060537, Train Accuracy: 89.78%\n",
            "[On Validation] ==> Val loss: 0.343643173, Total Time: 415.37s\n",
            "Epoch 19/100\n",
            "[On Training] ==> Time: 23.36s, Train Loss: 0.273827891, Train Accuracy: 90.15%\n",
            "[On Validation] ==> Val loss: 0.340262304, Total Time: 438.73s\n",
            "Epoch 20/100\n",
            "[On Training] ==> Time: 22.90s, Train Loss: 0.266174693, Train Accuracy: 90.50%\n",
            "[On Validation] ==> Val loss: 0.337354283, Total Time: 461.63s\n",
            "Epoch 21/100\n",
            "[On Training] ==> Time: 23.05s, Train Loss: 0.259033906, Train Accuracy: 90.83%\n",
            "[On Validation] ==> Val loss: 0.334848357, Total Time: 484.68s\n",
            "Epoch 22/100\n",
            "[On Training] ==> Time: 23.02s, Train Loss: 0.252346325, Train Accuracy: 91.09%\n",
            "[On Validation] ==> Val loss: 0.332691275, Total Time: 507.70s\n",
            "Epoch 23/100\n",
            "[On Training] ==> Time: 23.07s, Train Loss: 0.246057648, Train Accuracy: 91.32%\n",
            "[On Validation] ==> Val loss: 0.330843446, Total Time: 530.77s\n",
            "Epoch 24/100\n",
            "[On Training] ==> Time: 23.04s, Train Loss: 0.240124208, Train Accuracy: 91.50%\n",
            "[On Validation] ==> Val loss: 0.329258181, Total Time: 553.81s\n",
            "Epoch 25/100\n",
            "[On Training] ==> Time: 23.02s, Train Loss: 0.234509083, Train Accuracy: 91.74%\n",
            "[On Validation] ==> Val loss: 0.327911623, Total Time: 576.83s\n",
            "Epoch 26/100\n",
            "[On Training] ==> Time: 23.04s, Train Loss: 0.229178639, Train Accuracy: 91.96%\n",
            "[On Validation] ==> Val loss: 0.326771351, Total Time: 599.87s\n",
            "Epoch 27/100\n",
            "[On Training] ==> Time: 22.99s, Train Loss: 0.224104119, Train Accuracy: 92.15%\n",
            "[On Validation] ==> Val loss: 0.325820935, Total Time: 622.86s\n",
            "Epoch 28/100\n",
            "[On Training] ==> Time: 23.03s, Train Loss: 0.219262013, Train Accuracy: 92.41%\n",
            "[On Validation] ==> Val loss: 0.325040913, Total Time: 645.89s\n",
            "Epoch 29/100\n",
            "[On Training] ==> Time: 23.08s, Train Loss: 0.214632127, Train Accuracy: 92.61%\n",
            "[On Validation] ==> Val loss: 0.324416180, Total Time: 668.96s\n",
            "Epoch 30/100\n",
            "[On Training] ==> Time: 23.30s, Train Loss: 0.210196805, Train Accuracy: 92.78%\n",
            "[On Validation] ==> Val loss: 0.323935933, Total Time: 692.26s\n",
            "Epoch 31/100\n",
            "[On Training] ==> Time: 23.02s, Train Loss: 0.205939991, Train Accuracy: 92.93%\n",
            "[On Validation] ==> Val loss: 0.323584122, Total Time: 715.28s\n",
            "Epoch 32/100\n",
            "[On Training] ==> Time: 23.49s, Train Loss: 0.201847583, Train Accuracy: 93.11%\n",
            "[On Validation] ==> Val loss: 0.323348896, Total Time: 738.77s\n",
            "Epoch 33/100\n",
            "[On Training] ==> Time: 23.00s, Train Loss: 0.197906682, Train Accuracy: 93.27%\n",
            "[On Validation] ==> Val loss: 0.323222940, Total Time: 761.76s\n",
            "Epoch 34/100\n",
            "[On Training] ==> Time: 22.98s, Train Loss: 0.194106340, Train Accuracy: 93.46%\n",
            "[On Validation] ==> Val loss: 0.323196629, Total Time: 784.75s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type BOWClassifier. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Epoch 1/100\n",
            "[On Training] ==> Time: 26.59s, Train Loss: 0.682899648, Train Accuracy: 56.80%\n",
            "[On Validation] ==> Val loss: 0.667911785, Total Time: 26.59s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 26.65s, Train Loss: 0.651006659, Train Accuracy: 70.45%\n",
            "[On Validation] ==> Val loss: 0.630717264, Total Time: 53.25s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 26.63s, Train Loss: 0.610105854, Train Accuracy: 75.27%\n",
            "[On Validation] ==> Val loss: 0.588337909, Total Time: 79.88s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 26.63s, Train Loss: 0.565057077, Train Accuracy: 78.45%\n",
            "[On Validation] ==> Val loss: 0.545340420, Total Time: 106.51s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 26.60s, Train Loss: 0.521185253, Train Accuracy: 80.49%\n",
            "[On Validation] ==> Val loss: 0.506718940, Total Time: 133.11s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 26.64s, Train Loss: 0.482653855, Train Accuracy: 82.04%\n",
            "[On Validation] ==> Val loss: 0.474995293, Total Time: 159.75s\n",
            "Epoch 7/100\n",
            "[On Training] ==> Time: 26.76s, Train Loss: 0.450499358, Train Accuracy: 83.21%\n",
            "[On Validation] ==> Val loss: 0.449845155, Total Time: 186.51s\n",
            "Epoch 8/100\n",
            "[On Training] ==> Time: 26.53s, Train Loss: 0.423774791, Train Accuracy: 84.20%\n",
            "[On Validation] ==> Val loss: 0.429812936, Total Time: 213.04s\n",
            "Epoch 9/100\n",
            "[On Training] ==> Time: 26.85s, Train Loss: 0.401147593, Train Accuracy: 85.04%\n",
            "[On Validation] ==> Val loss: 0.413535266, Total Time: 239.89s\n",
            "Epoch 10/100\n",
            "[On Training] ==> Time: 26.60s, Train Loss: 0.381556442, Train Accuracy: 85.80%\n",
            "[On Validation] ==> Val loss: 0.400026059, Total Time: 266.49s\n",
            "Epoch 11/100\n",
            "[On Training] ==> Time: 26.60s, Train Loss: 0.364274001, Train Accuracy: 86.42%\n",
            "[On Validation] ==> Val loss: 0.388627116, Total Time: 293.10s\n",
            "Epoch 12/100\n",
            "[On Training] ==> Time: 26.59s, Train Loss: 0.348829922, Train Accuracy: 86.98%\n",
            "[On Validation] ==> Val loss: 0.378920804, Total Time: 319.69s\n",
            "Epoch 13/100\n",
            "[On Training] ==> Time: 26.59s, Train Loss: 0.334909365, Train Accuracy: 87.60%\n",
            "[On Validation] ==> Val loss: 0.370612546, Total Time: 346.28s\n",
            "Epoch 14/100\n",
            "[On Training] ==> Time: 26.56s, Train Loss: 0.322288166, Train Accuracy: 88.14%\n",
            "[On Validation] ==> Val loss: 0.363495547, Total Time: 372.84s\n",
            "Epoch 15/100\n",
            "[On Training] ==> Time: 26.60s, Train Loss: 0.310794484, Train Accuracy: 88.60%\n",
            "[On Validation] ==> Val loss: 0.357388993, Total Time: 399.44s\n",
            "Epoch 16/100\n",
            "[On Training] ==> Time: 26.62s, Train Loss: 0.300284192, Train Accuracy: 89.07%\n",
            "[On Validation] ==> Val loss: 0.352148550, Total Time: 426.06s\n",
            "Epoch 17/100\n",
            "[On Training] ==> Time: 26.60s, Train Loss: 0.290638839, Train Accuracy: 89.44%\n",
            "[On Validation] ==> Val loss: 0.347652298, Total Time: 452.66s\n",
            "Epoch 18/100\n",
            "[On Training] ==> Time: 26.57s, Train Loss: 0.281748234, Train Accuracy: 89.89%\n",
            "[On Validation] ==> Val loss: 0.343787264, Total Time: 479.23s\n",
            "Epoch 19/100\n",
            "[On Training] ==> Time: 26.76s, Train Loss: 0.273521514, Train Accuracy: 90.15%\n",
            "[On Validation] ==> Val loss: 0.340464893, Total Time: 505.99s\n",
            "Epoch 20/100\n",
            "[On Training] ==> Time: 26.65s, Train Loss: 0.265876848, Train Accuracy: 90.48%\n",
            "[On Validation] ==> Val loss: 0.337605076, Total Time: 532.63s\n",
            "Epoch 21/100\n",
            "[On Training] ==> Time: 26.88s, Train Loss: 0.258743349, Train Accuracy: 90.84%\n",
            "[On Validation] ==> Val loss: 0.335144273, Total Time: 559.51s\n",
            "Epoch 22/100\n",
            "[On Training] ==> Time: 26.61s, Train Loss: 0.252061058, Train Accuracy: 91.13%\n",
            "[On Validation] ==> Val loss: 0.333031024, Total Time: 586.12s\n",
            "Epoch 23/100\n",
            "[On Training] ==> Time: 26.56s, Train Loss: 0.245777580, Train Accuracy: 91.29%\n",
            "[On Validation] ==> Val loss: 0.331216401, Total Time: 612.68s\n",
            "Epoch 24/100\n",
            "[On Training] ==> Time: 26.62s, Train Loss: 0.239847226, Train Accuracy: 91.56%\n",
            "[On Validation] ==> Val loss: 0.329668282, Total Time: 639.30s\n",
            "Epoch 25/100\n",
            "[On Training] ==> Time: 26.61s, Train Loss: 0.234234402, Train Accuracy: 91.81%\n",
            "[On Validation] ==> Val loss: 0.328354061, Total Time: 665.91s\n",
            "Epoch 26/100\n",
            "[On Training] ==> Time: 26.63s, Train Loss: 0.228906399, Train Accuracy: 91.99%\n",
            "[On Validation] ==> Val loss: 0.327251136, Total Time: 692.54s\n",
            "Epoch 27/100\n",
            "[On Training] ==> Time: 26.42s, Train Loss: 0.223835175, Train Accuracy: 92.17%\n",
            "[On Validation] ==> Val loss: 0.326336317, Total Time: 718.95s\n",
            "Epoch 28/100\n",
            "[On Training] ==> Time: 26.61s, Train Loss: 0.218997411, Train Accuracy: 92.33%\n",
            "[On Validation] ==> Val loss: 0.325593385, Total Time: 745.56s\n",
            "Epoch 29/100\n",
            "[On Training] ==> Time: 26.65s, Train Loss: 0.214372382, Train Accuracy: 92.57%\n",
            "[On Validation] ==> Val loss: 0.325004277, Total Time: 772.21s\n",
            "Epoch 30/100\n",
            "[On Training] ==> Time: 26.76s, Train Loss: 0.209941635, Train Accuracy: 92.77%\n",
            "[On Validation] ==> Val loss: 0.324555185, Total Time: 798.97s\n",
            "Epoch 31/100\n",
            "[On Training] ==> Time: 26.61s, Train Loss: 0.205689046, Train Accuracy: 92.95%\n",
            "[On Validation] ==> Val loss: 0.324234524, Total Time: 825.58s\n",
            "Epoch 32/100\n",
            "[On Training] ==> Time: 26.59s, Train Loss: 0.201600494, Train Accuracy: 93.12%\n",
            "[On Validation] ==> Val loss: 0.324031535, Total Time: 852.17s\n",
            "Epoch 33/100\n",
            "[On Training] ==> Time: 26.79s, Train Loss: 0.197663776, Train Accuracy: 93.28%\n",
            "[On Validation] ==> Val loss: 0.323935602, Total Time: 878.96s\n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "[On Training] ==> Time: 29.14s, Train Loss: 0.684136365, Train Accuracy: 54.82%\n",
            "[On Validation] ==> Val loss: 0.671581390, Total Time: 29.14s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 29.13s, Train Loss: 0.655830315, Train Accuracy: 69.30%\n",
            "[On Validation] ==> Val loss: 0.637428028, Total Time: 58.27s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 29.09s, Train Loss: 0.617346347, Train Accuracy: 74.22%\n",
            "[On Validation] ==> Val loss: 0.596497116, Total Time: 87.37s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 29.14s, Train Loss: 0.573111680, Train Accuracy: 77.57%\n",
            "[On Validation] ==> Val loss: 0.553405911, Total Time: 116.51s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 29.15s, Train Loss: 0.528789045, Train Accuracy: 79.90%\n",
            "[On Validation] ==> Val loss: 0.513785726, Total Time: 145.66s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 29.13s, Train Loss: 0.489233097, Train Accuracy: 81.69%\n",
            "[On Validation] ==> Val loss: 0.480826069, Total Time: 174.79s\n",
            "Epoch 7/100\n",
            "[On Training] ==> Time: 29.35s, Train Loss: 0.456061409, Train Accuracy: 82.71%\n",
            "[On Validation] ==> Val loss: 0.454650018, Total Time: 204.14s\n",
            "Epoch 8/100\n",
            "[On Training] ==> Time: 29.20s, Train Loss: 0.428537606, Train Accuracy: 83.79%\n",
            "[On Validation] ==> Val loss: 0.433860014, Total Time: 233.34s\n",
            "Epoch 9/100\n",
            "[On Training] ==> Time: 29.37s, Train Loss: 0.405319883, Train Accuracy: 84.64%\n",
            "[On Validation] ==> Val loss: 0.417020082, Total Time: 262.71s\n",
            "Epoch 10/100\n",
            "[On Training] ==> Time: 29.14s, Train Loss: 0.385284947, Train Accuracy: 85.52%\n",
            "[On Validation] ==> Val loss: 0.403071554, Total Time: 291.84s\n",
            "Epoch 11/100\n",
            "[On Training] ==> Time: 29.17s, Train Loss: 0.367659489, Train Accuracy: 86.20%\n",
            "[On Validation] ==> Val loss: 0.391324375, Total Time: 321.01s\n",
            "Epoch 12/100\n",
            "[On Training] ==> Time: 29.23s, Train Loss: 0.351936177, Train Accuracy: 86.89%\n",
            "[On Validation] ==> Val loss: 0.381318773, Total Time: 350.24s\n",
            "Epoch 13/100\n",
            "[On Training] ==> Time: 29.18s, Train Loss: 0.337776760, Train Accuracy: 87.59%\n",
            "[On Validation] ==> Val loss: 0.372739008, Total Time: 379.43s\n",
            "Epoch 14/100\n",
            "[On Training] ==> Time: 29.17s, Train Loss: 0.324940252, Train Accuracy: 88.11%\n",
            "[On Validation] ==> Val loss: 0.365362599, Total Time: 408.60s\n",
            "Epoch 15/100\n",
            "[On Training] ==> Time: 29.17s, Train Loss: 0.313250437, Train Accuracy: 88.48%\n",
            "[On Validation] ==> Val loss: 0.359020904, Total Time: 437.77s\n",
            "Epoch 16/100\n",
            "[On Training] ==> Time: 29.24s, Train Loss: 0.302563478, Train Accuracy: 88.89%\n",
            "[On Validation] ==> Val loss: 0.353565587, Total Time: 467.01s\n",
            "Epoch 17/100\n",
            "[On Training] ==> Time: 29.13s, Train Loss: 0.292755544, Train Accuracy: 89.29%\n",
            "[On Validation] ==> Val loss: 0.348871678, Total Time: 496.14s\n",
            "Epoch 18/100\n",
            "[On Training] ==> Time: 29.31s, Train Loss: 0.283717585, Train Accuracy: 89.72%\n",
            "[On Validation] ==> Val loss: 0.344832755, Total Time: 525.44s\n",
            "Epoch 19/100\n",
            "[On Training] ==> Time: 29.15s, Train Loss: 0.275355918, Train Accuracy: 90.13%\n",
            "[On Validation] ==> Val loss: 0.341355016, Total Time: 554.60s\n",
            "Epoch 20/100\n",
            "[On Training] ==> Time: 29.46s, Train Loss: 0.267587763, Train Accuracy: 90.44%\n",
            "[On Validation] ==> Val loss: 0.338356650, Total Time: 584.06s\n",
            "Epoch 21/100\n",
            "[On Training] ==> Time: 29.17s, Train Loss: 0.260342468, Train Accuracy: 90.75%\n",
            "[On Validation] ==> Val loss: 0.335772791, Total Time: 613.23s\n",
            "Epoch 22/100\n",
            "[On Training] ==> Time: 29.22s, Train Loss: 0.253558596, Train Accuracy: 91.01%\n",
            "[On Validation] ==> Val loss: 0.333548638, Total Time: 642.45s\n",
            "Epoch 23/100\n",
            "[On Training] ==> Time: 29.19s, Train Loss: 0.247183099, Train Accuracy: 91.20%\n",
            "[On Validation] ==> Val loss: 0.331636653, Total Time: 671.64s\n",
            "Epoch 24/100\n",
            "[On Training] ==> Time: 29.23s, Train Loss: 0.241170453, Train Accuracy: 91.47%\n",
            "[On Validation] ==> Val loss: 0.329997870, Total Time: 700.87s\n",
            "Epoch 25/100\n",
            "[On Training] ==> Time: 29.19s, Train Loss: 0.235482395, Train Accuracy: 91.73%\n",
            "[On Validation] ==> Val loss: 0.328599487, Total Time: 730.07s\n",
            "Epoch 26/100\n",
            "[On Training] ==> Time: 29.17s, Train Loss: 0.230085478, Train Accuracy: 91.92%\n",
            "[On Validation] ==> Val loss: 0.327417096, Total Time: 759.23s\n",
            "Epoch 27/100\n",
            "[On Training] ==> Time: 29.17s, Train Loss: 0.224950983, Train Accuracy: 92.15%\n",
            "[On Validation] ==> Val loss: 0.326427803, Total Time: 788.40s\n",
            "Epoch 28/100\n",
            "[On Training] ==> Time: 29.37s, Train Loss: 0.220054106, Train Accuracy: 92.35%\n",
            "[On Validation] ==> Val loss: 0.325612216, Total Time: 817.77s\n",
            "Epoch 29/100\n",
            "[On Training] ==> Time: 29.19s, Train Loss: 0.215373615, Train Accuracy: 92.58%\n",
            "[On Validation] ==> Val loss: 0.324955247, Total Time: 846.96s\n",
            "Epoch 30/100\n",
            "[On Training] ==> Time: 29.15s, Train Loss: 0.210890642, Train Accuracy: 92.74%\n",
            "[On Validation] ==> Val loss: 0.324441438, Total Time: 876.11s\n",
            "Epoch 31/100\n",
            "[On Training] ==> Time: 29.36s, Train Loss: 0.206589250, Train Accuracy: 92.99%\n",
            "[On Validation] ==> Val loss: 0.324057952, Total Time: 905.46s\n",
            "Epoch 32/100\n",
            "[On Training] ==> Time: 29.18s, Train Loss: 0.202454885, Train Accuracy: 93.16%\n",
            "[On Validation] ==> Val loss: 0.323794335, Total Time: 934.64s\n",
            "Epoch 33/100\n",
            "[On Training] ==> Time: 29.15s, Train Loss: 0.198474713, Train Accuracy: 93.27%\n",
            "[On Validation] ==> Val loss: 0.323640336, Total Time: 963.80s\n",
            "Epoch 34/100\n",
            "[On Training] ==> Time: 29.21s, Train Loss: 0.194637563, Train Accuracy: 93.47%\n",
            "[On Validation] ==> Val loss: 0.323584518, Total Time: 993.01s\n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "[On Training] ==> Time: 49.67s, Train Loss: 0.683404957, Train Accuracy: 63.92%\n",
            "[On Validation] ==> Val loss: 0.671943526, Total Time: 49.67s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 49.71s, Train Loss: 0.658174460, Train Accuracy: 70.34%\n",
            "[On Validation] ==> Val loss: 0.641364627, Total Time: 99.38s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 49.67s, Train Loss: 0.622567369, Train Accuracy: 73.69%\n",
            "[On Validation] ==> Val loss: 0.602271243, Total Time: 149.05s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 49.73s, Train Loss: 0.579667740, Train Accuracy: 76.94%\n",
            "[On Validation] ==> Val loss: 0.559626417, Total Time: 198.77s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 49.68s, Train Loss: 0.535385378, Train Accuracy: 79.34%\n",
            "[On Validation] ==> Val loss: 0.519441324, Total Time: 248.46s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 49.70s, Train Loss: 0.495185356, Train Accuracy: 81.24%\n",
            "[On Validation] ==> Val loss: 0.485643489, Total Time: 298.16s\n",
            "Epoch 7/100\n",
            "[On Training] ==> Time: 49.60s, Train Loss: 0.461253751, Train Accuracy: 82.54%\n",
            "[On Validation] ==> Val loss: 0.458707905, Total Time: 347.76s\n",
            "Epoch 8/100\n",
            "[On Training] ==> Time: 49.68s, Train Loss: 0.433096484, Train Accuracy: 83.65%\n",
            "[On Validation] ==> Val loss: 0.437357414, Total Time: 397.44s\n",
            "Epoch 9/100\n",
            "[On Training] ==> Time: 49.61s, Train Loss: 0.409395259, Train Accuracy: 84.44%\n",
            "[On Validation] ==> Val loss: 0.420117094, Total Time: 447.05s\n",
            "Epoch 10/100\n",
            "[On Training] ==> Time: 49.61s, Train Loss: 0.388980248, Train Accuracy: 85.21%\n",
            "[On Validation] ==> Val loss: 0.405881058, Total Time: 496.66s\n",
            "Epoch 11/100\n",
            "[On Training] ==> Time: 49.62s, Train Loss: 0.371036533, Train Accuracy: 85.94%\n",
            "[On Validation] ==> Val loss: 0.393917711, Total Time: 546.28s\n",
            "Epoch 12/100\n",
            "[On Training] ==> Time: 49.61s, Train Loss: 0.355035153, Train Accuracy: 86.64%\n",
            "[On Validation] ==> Val loss: 0.383733253, Total Time: 595.90s\n",
            "Epoch 13/100\n",
            "[On Training] ==> Time: 49.63s, Train Loss: 0.340626611, Train Accuracy: 87.31%\n",
            "[On Validation] ==> Val loss: 0.375004442, Total Time: 645.53s\n",
            "Epoch 14/100\n",
            "[On Training] ==> Time: 49.60s, Train Loss: 0.327565578, Train Accuracy: 87.94%\n",
            "[On Validation] ==> Val loss: 0.367499193, Total Time: 695.13s\n",
            "Epoch 15/100\n",
            "[On Training] ==> Time: 49.60s, Train Loss: 0.315670670, Train Accuracy: 88.34%\n",
            "[On Validation] ==> Val loss: 0.361035378, Total Time: 744.73s\n",
            "Epoch 16/100\n",
            "[On Training] ==> Time: 49.64s, Train Loss: 0.304798003, Train Accuracy: 88.85%\n",
            "[On Validation] ==> Val loss: 0.355468643, Total Time: 794.38s\n",
            "Epoch 17/100\n",
            "[On Training] ==> Time: 49.63s, Train Loss: 0.294821491, Train Accuracy: 89.22%\n",
            "[On Validation] ==> Val loss: 0.350678157, Total Time: 844.00s\n",
            "Epoch 18/100\n",
            "[On Training] ==> Time: 49.69s, Train Loss: 0.285635613, Train Accuracy: 89.73%\n",
            "[On Validation] ==> Val loss: 0.346556021, Total Time: 893.69s\n",
            "Epoch 19/100\n",
            "[On Training] ==> Time: 49.64s, Train Loss: 0.277143586, Train Accuracy: 90.10%\n",
            "[On Validation] ==> Val loss: 0.343006237, Total Time: 943.33s\n",
            "Epoch 20/100\n",
            "[On Training] ==> Time: 49.65s, Train Loss: 0.269261316, Train Accuracy: 90.43%\n",
            "[On Validation] ==> Val loss: 0.339947157, Total Time: 992.98s\n",
            "Epoch 21/100\n",
            "[On Training] ==> Time: 49.67s, Train Loss: 0.261914686, Train Accuracy: 90.77%\n",
            "[On Validation] ==> Val loss: 0.337310735, Total Time: 1042.65s\n",
            "Epoch 22/100\n",
            "[On Training] ==> Time: 49.59s, Train Loss: 0.255040123, Train Accuracy: 90.94%\n",
            "[On Validation] ==> Val loss: 0.335039689, Total Time: 1092.24s\n",
            "Epoch 23/100\n",
            "[On Training] ==> Time: 49.66s, Train Loss: 0.248582706, Train Accuracy: 91.22%\n",
            "[On Validation] ==> Val loss: 0.333085401, Total Time: 1141.90s\n",
            "Epoch 24/100\n",
            "[On Training] ==> Time: 49.57s, Train Loss: 0.242496099, Train Accuracy: 91.52%\n",
            "[On Validation] ==> Val loss: 0.331407599, Total Time: 1191.47s\n",
            "Epoch 25/100\n",
            "[On Training] ==> Time: 49.71s, Train Loss: 0.236740120, Train Accuracy: 91.71%\n",
            "[On Validation] ==> Val loss: 0.329974528, Total Time: 1241.18s\n",
            "Epoch 26/100\n",
            "[On Training] ==> Time: 49.64s, Train Loss: 0.231280695, Train Accuracy: 91.89%\n",
            "[On Validation] ==> Val loss: 0.328759158, Total Time: 1290.82s\n",
            "Epoch 27/100\n",
            "[On Training] ==> Time: 49.60s, Train Loss: 0.226088694, Train Accuracy: 92.09%\n",
            "[On Validation] ==> Val loss: 0.327737424, Total Time: 1340.42s\n",
            "Epoch 28/100\n",
            "[On Training] ==> Time: 49.65s, Train Loss: 0.221138230, Train Accuracy: 92.33%\n",
            "[On Validation] ==> Val loss: 0.326890568, Total Time: 1390.07s\n",
            "Epoch 29/100\n",
            "[On Training] ==> Time: 49.62s, Train Loss: 0.216407462, Train Accuracy: 92.55%\n",
            "[On Validation] ==> Val loss: 0.326202153, Total Time: 1439.68s\n",
            "Epoch 30/100\n",
            "[On Training] ==> Time: 49.63s, Train Loss: 0.211877323, Train Accuracy: 92.72%\n",
            "[On Validation] ==> Val loss: 0.325657484, Total Time: 1489.31s\n",
            "Epoch 31/100\n",
            "[On Training] ==> Time: 49.60s, Train Loss: 0.207531356, Train Accuracy: 92.84%\n",
            "[On Validation] ==> Val loss: 0.325242962, Total Time: 1538.91s\n",
            "Epoch 32/100\n",
            "[On Training] ==> Time: 49.61s, Train Loss: 0.203354317, Train Accuracy: 93.05%\n",
            "[On Validation] ==> Val loss: 0.324948401, Total Time: 1588.52s\n",
            "Epoch 33/100\n",
            "[On Training] ==> Time: 49.63s, Train Loss: 0.199333637, Train Accuracy: 93.22%\n",
            "[On Validation] ==> Val loss: 0.324763801, Total Time: 1638.15s\n",
            "Epoch 34/100\n",
            "[On Training] ==> Time: 49.56s, Train Loss: 0.195457169, Train Accuracy: 93.39%\n",
            "[On Validation] ==> Val loss: 0.324679850, Total Time: 1687.72s\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4XoHF1M8IVdf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download('Task1A.mdl')\n",
        "files.download('Task1B.mdl')\n",
        "files.download('Task1C.mdl')\n",
        "files.download('Task1D.mdl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FxO_e5P1mX3I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Task-2"
      ]
    },
    {
      "metadata": {
        "id": "lpUB7Obs197T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "task2 = [(10, 10, 'Task2A.mdl'), (20, 10, 'Task2B.mdl'), (30, 30, 'Task2C.mdl'), (50, 50, 'Task2D.mdl'), (100, 50, 'Task2E.mdl')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UiqcS_4XmXbd",
        "colab_type": "code",
        "outputId": "75f8d7e3-11db-40e7-e143-a55f3ebb20e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5188
        }
      },
      "cell_type": "code",
      "source": [
        "for num_of_hidden1, num_of_hidden2, task_name in task2:\n",
        "    class BOWClassifier(nn.Module):\n",
        "        def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
        "            super(BOWClassifier, self).__init__()\n",
        "            SEED = 42\n",
        "            torch.manual_seed(SEED)\n",
        "            torch.cuda.manual_seed(SEED)\n",
        "            self.i2h = nn.Linear(input_size, hidden_size1) # initialises weights and biases i2h\n",
        "            self.h2h = nn.Linear(hidden_size1, hidden_size2) # initialises weights and biases h2h\n",
        "            self.h2o = nn.Linear(hidden_size2, output_size) # initialises weights and biases h2o\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = torch.relu(self.i2h(x)) # relu activation @ hidden layer\n",
        "            x = torch.relu(self.h2h(x)) # relu activation @ hidden layer\n",
        "            x = torch.sigmoid(self.h2o(x)) # sigmoid activation @ output layer\n",
        "            return x\n",
        "\n",
        "    num_of_input = VOCAB_SIZE # Bag word restriction has to be equal to vocabulary\n",
        "    num_of_output = 1 # binary sentiment classes (+ve, -ve)\n",
        "\n",
        "    bow = BOWClassifier(num_of_input, num_of_hidden1, num_of_hidden2, num_of_output).to('cuda:0') # initialises weights and biases\n",
        "\n",
        "    # define a loss function and an optimizer\n",
        "    loss_function = nn.BCELoss()\n",
        "    opt = torch.optim.SGD(bow.parameters(), lr = 0.0001)\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    # Train The Model\n",
        "    epochs = 100\n",
        "    # the training loop\n",
        "    total_time = 0.0\n",
        "    prev_val_loss = float('inf')\n",
        "    val_loss = 0\n",
        "    early_stop_bow = 0\n",
        "\n",
        "    for e in range(epochs):\n",
        "        tic = time.time() # start the timer\n",
        "        correct = 0\n",
        "        cumulative_loss = 0\n",
        "        incorrect = 0\n",
        "\n",
        "        for i, instance in enumerate(train_data): # train_text_reviews \n",
        "            # get the training data\n",
        "            label = train_text_labels[i] # get the label of the corresponding instace\n",
        "            label = Variable(torch.FloatTensor([label])).resize_((1,1)).to('cuda:0') # wraps a tensor for label\n",
        "\n",
        "            bow.zero_grad() # reset the gradient for each instance\n",
        "\n",
        "            pred = bow.forward(instance)\n",
        "\n",
        "            loss = loss_function(pred, label) # compute the loss\n",
        "            loss.backward() # backprop the loss\n",
        "            opt.step() # performs parameter updation based on the current gradient\n",
        "\n",
        "            cumulative_loss += float(loss.item()) # accumulate the loss over whole training sample\n",
        "\n",
        "            pred_class = 1 if pred.item() > 0.5 else 0 # as sigmoid activated\n",
        "            if(int(label) == pred_class): # counting correct prediction in each epoch\n",
        "                correct += 1\n",
        "\n",
        "        train_loss = float(cumulative_loss)/num_train_data\n",
        "        train_accuracy = correct*float(100)/num_train_data\n",
        "\n",
        "#         val_loss, val_accuracy, p, r, f = evaluate_validation_accuracy(val_data, bow) # test val-data-set on currently trained model\n",
        "        val_loss = evaluate_validation_accuracy(val_data, bow) # test val-data-set on currently trained model\n",
        "        \n",
        "        if(prev_val_loss-val_loss < 0.0): # early stop if nearly no change\n",
        "            bow = copy.deepcopy(early_stop_bow)\n",
        "            break\n",
        "\n",
        "        early_stop_bow = copy.deepcopy(bow)\n",
        "\n",
        "        prev_val_loss = val_loss\n",
        "\n",
        "        toc = time.time() # final time\n",
        "        total_time += (toc-tic)\n",
        "\n",
        "        print(\"Epoch {}/{}\\n[On Training] ==> Time: {:.2f}s, Train Loss: {:.9f}, Train Accuracy: {:.2f}%\".format(e+1, epochs, (toc-tic), train_loss, train_accuracy))\n",
        "#         print(\"[On Validation] ==> Precision: {:.3f}, Recall: {:.3f}, F-Score: {:.3f}, Val loss: {:.9f}, Val Accuracy: {:.2f}% Total Time: {:.2f}s\".format(p, r, f, val_loss, val_accuracy, total_time))\n",
        "        print(\"[On Validation] ==> Val loss: {:.9f}, Total Time: {:.2f}s\".format(val_loss, total_time))\n",
        "\n",
        "    torch.save(bow, task_name)\n",
        "\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "[On Training] ==> Time: 24.51s, Train Loss: 0.689421182, Train Accuracy: 58.04%\n",
            "[On Validation] ==> Val loss: 0.684426882, Total Time: 24.51s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 24.51s, Train Loss: 0.679394369, Train Accuracy: 65.96%\n",
            "[On Validation] ==> Val loss: 0.673251196, Total Time: 49.02s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 24.50s, Train Loss: 0.665711512, Train Accuracy: 68.89%\n",
            "[On Validation] ==> Val loss: 0.656419923, Total Time: 73.52s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 24.58s, Train Loss: 0.644980399, Train Accuracy: 72.19%\n",
            "[On Validation] ==> Val loss: 0.631690873, Total Time: 98.10s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 24.52s, Train Loss: 0.615486051, Train Accuracy: 75.50%\n",
            "[On Validation] ==> Val loss: 0.597660946, Total Time: 122.62s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 24.82s, Train Loss: 0.575825682, Train Accuracy: 78.76%\n",
            "[On Validation] ==> Val loss: 0.554664992, Total Time: 147.45s\n",
            "Epoch 7/100\n",
            "[On Training] ==> Time: 24.52s, Train Loss: 0.528079712, Train Accuracy: 81.35%\n",
            "[On Validation] ==> Val loss: 0.508067211, Total Time: 171.96s\n",
            "Epoch 8/100\n",
            "[On Training] ==> Time: 24.51s, Train Loss: 0.478047534, Train Accuracy: 83.09%\n",
            "[On Validation] ==> Val loss: 0.464670608, Total Time: 196.47s\n",
            "Epoch 9/100\n",
            "[On Training] ==> Time: 24.52s, Train Loss: 0.431914570, Train Accuracy: 84.38%\n",
            "[On Validation] ==> Val loss: 0.429271350, Total Time: 221.00s\n",
            "Epoch 10/100\n",
            "[On Training] ==> Time: 24.46s, Train Loss: 0.392603311, Train Accuracy: 85.61%\n",
            "[On Validation] ==> Val loss: 0.402076328, Total Time: 245.46s\n",
            "Epoch 11/100\n",
            "[On Training] ==> Time: 24.99s, Train Loss: 0.359650431, Train Accuracy: 86.91%\n",
            "[On Validation] ==> Val loss: 0.381262800, Total Time: 270.45s\n",
            "Epoch 12/100\n",
            "[On Training] ==> Time: 24.49s, Train Loss: 0.331764028, Train Accuracy: 87.91%\n",
            "[On Validation] ==> Val loss: 0.365464613, Total Time: 294.95s\n",
            "Epoch 13/100\n",
            "[On Training] ==> Time: 24.42s, Train Loss: 0.307859366, Train Accuracy: 88.84%\n",
            "[On Validation] ==> Val loss: 0.353362345, Total Time: 319.36s\n",
            "Epoch 14/100\n",
            "[On Training] ==> Time: 24.41s, Train Loss: 0.287202695, Train Accuracy: 89.62%\n",
            "[On Validation] ==> Val loss: 0.344392954, Total Time: 343.77s\n",
            "Epoch 15/100\n",
            "[On Training] ==> Time: 24.44s, Train Loss: 0.269230133, Train Accuracy: 90.46%\n",
            "[On Validation] ==> Val loss: 0.337930498, Total Time: 368.22s\n",
            "Epoch 16/100\n",
            "[On Training] ==> Time: 24.45s, Train Loss: 0.253431854, Train Accuracy: 91.08%\n",
            "[On Validation] ==> Val loss: 0.333454437, Total Time: 392.67s\n",
            "Epoch 17/100\n",
            "[On Training] ==> Time: 24.38s, Train Loss: 0.239388438, Train Accuracy: 91.49%\n",
            "[On Validation] ==> Val loss: 0.330547962, Total Time: 417.05s\n",
            "Epoch 18/100\n",
            "[On Training] ==> Time: 24.41s, Train Loss: 0.226722627, Train Accuracy: 91.97%\n",
            "[On Validation] ==> Val loss: 0.328849619, Total Time: 441.46s\n",
            "Epoch 19/100\n",
            "[On Training] ==> Time: 24.79s, Train Loss: 0.215141124, Train Accuracy: 92.51%\n",
            "[On Validation] ==> Val loss: 0.328054509, Total Time: 466.25s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type BOWClassifier. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Epoch 1/100\n",
            "[On Training] ==> Time: 24.99s, Train Loss: 0.694682182, Train Accuracy: 49.98%\n",
            "[On Validation] ==> Val loss: 0.690707997, Total Time: 24.99s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 24.94s, Train Loss: 0.687139104, Train Accuracy: 51.33%\n",
            "[On Validation] ==> Val loss: 0.681724726, Total Time: 49.93s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 25.13s, Train Loss: 0.675940750, Train Accuracy: 62.92%\n",
            "[On Validation] ==> Val loss: 0.668003307, Total Time: 75.06s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 25.15s, Train Loss: 0.659160365, Train Accuracy: 72.10%\n",
            "[On Validation] ==> Val loss: 0.647091466, Total Time: 100.22s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 25.00s, Train Loss: 0.632785946, Train Accuracy: 74.40%\n",
            "[On Validation] ==> Val loss: 0.614805578, Total Time: 125.22s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 24.96s, Train Loss: 0.593166607, Train Accuracy: 76.78%\n",
            "[On Validation] ==> Val loss: 0.569449981, Total Time: 150.17s\n",
            "Epoch 7/100\n",
            "[On Training] ==> Time: 24.90s, Train Loss: 0.541100827, Train Accuracy: 79.44%\n",
            "[On Validation] ==> Val loss: 0.516519530, Total Time: 175.08s\n",
            "Epoch 8/100\n",
            "[On Training] ==> Time: 24.95s, Train Loss: 0.485364500, Train Accuracy: 81.66%\n",
            "[On Validation] ==> Val loss: 0.467574662, Total Time: 200.03s\n",
            "Epoch 9/100\n",
            "[On Training] ==> Time: 25.06s, Train Loss: 0.435959637, Train Accuracy: 83.53%\n",
            "[On Validation] ==> Val loss: 0.429521457, Total Time: 225.09s\n",
            "Epoch 10/100\n",
            "[On Training] ==> Time: 25.15s, Train Loss: 0.395712767, Train Accuracy: 84.99%\n",
            "[On Validation] ==> Val loss: 0.401385885, Total Time: 250.24s\n",
            "Epoch 11/100\n",
            "[On Training] ==> Time: 25.29s, Train Loss: 0.362699174, Train Accuracy: 86.18%\n",
            "[On Validation] ==> Val loss: 0.380343804, Total Time: 275.53s\n",
            "Epoch 12/100\n",
            "[On Training] ==> Time: 24.98s, Train Loss: 0.334798250, Train Accuracy: 87.36%\n",
            "[On Validation] ==> Val loss: 0.364272562, Total Time: 300.51s\n",
            "Epoch 13/100\n",
            "[On Training] ==> Time: 24.85s, Train Loss: 0.310686978, Train Accuracy: 88.36%\n",
            "[On Validation] ==> Val loss: 0.352103375, Total Time: 325.36s\n",
            "Epoch 14/100\n",
            "[On Training] ==> Time: 25.10s, Train Loss: 0.289700043, Train Accuracy: 89.10%\n",
            "[On Validation] ==> Val loss: 0.343084138, Total Time: 350.46s\n",
            "Epoch 15/100\n",
            "[On Training] ==> Time: 25.05s, Train Loss: 0.271387189, Train Accuracy: 89.96%\n",
            "[On Validation] ==> Val loss: 0.336424828, Total Time: 375.51s\n",
            "Epoch 16/100\n",
            "[On Training] ==> Time: 25.50s, Train Loss: 0.255300525, Train Accuracy: 90.75%\n",
            "[On Validation] ==> Val loss: 0.331645716, Total Time: 401.00s\n",
            "Epoch 17/100\n",
            "[On Training] ==> Time: 25.08s, Train Loss: 0.240983734, Train Accuracy: 91.32%\n",
            "[On Validation] ==> Val loss: 0.328363942, Total Time: 426.09s\n",
            "Epoch 18/100\n",
            "[On Training] ==> Time: 25.15s, Train Loss: 0.228094389, Train Accuracy: 91.79%\n",
            "[On Validation] ==> Val loss: 0.326269305, Total Time: 451.23s\n",
            "Epoch 19/100\n",
            "[On Training] ==> Time: 25.13s, Train Loss: 0.216326000, Train Accuracy: 92.34%\n",
            "[On Validation] ==> Val loss: 0.325266063, Total Time: 476.37s\n",
            "Epoch 20/100\n",
            "[On Training] ==> Time: 25.24s, Train Loss: 0.205493376, Train Accuracy: 92.80%\n",
            "[On Validation] ==> Val loss: 0.325161850, Total Time: 501.61s\n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "[On Training] ==> Time: 25.59s, Train Loss: 0.691708546, Train Accuracy: 50.45%\n",
            "[On Validation] ==> Val loss: 0.687880974, Total Time: 25.59s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 25.91s, Train Loss: 0.683738443, Train Accuracy: 59.41%\n",
            "[On Validation] ==> Val loss: 0.678444458, Total Time: 51.50s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 25.55s, Train Loss: 0.672228311, Train Accuracy: 69.21%\n",
            "[On Validation] ==> Val loss: 0.663916428, Total Time: 77.06s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 25.64s, Train Loss: 0.653295451, Train Accuracy: 73.23%\n",
            "[On Validation] ==> Val loss: 0.639674293, Total Time: 102.69s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 25.49s, Train Loss: 0.621814271, Train Accuracy: 76.16%\n",
            "[On Validation] ==> Val loss: 0.601028089, Total Time: 128.18s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 25.55s, Train Loss: 0.574794819, Train Accuracy: 78.78%\n",
            "[On Validation] ==> Val loss: 0.548961556, Total Time: 153.73s\n",
            "Epoch 7/100\n",
            "[On Training] ==> Time: 25.98s, Train Loss: 0.517580539, Train Accuracy: 81.18%\n",
            "[On Validation] ==> Val loss: 0.494374787, Total Time: 179.71s\n",
            "Epoch 8/100\n",
            "[On Training] ==> Time: 25.62s, Train Loss: 0.462209930, Train Accuracy: 83.03%\n",
            "[On Validation] ==> Val loss: 0.448949969, Total Time: 205.33s\n",
            "Epoch 9/100\n",
            "[On Training] ==> Time: 25.61s, Train Loss: 0.416176619, Train Accuracy: 84.36%\n",
            "[On Validation] ==> Val loss: 0.415428632, Total Time: 230.95s\n",
            "Epoch 10/100\n",
            "[On Training] ==> Time: 25.60s, Train Loss: 0.379103702, Train Accuracy: 85.75%\n",
            "[On Validation] ==> Val loss: 0.390820144, Total Time: 256.55s\n",
            "Epoch 11/100\n",
            "[On Training] ==> Time: 25.55s, Train Loss: 0.348316810, Train Accuracy: 86.92%\n",
            "[On Validation] ==> Val loss: 0.372234728, Total Time: 282.10s\n",
            "Epoch 12/100\n",
            "[On Training] ==> Time: 25.58s, Train Loss: 0.322043761, Train Accuracy: 87.92%\n",
            "[On Validation] ==> Val loss: 0.358024265, Total Time: 307.68s\n",
            "Epoch 13/100\n",
            "[On Training] ==> Time: 25.47s, Train Loss: 0.299343379, Train Accuracy: 88.84%\n",
            "[On Validation] ==> Val loss: 0.347368676, Total Time: 333.15s\n",
            "Epoch 14/100\n",
            "[On Training] ==> Time: 25.71s, Train Loss: 0.279660251, Train Accuracy: 89.73%\n",
            "[On Validation] ==> Val loss: 0.339556322, Total Time: 358.85s\n",
            "Epoch 15/100\n",
            "[On Training] ==> Time: 25.54s, Train Loss: 0.262473111, Train Accuracy: 90.45%\n",
            "[On Validation] ==> Val loss: 0.333928626, Total Time: 384.39s\n",
            "Epoch 16/100\n",
            "[On Training] ==> Time: 25.54s, Train Loss: 0.247318356, Train Accuracy: 91.12%\n",
            "[On Validation] ==> Val loss: 0.330006113, Total Time: 409.93s\n",
            "Epoch 17/100\n",
            "[On Training] ==> Time: 25.48s, Train Loss: 0.233772271, Train Accuracy: 91.60%\n",
            "[On Validation] ==> Val loss: 0.327426775, Total Time: 435.41s\n",
            "Epoch 18/100\n",
            "[On Training] ==> Time: 25.43s, Train Loss: 0.221510454, Train Accuracy: 92.08%\n",
            "[On Validation] ==> Val loss: 0.325972469, Total Time: 460.84s\n",
            "Epoch 19/100\n",
            "[On Training] ==> Time: 25.80s, Train Loss: 0.210284573, Train Accuracy: 92.64%\n",
            "[On Validation] ==> Val loss: 0.325482601, Total Time: 486.64s\n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "[On Training] ==> Time: 26.09s, Train Loss: 0.690844142, Train Accuracy: 56.98%\n",
            "[On Validation] ==> Val loss: 0.687873020, Total Time: 26.09s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 26.08s, Train Loss: 0.684706496, Train Accuracy: 67.66%\n",
            "[On Validation] ==> Val loss: 0.680298485, Total Time: 52.18s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 26.07s, Train Loss: 0.675163021, Train Accuracy: 71.58%\n",
            "[On Validation] ==> Val loss: 0.668093843, Total Time: 78.25s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 26.02s, Train Loss: 0.659132799, Train Accuracy: 74.16%\n",
            "[On Validation] ==> Val loss: 0.647630779, Total Time: 104.28s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 26.03s, Train Loss: 0.631848473, Train Accuracy: 76.53%\n",
            "[On Validation] ==> Val loss: 0.613632087, Total Time: 130.31s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 26.28s, Train Loss: 0.588568966, Train Accuracy: 78.80%\n",
            "[On Validation] ==> Val loss: 0.563971968, Total Time: 156.59s\n",
            "Epoch 7/100\n",
            "[On Training] ==> Time: 26.04s, Train Loss: 0.531960968, Train Accuracy: 80.90%\n",
            "[On Validation] ==> Val loss: 0.507690117, Total Time: 182.63s\n",
            "Epoch 8/100\n",
            "[On Training] ==> Time: 26.06s, Train Loss: 0.474332816, Train Accuracy: 82.62%\n",
            "[On Validation] ==> Val loss: 0.458740161, Total Time: 208.69s\n",
            "Epoch 9/100\n",
            "[On Training] ==> Time: 26.09s, Train Loss: 0.425536875, Train Accuracy: 83.99%\n",
            "[On Validation] ==> Val loss: 0.422187640, Total Time: 234.79s\n",
            "Epoch 10/100\n",
            "[On Training] ==> Time: 26.02s, Train Loss: 0.386396007, Train Accuracy: 85.52%\n",
            "[On Validation] ==> Val loss: 0.395579551, Total Time: 260.81s\n",
            "Epoch 11/100\n",
            "[On Training] ==> Time: 26.45s, Train Loss: 0.354210745, Train Accuracy: 86.67%\n",
            "[On Validation] ==> Val loss: 0.375726390, Total Time: 287.26s\n",
            "Epoch 12/100\n",
            "[On Training] ==> Time: 26.00s, Train Loss: 0.326962539, Train Accuracy: 87.84%\n",
            "[On Validation] ==> Val loss: 0.360807357, Total Time: 313.26s\n",
            "Epoch 13/100\n",
            "[On Training] ==> Time: 26.08s, Train Loss: 0.303510840, Train Accuracy: 88.73%\n",
            "[On Validation] ==> Val loss: 0.349753482, Total Time: 339.34s\n",
            "Epoch 14/100\n",
            "[On Training] ==> Time: 26.03s, Train Loss: 0.283199370, Train Accuracy: 89.63%\n",
            "[On Validation] ==> Val loss: 0.341730797, Total Time: 365.37s\n",
            "Epoch 15/100\n",
            "[On Training] ==> Time: 26.05s, Train Loss: 0.265506490, Train Accuracy: 90.25%\n",
            "[On Validation] ==> Val loss: 0.335976365, Total Time: 391.43s\n",
            "Epoch 16/100\n",
            "[On Training] ==> Time: 26.01s, Train Loss: 0.249915905, Train Accuracy: 90.94%\n",
            "[On Validation] ==> Val loss: 0.331947201, Total Time: 417.44s\n",
            "Epoch 17/100\n",
            "[On Training] ==> Time: 26.11s, Train Loss: 0.235996427, Train Accuracy: 91.49%\n",
            "[On Validation] ==> Val loss: 0.329367963, Total Time: 443.55s\n",
            "Epoch 18/100\n",
            "[On Training] ==> Time: 26.13s, Train Loss: 0.223431750, Train Accuracy: 91.99%\n",
            "[On Validation] ==> Val loss: 0.327960200, Total Time: 469.68s\n",
            "Epoch 19/100\n",
            "[On Training] ==> Time: 26.03s, Train Loss: 0.211944162, Train Accuracy: 92.45%\n",
            "[On Validation] ==> Val loss: 0.327513765, Total Time: 495.71s\n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "[On Training] ==> Time: 29.96s, Train Loss: 0.692269038, Train Accuracy: 50.07%\n",
            "[On Validation] ==> Val loss: 0.689759713, Total Time: 29.96s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 29.83s, Train Loss: 0.686658623, Train Accuracy: 55.33%\n",
            "[On Validation] ==> Val loss: 0.682453607, Total Time: 59.80s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 30.00s, Train Loss: 0.677468912, Train Accuracy: 65.63%\n",
            "[On Validation] ==> Val loss: 0.670410879, Total Time: 89.80s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 29.79s, Train Loss: 0.662098462, Train Accuracy: 71.77%\n",
            "[On Validation] ==> Val loss: 0.650709788, Total Time: 119.60s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 29.93s, Train Loss: 0.636161985, Train Accuracy: 75.15%\n",
            "[On Validation] ==> Val loss: 0.618412222, Total Time: 149.53s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 29.80s, Train Loss: 0.595248755, Train Accuracy: 77.70%\n",
            "[On Validation] ==> Val loss: 0.571298826, Total Time: 179.33s\n",
            "Epoch 7/100\n",
            "[On Training] ==> Time: 29.78s, Train Loss: 0.540917761, Train Accuracy: 80.10%\n",
            "[On Validation] ==> Val loss: 0.516206182, Total Time: 209.11s\n",
            "Epoch 8/100\n",
            "[On Training] ==> Time: 29.88s, Train Loss: 0.483913137, Train Accuracy: 82.18%\n",
            "[On Validation] ==> Val loss: 0.466454143, Total Time: 238.99s\n",
            "Epoch 9/100\n",
            "[On Training] ==> Time: 29.90s, Train Loss: 0.434539593, Train Accuracy: 83.74%\n",
            "[On Validation] ==> Val loss: 0.428430034, Total Time: 268.90s\n",
            "Epoch 10/100\n",
            "[On Training] ==> Time: 29.75s, Train Loss: 0.394634003, Train Accuracy: 85.06%\n",
            "[On Validation] ==> Val loss: 0.400548802, Total Time: 298.64s\n",
            "Epoch 11/100\n",
            "[On Training] ==> Time: 29.70s, Train Loss: 0.361781914, Train Accuracy: 86.34%\n",
            "[On Validation] ==> Val loss: 0.379667665, Total Time: 328.35s\n",
            "Epoch 12/100\n",
            "[On Training] ==> Time: 29.78s, Train Loss: 0.333873167, Train Accuracy: 87.36%\n",
            "[On Validation] ==> Val loss: 0.363793036, Total Time: 358.13s\n",
            "Epoch 13/100\n",
            "[On Training] ==> Time: 30.13s, Train Loss: 0.309777845, Train Accuracy: 88.38%\n",
            "[On Validation] ==> Val loss: 0.351815082, Total Time: 388.26s\n",
            "Epoch 14/100\n",
            "[On Training] ==> Time: 29.72s, Train Loss: 0.288857272, Train Accuracy: 89.18%\n",
            "[On Validation] ==> Val loss: 0.342953388, Total Time: 417.97s\n",
            "Epoch 15/100\n",
            "[On Training] ==> Time: 29.64s, Train Loss: 0.270634395, Train Accuracy: 90.05%\n",
            "[On Validation] ==> Val loss: 0.336485022, Total Time: 447.61s\n",
            "Epoch 16/100\n",
            "[On Training] ==> Time: 29.75s, Train Loss: 0.254615866, Train Accuracy: 90.69%\n",
            "[On Validation] ==> Val loss: 0.331857529, Total Time: 477.36s\n",
            "Epoch 17/100\n",
            "[On Training] ==> Time: 29.94s, Train Loss: 0.240366603, Train Accuracy: 91.17%\n",
            "[On Validation] ==> Val loss: 0.328702211, Total Time: 507.30s\n",
            "Epoch 18/100\n",
            "[On Training] ==> Time: 29.74s, Train Loss: 0.227516833, Train Accuracy: 91.80%\n",
            "[On Validation] ==> Val loss: 0.326793447, Total Time: 537.04s\n",
            "Epoch 19/100\n",
            "[On Training] ==> Time: 29.68s, Train Loss: 0.215805692, Train Accuracy: 92.35%\n",
            "[On Validation] ==> Val loss: 0.325901880, Total Time: 566.72s\n",
            "Epoch 20/100\n",
            "[On Training] ==> Time: 29.77s, Train Loss: 0.205030306, Train Accuracy: 92.86%\n",
            "[On Validation] ==> Val loss: 0.325890888, Total Time: 596.49s\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eMMAv_S1WLNF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download('Task2A.mdl')\n",
        "files.download('Task2B.mdl')\n",
        "files.download('Task2C.mdl')\n",
        "files.download('Task2D.mdl')\n",
        "files.download('Task2E.mdl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n2JByG_xnrr1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Task-3"
      ]
    },
    {
      "metadata": {
        "id": "vtAGGeuU2ect",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "task3 = [(100, 50, 10, 'Task3A.mdl'), (200, 100, 10, 'Task3B.mdl')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rz1rm-runrPt",
        "colab_type": "code",
        "outputId": "47a79a85-8cde-4151-997c-8ba8e3792e21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2332
        }
      },
      "cell_type": "code",
      "source": [
        "for num_of_hidden1, num_of_hidden2, num_of_hidden3, task_name in task3:\n",
        "    class BOWClassifier(nn.Module):\n",
        "        def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
        "            super(BOWClassifier, self).__init__()\n",
        "            SEED = 42\n",
        "            torch.manual_seed(SEED)\n",
        "            torch.cuda.manual_seed(SEED)\n",
        "            self.i2h1 = nn.Linear(input_size, hidden_size1) # initialises weights and biases i2h1\n",
        "            self.h12h2 = nn.Linear(hidden_size1, hidden_size2) # initialises weights and biases h12h2\n",
        "            self.h22h3 = nn.Linear(hidden_size2, hidden_size3) # initialises weights and biases h22h3\n",
        "            self.h32o = nn.Linear(hidden_size3, output_size) # initialises weights and biases h32o\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = torch.relu(self.i2h1(x)) # relu activation @ hidden layer\n",
        "            x = torch.relu(self.h12h2(x)) # relu activation @ hidden layer\n",
        "            x = torch.relu(self.h22h3(x)) # relu activation @ hidden layer\n",
        "            x = torch.sigmoid(self.h32o(x)) # sigmoid activation @ output layer\n",
        "            return x\n",
        "\n",
        "    num_of_input = VOCAB_SIZE # Bag word restriction has to be equal to vocabulary\n",
        "    num_of_output = 1 # binary sentiment classes (+ve, -ve)\n",
        "\n",
        "    bow = BOWClassifier(num_of_input, num_of_hidden1, num_of_hidden2, num_of_hidden3, num_of_output).to('cuda:0') # initialises weights and biases\n",
        "\n",
        "    # define a loss function and an optimizer\n",
        "    loss_function = nn.BCELoss()\n",
        "    opt = torch.optim.SGD(bow.parameters(), lr = 0.0001)\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    # Train The Model\n",
        "    epochs = 100\n",
        "    # the training loop\n",
        "    total_time = 0.0\n",
        "    prev_val_loss = float('inf')\n",
        "    val_loss = 0\n",
        "    early_stop_bow = 0\n",
        "\n",
        "    for e in range(epochs):\n",
        "        tic = time.time() # start the timer\n",
        "        correct = 0\n",
        "        cumulative_loss = 0\n",
        "        incorrect = 0\n",
        "\n",
        "        for i, instance in enumerate(train_data): # train_text_reviews \n",
        "            # get the training data\n",
        "            label = train_text_labels[i] # get the label of the corresponding instace\n",
        "            label = Variable(torch.FloatTensor([label])).resize_((1,1)).to('cuda:0') # wraps a tensor for label\n",
        "\n",
        "            bow.zero_grad() # reset the gradient for each instance\n",
        "\n",
        "            pred = bow.forward(instance)\n",
        "\n",
        "            loss = loss_function(pred, label) # compute the loss\n",
        "            loss.backward() # backprop the loss\n",
        "            opt.step() # performs parameter updation based on the current gradient\n",
        "\n",
        "            cumulative_loss += float(loss.item()) # accumulate the loss over whole training sample\n",
        "\n",
        "            pred_class = 1 if pred.item() > 0.5 else 0 # as sigmoid activated\n",
        "            if(int(label) == pred_class): # counting correct prediction in each epoch\n",
        "                correct += 1\n",
        "\n",
        "        train_loss = float(cumulative_loss)/num_train_data\n",
        "        train_accuracy = correct*float(100)/num_train_data\n",
        "\n",
        "#         val_loss, val_accuracy, p, r, f = evaluate_validation_accuracy(val_data, bow) # test val-data-set on currently trained model\n",
        "        val_loss = evaluate_validation_accuracy(val_data, bow) # test val-data-set on currently trained model\n",
        "\n",
        "        if(prev_val_loss-val_loss < 0.0): # early stop if nearly no change\n",
        "            bow = copy.deepcopy(early_stop_bow)\n",
        "            break\n",
        "\n",
        "        early_stop_bow = copy.deepcopy(bow)\n",
        "\n",
        "        prev_val_loss = val_loss\n",
        "\n",
        "        toc = time.time() # final time\n",
        "        total_time += (toc-tic)\n",
        "\n",
        "        print(\"Epoch {}/{}\\n[On Training] ==> Time: {:.2f}s, Train Loss: {:.9f}, Train Accuracy: {:.2f}%\".format(e+1, epochs, (toc-tic), train_loss, train_accuracy))\n",
        "#         print(\"[On Validation] ==> Precision: {:.3f}, Recall: {:.3f}, F-Score: {:.3f}, Val loss: {:.9f}, Val Accuracy: {:.2f}% Total Time: {:.2f}s\".format(p, r, f, val_loss, val_accuracy, total_time))\n",
        "        print(\"[On Validation] ==> Val loss: {:.9f}, Total Time: {:.2f}s\".format(val_loss, total_time))\n",
        "    \n",
        "    torch.save(bow, task_name)\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "[On Training] ==> Time: 32.90s, Train Loss: 0.695067301, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.694134274, Total Time: 32.90s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 32.98s, Train Loss: 0.693484625, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.692994911, Total Time: 65.88s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 33.09s, Train Loss: 0.692508921, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.692160799, Total Time: 98.97s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 32.90s, Train Loss: 0.691661817, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.691316850, Total Time: 131.87s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 32.99s, Train Loss: 0.690723652, Train Accuracy: 50.14%\n",
            "[On Validation] ==> Val loss: 0.690297857, Total Time: 164.85s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 32.85s, Train Loss: 0.689487600, Train Accuracy: 56.61%\n",
            "[On Validation] ==> Val loss: 0.688804753, Total Time: 197.71s\n",
            "Epoch 7/100\n",
            "[On Training] ==> Time: 32.78s, Train Loss: 0.687412746, Train Accuracy: 65.63%\n",
            "[On Validation] ==> Val loss: 0.686148670, Total Time: 230.48s\n",
            "Epoch 8/100\n",
            "[On Training] ==> Time: 33.03s, Train Loss: 0.684048856, Train Accuracy: 67.79%\n",
            "[On Validation] ==> Val loss: 0.682153922, Total Time: 263.51s\n",
            "Epoch 9/100\n",
            "[On Training] ==> Time: 32.92s, Train Loss: 0.679069688, Train Accuracy: 69.27%\n",
            "[On Validation] ==> Val loss: 0.676040388, Total Time: 296.43s\n",
            "Epoch 10/100\n",
            "[On Training] ==> Time: 32.89s, Train Loss: 0.671152555, Train Accuracy: 70.72%\n",
            "[On Validation] ==> Val loss: 0.666213196, Total Time: 329.32s\n",
            "Epoch 11/100\n",
            "[On Training] ==> Time: 32.95s, Train Loss: 0.658283695, Train Accuracy: 72.27%\n",
            "[On Validation] ==> Val loss: 0.650273447, Total Time: 362.27s\n",
            "Epoch 12/100\n",
            "[On Training] ==> Time: 32.91s, Train Loss: 0.637355485, Train Accuracy: 74.29%\n",
            "[On Validation] ==> Val loss: 0.624653579, Total Time: 395.18s\n",
            "Epoch 13/100\n",
            "[On Training] ==> Time: 33.11s, Train Loss: 0.604715171, Train Accuracy: 77.07%\n",
            "[On Validation] ==> Val loss: 0.586663317, Total Time: 428.29s\n",
            "Epoch 14/100\n",
            "[On Training] ==> Time: 32.78s, Train Loss: 0.558278972, Train Accuracy: 80.00%\n",
            "[On Validation] ==> Val loss: 0.536776084, Total Time: 461.07s\n",
            "Epoch 15/100\n",
            "[On Training] ==> Time: 32.87s, Train Loss: 0.500564620, Train Accuracy: 82.45%\n",
            "[On Validation] ==> Val loss: 0.482089832, Total Time: 493.94s\n",
            "Epoch 16/100\n",
            "[On Training] ==> Time: 32.90s, Train Loss: 0.440410204, Train Accuracy: 84.21%\n",
            "[On Validation] ==> Val loss: 0.433217083, Total Time: 526.84s\n",
            "Epoch 17/100\n",
            "[On Training] ==> Time: 33.12s, Train Loss: 0.387011500, Train Accuracy: 85.75%\n",
            "[On Validation] ==> Val loss: 0.396308045, Total Time: 559.96s\n",
            "Epoch 18/100\n",
            "[On Training] ==> Time: 32.84s, Train Loss: 0.343263168, Train Accuracy: 87.39%\n",
            "[On Validation] ==> Val loss: 0.370524449, Total Time: 592.80s\n",
            "Epoch 19/100\n",
            "[On Training] ==> Time: 32.86s, Train Loss: 0.307474746, Train Accuracy: 88.66%\n",
            "[On Validation] ==> Val loss: 0.353166263, Total Time: 625.66s\n",
            "Epoch 20/100\n",
            "[On Training] ==> Time: 32.91s, Train Loss: 0.277937054, Train Accuracy: 89.83%\n",
            "[On Validation] ==> Val loss: 0.342089181, Total Time: 658.57s\n",
            "Epoch 21/100\n",
            "[On Training] ==> Time: 32.69s, Train Loss: 0.253244421, Train Accuracy: 90.96%\n",
            "[On Validation] ==> Val loss: 0.335871343, Total Time: 691.26s\n",
            "Epoch 22/100\n",
            "[On Training] ==> Time: 33.13s, Train Loss: 0.232256034, Train Accuracy: 91.75%\n",
            "[On Validation] ==> Val loss: 0.333261355, Total Time: 724.39s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type BOWClassifier. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Epoch 1/100\n",
            "[On Training] ==> Time: 36.89s, Train Loss: 0.692999312, Train Accuracy: 49.99%\n",
            "[On Validation] ==> Val loss: 0.692633849, Total Time: 36.89s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 36.85s, Train Loss: 0.692311204, Train Accuracy: 50.77%\n",
            "[On Validation] ==> Val loss: 0.691836207, Total Time: 73.74s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 37.13s, Train Loss: 0.691399423, Train Accuracy: 56.72%\n",
            "[On Validation] ==> Val loss: 0.690782112, Total Time: 110.87s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 37.31s, Train Loss: 0.690142062, Train Accuracy: 64.14%\n",
            "[On Validation] ==> Val loss: 0.689282023, Total Time: 148.17s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 37.00s, Train Loss: 0.688220445, Train Accuracy: 67.48%\n",
            "[On Validation] ==> Val loss: 0.686881523, Total Time: 185.18s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 37.04s, Train Loss: 0.685253689, Train Accuracy: 68.01%\n",
            "[On Validation] ==> Val loss: 0.683452179, Total Time: 222.21s\n",
            "Epoch 7/100\n",
            "[On Training] ==> Time: 37.05s, Train Loss: 0.681003536, Train Accuracy: 69.39%\n",
            "[On Validation] ==> Val loss: 0.678294469, Total Time: 259.26s\n",
            "Epoch 8/100\n",
            "[On Training] ==> Time: 37.09s, Train Loss: 0.674392150, Train Accuracy: 70.63%\n",
            "[On Validation] ==> Val loss: 0.670162914, Total Time: 296.36s\n",
            "Epoch 9/100\n",
            "[On Training] ==> Time: 37.05s, Train Loss: 0.663957500, Train Accuracy: 71.95%\n",
            "[On Validation] ==> Val loss: 0.657327972, Total Time: 333.41s\n",
            "Epoch 10/100\n",
            "[On Training] ==> Time: 37.15s, Train Loss: 0.647571198, Train Accuracy: 73.68%\n",
            "[On Validation] ==> Val loss: 0.637375634, Total Time: 370.56s\n",
            "Epoch 11/100\n",
            "[On Training] ==> Time: 37.24s, Train Loss: 0.622509502, Train Accuracy: 76.07%\n",
            "[On Validation] ==> Val loss: 0.607718767, Total Time: 407.80s\n",
            "Epoch 12/100\n",
            "[On Training] ==> Time: 37.00s, Train Loss: 0.586287715, Train Accuracy: 78.86%\n",
            "[On Validation] ==> Val loss: 0.567289468, Total Time: 444.80s\n",
            "Epoch 13/100\n",
            "[On Training] ==> Time: 37.19s, Train Loss: 0.538835200, Train Accuracy: 81.62%\n",
            "[On Validation] ==> Val loss: 0.518972735, Total Time: 481.99s\n",
            "Epoch 14/100\n",
            "[On Training] ==> Time: 37.24s, Train Loss: 0.484139662, Train Accuracy: 83.43%\n",
            "[On Validation] ==> Val loss: 0.468952735, Total Time: 519.23s\n",
            "Epoch 15/100\n",
            "[On Training] ==> Time: 37.11s, Train Loss: 0.428338169, Train Accuracy: 84.95%\n",
            "[On Validation] ==> Val loss: 0.424181763, Total Time: 556.33s\n",
            "Epoch 16/100\n",
            "[On Training] ==> Time: 37.33s, Train Loss: 0.377881556, Train Accuracy: 86.44%\n",
            "[On Validation] ==> Val loss: 0.389688761, Total Time: 593.66s\n",
            "Epoch 17/100\n",
            "[On Training] ==> Time: 37.14s, Train Loss: 0.335922818, Train Accuracy: 87.86%\n",
            "[On Validation] ==> Val loss: 0.365453869, Total Time: 630.80s\n",
            "Epoch 18/100\n",
            "[On Training] ==> Time: 37.07s, Train Loss: 0.301628964, Train Accuracy: 89.07%\n",
            "[On Validation] ==> Val loss: 0.349519926, Total Time: 667.87s\n",
            "Epoch 19/100\n",
            "[On Training] ==> Time: 37.24s, Train Loss: 0.273272724, Train Accuracy: 90.18%\n",
            "[On Validation] ==> Val loss: 0.339662957, Total Time: 705.10s\n",
            "Epoch 20/100\n",
            "[On Training] ==> Time: 37.23s, Train Loss: 0.249484998, Train Accuracy: 91.23%\n",
            "[On Validation] ==> Val loss: 0.334362202, Total Time: 742.34s\n",
            "Epoch 21/100\n",
            "[On Training] ==> Time: 36.77s, Train Loss: 0.229029350, Train Accuracy: 92.04%\n",
            "[On Validation] ==> Val loss: 0.332416809, Total Time: 779.10s\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vGf7IdbRdbk7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download('Task3A.mdl')\n",
        "files.download('Task3B.mdl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-MYVmtv9jn7n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "quMcuqqyjqP2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## TASK-4"
      ]
    },
    {
      "metadata": {
        "id": "FznXcSrri_l6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "task4 = [(30, 20, 10, 'Task4A.mdl'), (100, 100, 0, 'Task4B.mdl'), (100, 10, 0, 'Task4C.mdl')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TMTr6ZqQjoZA",
        "colab_type": "code",
        "outputId": "9d2d1401-8b6f-4006-a860-66cb368b759b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3335
        }
      },
      "cell_type": "code",
      "source": [
        "k = 1\n",
        "for num_of_hidden1, num_of_hidden2, num_of_hidden3, task_name in task4:\n",
        "    if(k == 1):\n",
        "        k = 2\n",
        "        class BOWClassifier(nn.Module):\n",
        "            def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
        "                super(BOWClassifier, self).__init__()\n",
        "                SEED = 42\n",
        "                torch.manual_seed(SEED)\n",
        "                torch.cuda.manual_seed(SEED)\n",
        "                self.i2h1 = nn.Linear(input_size, hidden_size1) # initialises weights and biases i2h1\n",
        "                self.drop1 = nn.Dropout(0.5) # Adding dropout layer\n",
        "                \n",
        "                self.h12h2 = nn.Linear(hidden_size1, hidden_size2) # initialises weights and biases h12h2\n",
        "                self.drop2 = nn.Dropout(0.5) # Adding dropout layer\n",
        "                \n",
        "                self.h22h3 = nn.Linear(hidden_size2, hidden_size3) # initialises weights and biases h22h3\n",
        "                self.drop3 = nn.Dropout(0.5) # Adding dropout layer\n",
        "                \n",
        "                self.h32o = nn.Linear(hidden_size3, output_size) # initialises weights and biases h32o\n",
        "\n",
        "            def forward(self, x):\n",
        "                x = torch.relu(self.i2h1(x)) # relu activation @ hidden layer\n",
        "                x = torch.relu(self.h12h2(x)) # relu activation @ hidden layer\n",
        "                x = torch.relu(self.h22h3(x)) # relu activation @ hidden layer\n",
        "                x = torch.sigmoid(self.h32o(x)) # sigmoid activation @ output layer\n",
        "                return x\n",
        "\n",
        "        num_of_input = VOCAB_SIZE # Bag word restriction has to be equal to vocabulary\n",
        "        num_of_output = 1 # binary sentiment classes (+ve, -ve)\n",
        "\n",
        "        bow = BOWClassifier(num_of_input, num_of_hidden1, num_of_hidden2, num_of_hidden3, num_of_output).to('cuda:0') # initialises weights and biases\n",
        "    elif(k == 2):\n",
        "        k = 2\n",
        "        class BOWClassifier(nn.Module):\n",
        "            def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
        "                super(BOWClassifier, self).__init__()\n",
        "                SEED = 42\n",
        "                torch.manual_seed(SEED)\n",
        "                torch.cuda.manual_seed(SEED)\n",
        "                self.i2h = nn.Linear(input_size, hidden_size1) # initialises weights and biases i2h\n",
        "                self.drop1 = nn.Dropout(0.5) # Adding dropout layer\n",
        "                \n",
        "                self.h2h = nn.Linear(hidden_size1, hidden_size2) # initialises weights and biases i2h\n",
        "                self.drop2 = nn.Dropout(0.5) # Adding dropout layer\n",
        "                \n",
        "                self.h2o = nn.Linear(hidden_size2, output_size) # initialises weights and biases h2o\n",
        "\n",
        "            def forward(self, x):\n",
        "                x = torch.relu(self.i2h(x)) # relu activation @ hidden layer\n",
        "                x = torch.relu(self.h2h(x)) # relu activation @ hidden layer\n",
        "                x = torch.sigmoid(self.h2o(x)) # sigmoid activation @ output layer\n",
        "                return x\n",
        "\n",
        "        num_of_input = VOCAB_SIZE # Bag word restriction has to be equal to vocabulary\n",
        "        num_of_output = 1 # binary sentiment classes (+ve, -ve)\n",
        "\n",
        "        bow = BOWClassifier(num_of_input, num_of_hidden1, num_of_hidden2, num_of_output).to('cuda:0') # initialises weights and biases\n",
        "\n",
        "    # define a loss function and an optimizer\n",
        "    loss_function = nn.BCELoss()\n",
        "    opt = torch.optim.SGD(bow.parameters(), lr = 0.0001)\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    # Train The Model\n",
        "    epochs = 100\n",
        "    # the training loop\n",
        "    total_time = 0.0\n",
        "    prev_val_loss = float('inf')\n",
        "    val_loss = 0\n",
        "    early_stop_bow = 0\n",
        "\n",
        "    for e in range(epochs):\n",
        "        tic = time.time() # start the timer\n",
        "        correct = 0\n",
        "        cumulative_loss = 0\n",
        "        incorrect = 0\n",
        "\n",
        "        for i, instance in enumerate(train_data): # train_text_reviews \n",
        "            # get the training data\n",
        "            label = train_text_labels[i] # get the label of the corresponding instace\n",
        "            label = Variable(torch.FloatTensor([label])).resize_((1,1)).to('cuda:0') # wraps a tensor for label\n",
        "\n",
        "            bow.zero_grad() # reset the gradient for each instance\n",
        "\n",
        "            pred = bow.forward(instance)\n",
        "\n",
        "            loss = loss_function(pred, label) # compute the loss\n",
        "            loss.backward() # backprop the loss\n",
        "            opt.step() # performs parameter updation based on the current gradient\n",
        "\n",
        "            cumulative_loss += float(loss.item()) # accumulate the loss over whole training sample\n",
        "\n",
        "            pred_class = 1 if pred.item() > 0.5 else 0 # as sigmoid activated\n",
        "            if(int(label) == pred_class): # counting correct prediction in each epoch\n",
        "                correct += 1\n",
        "\n",
        "        train_loss = float(cumulative_loss)/num_train_data\n",
        "        train_accuracy = correct*float(100)/num_train_data\n",
        "\n",
        "#         val_loss, val_accuracy, p, r, f = evaluate_validation_accuracy(val_data, bow) # test val-data-set on currently trained model\n",
        "        val_loss = evaluate_validation_accuracy(val_data, bow) # test val-data-set on currently trained model\n",
        "    \n",
        "        if(prev_val_loss-val_loss < 0.0): # early stop if nearly no change\n",
        "            bow = copy.deepcopy(early_stop_bow)\n",
        "            break\n",
        "\n",
        "        early_stop_bow = copy.deepcopy(bow)\n",
        "\n",
        "        prev_val_loss = val_loss\n",
        "\n",
        "        toc = time.time() # final time\n",
        "        total_time += (toc-tic)\n",
        "\n",
        "        print(\"Epoch {}/{}\\n[On Training] ==> Time: {:.2f}s, Train Loss: {:.9f}, Train Accuracy: {:.2f}%\".format(e+1, epochs, (toc-tic), train_loss, train_accuracy))\n",
        "#         print(\"[On Validation] ==> Precision: {:.3f}, Recall: {:.3f}, F-Score: {:.3f}, Val loss: {:.9f}, Val Accuracy: {:.2f}% Total Time: {:.2f}s\".format(p, r, f, val_loss, val_accuracy, total_time))\n",
        "        print(\"[On Validation] ==> Val loss: {:.9f}, Total Time: {:.2f}s\".format(val_loss, total_time))\n",
        "\n",
        "    torch.save(bow, task_name)\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "[On Training] ==> Time: 28.59s, Train Loss: 0.694795180, Train Accuracy: 50.03%\n",
            "[On Validation] ==> Val loss: 0.693983703, Total Time: 28.59s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 28.64s, Train Loss: 0.693449449, Train Accuracy: 50.03%\n",
            "[On Validation] ==> Val loss: 0.693071867, Total Time: 57.23s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 28.69s, Train Loss: 0.692733096, Train Accuracy: 50.04%\n",
            "[On Validation] ==> Val loss: 0.692487392, Total Time: 85.92s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 29.17s, Train Loss: 0.692184894, Train Accuracy: 50.86%\n",
            "[On Validation] ==> Val loss: 0.691935766, Total Time: 115.09s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 28.66s, Train Loss: 0.691598816, Train Accuracy: 55.56%\n",
            "[On Validation] ==> Val loss: 0.691273065, Total Time: 143.75s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 28.47s, Train Loss: 0.690864469, Train Accuracy: 61.96%\n",
            "[On Validation] ==> Val loss: 0.690422513, Total Time: 172.22s\n",
            "Epoch 7/100\n",
            "[On Training] ==> Time: 28.69s, Train Loss: 0.689910541, Train Accuracy: 66.06%\n",
            "[On Validation] ==> Val loss: 0.689299076, Total Time: 200.92s\n",
            "Epoch 8/100\n",
            "[On Training] ==> Time: 28.91s, Train Loss: 0.688613697, Train Accuracy: 68.60%\n",
            "[On Validation] ==> Val loss: 0.687729494, Total Time: 229.83s\n",
            "Epoch 9/100\n",
            "[On Training] ==> Time: 28.72s, Train Loss: 0.686731613, Train Accuracy: 69.98%\n",
            "[On Validation] ==> Val loss: 0.685366456, Total Time: 258.55s\n",
            "Epoch 10/100\n",
            "[On Training] ==> Time: 28.66s, Train Loss: 0.683724225, Train Accuracy: 70.95%\n",
            "[On Validation] ==> Val loss: 0.681499057, Total Time: 287.20s\n",
            "Epoch 11/100\n",
            "[On Training] ==> Time: 28.65s, Train Loss: 0.678682017, Train Accuracy: 71.78%\n",
            "[On Validation] ==> Val loss: 0.675167623, Total Time: 315.86s\n",
            "Epoch 12/100\n",
            "[On Training] ==> Time: 28.80s, Train Loss: 0.670855634, Train Accuracy: 72.94%\n",
            "[On Validation] ==> Val loss: 0.665693039, Total Time: 344.66s\n",
            "Epoch 13/100\n",
            "[On Training] ==> Time: 28.50s, Train Loss: 0.658622661, Train Accuracy: 74.33%\n",
            "[On Validation] ==> Val loss: 0.650510521, Total Time: 373.16s\n",
            "Epoch 14/100\n",
            "[On Training] ==> Time: 28.60s, Train Loss: 0.638795865, Train Accuracy: 75.81%\n",
            "[On Validation] ==> Val loss: 0.626063456, Total Time: 401.76s\n",
            "Epoch 15/100\n",
            "[On Training] ==> Time: 29.12s, Train Loss: 0.607585767, Train Accuracy: 77.96%\n",
            "[On Validation] ==> Val loss: 0.588987388, Total Time: 430.88s\n",
            "Epoch 16/100\n",
            "[On Training] ==> Time: 28.71s, Train Loss: 0.561843066, Train Accuracy: 80.26%\n",
            "[On Validation] ==> Val loss: 0.538207627, Total Time: 459.59s\n",
            "Epoch 17/100\n",
            "[On Training] ==> Time: 28.73s, Train Loss: 0.502884092, Train Accuracy: 82.50%\n",
            "[On Validation] ==> Val loss: 0.480350020, Total Time: 488.32s\n",
            "Epoch 18/100\n",
            "[On Training] ==> Time: 28.54s, Train Loss: 0.440758210, Train Accuracy: 84.36%\n",
            "[On Validation] ==> Val loss: 0.429489539, Total Time: 516.86s\n",
            "Epoch 19/100\n",
            "[On Training] ==> Time: 28.84s, Train Loss: 0.386760483, Train Accuracy: 85.70%\n",
            "[On Validation] ==> Val loss: 0.392739047, Total Time: 545.70s\n",
            "Epoch 20/100\n",
            "[On Training] ==> Time: 28.60s, Train Loss: 0.343197420, Train Accuracy: 87.29%\n",
            "[On Validation] ==> Val loss: 0.367379366, Total Time: 574.30s\n",
            "Epoch 21/100\n",
            "[On Training] ==> Time: 28.77s, Train Loss: 0.307625815, Train Accuracy: 88.65%\n",
            "[On Validation] ==> Val loss: 0.350496724, Total Time: 603.07s\n",
            "Epoch 22/100\n",
            "[On Training] ==> Time: 28.74s, Train Loss: 0.278133332, Train Accuracy: 89.88%\n",
            "[On Validation] ==> Val loss: 0.339668695, Total Time: 631.81s\n",
            "Epoch 23/100\n",
            "[On Training] ==> Time: 28.68s, Train Loss: 0.253645348, Train Accuracy: 90.83%\n",
            "[On Validation] ==> Val loss: 0.333740682, Total Time: 660.49s\n",
            "Epoch 24/100\n",
            "[On Training] ==> Time: 28.79s, Train Loss: 0.232794682, Train Accuracy: 91.68%\n",
            "[On Validation] ==> Val loss: 0.331449520, Total Time: 689.28s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type BOWClassifier. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Epoch 1/100\n",
            "[On Training] ==> Time: 30.04s, Train Loss: 0.691826512, Train Accuracy: 50.01%\n",
            "[On Validation] ==> Val loss: 0.688490567, Total Time: 30.04s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 30.00s, Train Loss: 0.684480117, Train Accuracy: 56.16%\n",
            "[On Validation] ==> Val loss: 0.678895941, Total Time: 60.04s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 29.83s, Train Loss: 0.671954157, Train Accuracy: 68.77%\n",
            "[On Validation] ==> Val loss: 0.662479243, Total Time: 89.87s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 30.04s, Train Loss: 0.650381625, Train Accuracy: 73.08%\n",
            "[On Validation] ==> Val loss: 0.634912236, Total Time: 119.91s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 29.90s, Train Loss: 0.615266485, Train Accuracy: 76.09%\n",
            "[On Validation] ==> Val loss: 0.592881717, Total Time: 149.81s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 30.03s, Train Loss: 0.565478897, Train Accuracy: 78.69%\n",
            "[On Validation] ==> Val loss: 0.539525427, Total Time: 179.84s\n",
            "Epoch 7/100\n",
            "[On Training] ==> Time: 29.99s, Train Loss: 0.508059802, Train Accuracy: 81.21%\n",
            "[On Validation] ==> Val loss: 0.486249860, Total Time: 209.83s\n",
            "Epoch 8/100\n",
            "[On Training] ==> Time: 29.87s, Train Loss: 0.454704858, Train Accuracy: 83.07%\n",
            "[On Validation] ==> Val loss: 0.443397041, Total Time: 239.71s\n",
            "Epoch 9/100\n",
            "[On Training] ==> Time: 29.90s, Train Loss: 0.410952383, Train Accuracy: 84.37%\n",
            "[On Validation] ==> Val loss: 0.411896852, Total Time: 269.61s\n",
            "Epoch 10/100\n",
            "[On Training] ==> Time: 30.11s, Train Loss: 0.375388464, Train Accuracy: 85.73%\n",
            "[On Validation] ==> Val loss: 0.388483530, Total Time: 299.72s\n",
            "Epoch 11/100\n",
            "[On Training] ==> Time: 30.17s, Train Loss: 0.345466558, Train Accuracy: 87.00%\n",
            "[On Validation] ==> Val loss: 0.370653603, Total Time: 329.90s\n",
            "Epoch 12/100\n",
            "[On Training] ==> Time: 29.92s, Train Loss: 0.319683715, Train Accuracy: 88.10%\n",
            "[On Validation] ==> Val loss: 0.357016816, Total Time: 359.82s\n",
            "Epoch 13/100\n",
            "[On Training] ==> Time: 29.91s, Train Loss: 0.297345083, Train Accuracy: 88.87%\n",
            "[On Validation] ==> Val loss: 0.346732155, Total Time: 389.73s\n",
            "Epoch 14/100\n",
            "[On Training] ==> Time: 30.09s, Train Loss: 0.277955552, Train Accuracy: 89.70%\n",
            "[On Validation] ==> Val loss: 0.339238057, Total Time: 419.82s\n",
            "Epoch 15/100\n",
            "[On Training] ==> Time: 29.97s, Train Loss: 0.261003343, Train Accuracy: 90.49%\n",
            "[On Validation] ==> Val loss: 0.333942103, Total Time: 449.79s\n",
            "Epoch 16/100\n",
            "[On Training] ==> Time: 29.93s, Train Loss: 0.246016482, Train Accuracy: 91.15%\n",
            "[On Validation] ==> Val loss: 0.330320135, Total Time: 479.72s\n",
            "Epoch 17/100\n",
            "[On Training] ==> Time: 29.93s, Train Loss: 0.232587528, Train Accuracy: 91.70%\n",
            "[On Validation] ==> Val loss: 0.328022862, Total Time: 509.65s\n",
            "Epoch 18/100\n",
            "[On Training] ==> Time: 30.06s, Train Loss: 0.220401927, Train Accuracy: 92.15%\n",
            "[On Validation] ==> Val loss: 0.326804660, Total Time: 539.71s\n",
            "Epoch 19/100\n",
            "[On Training] ==> Time: 29.96s, Train Loss: 0.209211109, Train Accuracy: 92.62%\n",
            "[On Validation] ==> Val loss: 0.326498871, Total Time: 569.67s\n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "[On Training] ==> Time: 30.00s, Train Loss: 0.695671513, Train Accuracy: 49.97%\n",
            "[On Validation] ==> Val loss: 0.691130726, Total Time: 30.00s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 30.03s, Train Loss: 0.686333792, Train Accuracy: 50.89%\n",
            "[On Validation] ==> Val loss: 0.679947853, Total Time: 60.03s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 29.73s, Train Loss: 0.671361264, Train Accuracy: 58.58%\n",
            "[On Validation] ==> Val loss: 0.661706297, Total Time: 89.75s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 29.80s, Train Loss: 0.649057044, Train Accuracy: 70.15%\n",
            "[On Validation] ==> Val loss: 0.634990839, Total Time: 119.56s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 30.12s, Train Loss: 0.616226659, Train Accuracy: 75.78%\n",
            "[On Validation] ==> Val loss: 0.596432979, Total Time: 149.68s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 29.88s, Train Loss: 0.570560461, Train Accuracy: 79.05%\n",
            "[On Validation] ==> Val loss: 0.546956700, Total Time: 179.56s\n",
            "Epoch 7/100\n",
            "[On Training] ==> Time: 29.88s, Train Loss: 0.516091528, Train Accuracy: 81.25%\n",
            "[On Validation] ==> Val loss: 0.495310783, Total Time: 209.43s\n",
            "Epoch 8/100\n",
            "[On Training] ==> Time: 29.86s, Train Loss: 0.462933767, Train Accuracy: 82.97%\n",
            "[On Validation] ==> Val loss: 0.451823411, Total Time: 239.30s\n",
            "Epoch 9/100\n",
            "[On Training] ==> Time: 30.03s, Train Loss: 0.418076772, Train Accuracy: 84.42%\n",
            "[On Validation] ==> Val loss: 0.419220642, Total Time: 269.33s\n",
            "Epoch 10/100\n",
            "[On Training] ==> Time: 29.89s, Train Loss: 0.381570939, Train Accuracy: 85.66%\n",
            "[On Validation] ==> Val loss: 0.394996102, Total Time: 299.22s\n",
            "Epoch 11/100\n",
            "[On Training] ==> Time: 29.79s, Train Loss: 0.351014670, Train Accuracy: 86.93%\n",
            "[On Validation] ==> Val loss: 0.376498144, Total Time: 329.01s\n",
            "Epoch 12/100\n",
            "[On Training] ==> Time: 30.08s, Train Loss: 0.324815647, Train Accuracy: 87.99%\n",
            "[On Validation] ==> Val loss: 0.362133927, Total Time: 359.08s\n",
            "Epoch 13/100\n",
            "[On Training] ==> Time: 30.02s, Train Loss: 0.302123027, Train Accuracy: 88.81%\n",
            "[On Validation] ==> Val loss: 0.351187800, Total Time: 389.11s\n",
            "Epoch 14/100\n",
            "[On Training] ==> Time: 29.81s, Train Loss: 0.282386523, Train Accuracy: 89.58%\n",
            "[On Validation] ==> Val loss: 0.343135807, Total Time: 418.91s\n",
            "Epoch 15/100\n",
            "[On Training] ==> Time: 29.96s, Train Loss: 0.265136492, Train Accuracy: 90.42%\n",
            "[On Validation] ==> Val loss: 0.337429821, Total Time: 448.87s\n",
            "Epoch 16/100\n",
            "[On Training] ==> Time: 29.86s, Train Loss: 0.249925258, Train Accuracy: 91.02%\n",
            "[On Validation] ==> Val loss: 0.333484938, Total Time: 478.74s\n",
            "Epoch 17/100\n",
            "[On Training] ==> Time: 30.02s, Train Loss: 0.236287602, Train Accuracy: 91.55%\n",
            "[On Validation] ==> Val loss: 0.330859303, Total Time: 508.76s\n",
            "Epoch 18/100\n",
            "[On Training] ==> Time: 29.89s, Train Loss: 0.223906584, Train Accuracy: 92.00%\n",
            "[On Validation] ==> Val loss: 0.329312767, Total Time: 538.65s\n",
            "Epoch 19/100\n",
            "[On Training] ==> Time: 29.84s, Train Loss: 0.212548837, Train Accuracy: 92.59%\n",
            "[On Validation] ==> Val loss: 0.328702841, Total Time: 568.49s\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qD-kkwEKW0iM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download('Task4A.mdl')\n",
        "files.download('Task4B.mdl')\n",
        "files.download('Task4C.mdl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "12KQYM1RYuiH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Task-5"
      ]
    },
    {
      "metadata": {
        "id": "cd6rex2FYwmp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "task5 = [('relu', 'Task5A.mdl'), ('tanh', 'Task5B.mdl'), ('sigmoid', 'Task5C.mdl')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-lCj7dx6ZB-Y",
        "colab_type": "code",
        "outputId": "f0f12353-378e-436f-d5de-541c447cbb22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        }
      },
      "cell_type": "code",
      "source": [
        "for activation, task_name in task5:\n",
        "    if(activation == 'relu'):\n",
        "        class BOWClassifier(nn.Module):\n",
        "            def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
        "                super(BOWClassifier, self).__init__()\n",
        "                SEED = 42\n",
        "                torch.manual_seed(SEED)\n",
        "                torch.cuda.manual_seed(SEED)\n",
        "                self.i2h1 = nn.Linear(input_size, hidden_size1) # initialises weights and biases i2h1\n",
        "                self.drop1 = nn.Dropout(0.5) # Adding dropout layer\n",
        "                \n",
        "                self.h12h2 = nn.Linear(hidden_size1, hidden_size2) # initialises weights and biases h12h2\n",
        "                self.drop2 = nn.Dropout(0.5) # Adding dropout layer\n",
        "                \n",
        "                self.h22h3 = nn.Linear(hidden_size2, hidden_size3) # initialises weights and biases h22h3\n",
        "                self.drop3 = nn.Dropout(0.5) # Adding dropout layer\n",
        "                \n",
        "                self.h32o = nn.Linear(hidden_size3, output_size) # initialises weights and biases h32o\n",
        "\n",
        "            def forward(self, x):\n",
        "                x = torch.relu(self.i2h1(x)) # relu activation @ hidden layer\n",
        "                x = torch.relu(self.h12h2(x)) # relu activation @ hidden layer\n",
        "                x = torch.relu(self.h22h3(x)) # relu activation @ hidden layer\n",
        "                x = torch.sigmoid(self.h32o(x)) # sigmoid activation @ output layer\n",
        "                return x\n",
        "\n",
        "        num_of_input = VOCAB_SIZE # Bag word restriction has to be equal to vocabulary\n",
        "        num_of_hidden1 = 30\n",
        "        num_of_hidden2 = 20\n",
        "        num_of_hidden3 = 10\n",
        "        num_of_output = 1 # binary sentiment classes (+ve, -ve)\n",
        "\n",
        "        bow = BOWClassifier(num_of_input, num_of_hidden1, num_of_hidden2, num_of_hidden3, num_of_output).to('cuda:0') # initialises weights and biases\n",
        "\n",
        "    # define a loss function and an optimizer\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    opt = torch.optim.SGD(bow.parameters(), lr = 0.0001)\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    # Train The Model\n",
        "    epochs = 100\n",
        "    # the training loop\n",
        "    total_time = 0.0\n",
        "    prev_val_loss = float('inf')\n",
        "    val_loss = 0\n",
        "    early_stop_bow = 0\n",
        "\n",
        "    for e in range(epochs):\n",
        "        tic = time.time() # start the timer\n",
        "        correct = 0\n",
        "        cumulative_loss = 0\n",
        "        incorrect = 0\n",
        "\n",
        "        for i, instance in enumerate(train_data): # train_text_reviews \n",
        "            # get the training data\n",
        "            label = train_text_labels[i] # get the label of the corresponding instace\n",
        "            label = Variable(torch.FloatTensor([label])).resize_((1,1)).to('cuda:0') # wraps a tensor for label\n",
        "\n",
        "            bow.zero_grad() # reset the gradient for each instance\n",
        "\n",
        "            pred = bow.forward(instance)\n",
        "\n",
        "            loss = loss_function(pred, label) # compute the loss\n",
        "            loss.backward() # backprop the loss\n",
        "            opt.step() # performs parameter updation based on the current gradient\n",
        "\n",
        "            cumulative_loss += float(loss.item()) # accumulate the loss over whole training sample\n",
        "\n",
        "            pred_class = 1 if pred.item() > 0.5 else 0 # as sigmoid activated\n",
        "            if(int(label) == pred_class): # counting correct prediction in each epoch\n",
        "                correct += 1\n",
        "\n",
        "        train_loss = float(cumulative_loss)/num_train_data\n",
        "        train_accuracy = correct*float(100)/num_train_data\n",
        "\n",
        "#         val_loss, val_accuracy, p, r, f = evaluate_validation_accuracy(val_data, bow) # test val-data-set on currently trained model\n",
        "        val_loss = evaluate_validation_accuracy(val_data, bow) # test val-data-set on currently trained model\n",
        "    \n",
        "        if(prev_val_loss-val_loss < 0.0): # early stop if nearly no change\n",
        "            bow = copy.deepcopy(early_stop_bow)\n",
        "            break\n",
        "\n",
        "        early_stop_bow = copy.deepcopy(bow)\n",
        "\n",
        "        prev_val_loss = val_loss\n",
        "\n",
        "        toc = time.time() # final time\n",
        "        total_time += (toc-tic)\n",
        "\n",
        "        print(\"Epoch {}/{}\\n[On Training] ==> Time: {:.2f}s, Train Loss: {:.9f}, Train Accuracy: {:.2f}%\".format(e+1, epochs, (toc-tic), train_loss, train_accuracy))\n",
        "#         print(\"[On Validation] ==> Precision: {:.3f}, Recall: {:.3f}, F-Score: {:.3f}, Val loss: {:.9f}, Val Accuracy: {:.2f}% Total Time: {:.2f}s\".format(p, r, f, val_loss, val_accuracy, total_time))\n",
        "        print(\"[On Validation] ==> Val loss: {:.9f}, Total Time: {:.2f}s\".format(val_loss, total_time))\n",
        "\n",
        "    torch.save(bow, task_name)\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-e1cdf026802d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mnum_of_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# binary sentiment classes (+ve, -ve)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBOWClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_of_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_of_hidden1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_of_hidden2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_of_hidden3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_of_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# initialises weights and biases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# define a loss function and an optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;31m# Tensors stored in modules are graph leaves, and we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                 \u001b[0;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "FDUyFfwmXSew",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# plt.figure(\"Image\")\n",
        "# plt.title(\"Loss vs Epoch\")\n",
        "# val_loss_history_plt =  [float(i)/sum(val_loss_history) for i in val_loss_history] # normalised between 0-1\n",
        "# train_loss_history_plt =  [float(i)/sum(train_loss_history) for i in train_loss_history] # normalised between 0-1\n",
        "# f_score_plt = [float(i)/sum(f_score) for i in f_score] # normalised between 0-1\n",
        "# plt.plot(val_loss_history_plt, c=\"red\", label=\"Validation Loss\")\n",
        "# plt.plot(train_loss_history_plt, c=\"blue\", label = \"Training Loss\")\n",
        "# plt.plot(f_score_plt, c=\"green\", label = \"F-Score\")\n",
        "# plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zz5PwYjVudEk",
        "colab_type": "code",
        "outputId": "b827a586-3ef1-4a92-8a95-3bbf68dabc54",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "temp_test = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7063ed6b-ee9a-4382-bd21-300e434055d5\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-7063ed6b-ee9a-4382-bd21-300e434055d5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Task3A.mdl to Task3A.mdl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "29twCfxpuc8r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# _model_ = torch.load(io.BytesIO(temp_test['Task2C.mdl']))\n",
        "_model_ = torch.load('Task4C.mdl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ixy30WkrH3_-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# for params in _model_.parameters():\n",
        "#     print (params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fkYBaktsF-sm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_test = []\n",
        "data_test = [Variable(make_bow_vector(instance, word_to_ix)).to('cuda:0') for instance in test_text_reviews]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ffYv5PAmjyp-",
        "colab_type": "code",
        "outputId": "d143846e-934c-491b-c992-eecc048dde4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print('--- AFTER TRAINING ---')\n",
        "correct = 0\n",
        "tic = time.time()\n",
        "for i, instance in enumerate(data_test):\n",
        "    label = test_text_labels[i] # get the label of the corresponding instace\n",
        "    label = Variable(torch.FloatTensor([label])).resize_((1,1)).to('cuda:0') # wraps a tensor for label\n",
        "    \n",
        "    pred = _model_.forward(instance)\n",
        "    pred_class = 1 if pred.item() > 0.5 else 0 # sigmoid activated\n",
        "    \n",
        "    if(int(label) == pred_class):\n",
        "        correct += 1\n",
        "toc = time.time()\n",
        "print(\"Time: {}, Test Accuracy: {:.2f}%\".format((toc-tic), correct*100/len(data_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- AFTER TRAINING ---\n",
            "Time: 2.4268977642059326, Test Accuracy: 87.50%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hBL6jT8ReP6p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# predict on test dataset\n",
        "# with open(\"Task1A.csv\", \"wb\") as _f:\n",
        "#     writer = csv.writer(_f)\n",
        "#     writer.writerows(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0VkVoMzMUdeh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.save('vocab.npy', word_to_ix)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"vocab.npy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ub0HKgL8H37z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lCraW41hH32S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mhftDt1DH3x6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kE_TcX1KH3tQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T60wqxmoH3Zl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}