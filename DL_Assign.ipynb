{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Assign.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajeshsahu09/CS69002_9A_18CS60R19/blob/master/DL_Assign.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "wuB0HM37teAc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Movie Review Sentiment Analysis"
      ]
    },
    {
      "metadata": {
        "id": "rKlXTA5zfx20",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Run on **GPU**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "_NV6CE-qtvGO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Import Header Files"
      ]
    },
    {
      "metadata": {
        "id": "WE7OEcPOtzJf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v4037ccUuFFp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load the Dataset and Visualise"
      ]
    },
    {
      "metadata": {
        "id": "EhWFepP2t3eA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# import io\n",
        "# df_train = pd.read_csv(io.StringIO(uploaded['Train_20K.csv'].decode('utf-8')), sep='\\t')\n",
        "# df_train.head()\n",
        "url = \"https://raw.githubusercontent.com/binny-mathew/IITKGP_CS69002_Spring_2019/master/Dataset/Train_20K.csv\"\n",
        "df = pd.read_csv(url, sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mvAXyCoDaR3b",
        "colab_type": "code",
        "outputId": "656cd434-5fba-4798-f075-c5ea30755822",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17999, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "44YtF48vX24a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_1 = df[df['label']==1] # get only label == 1\n",
        "df_0 = df[df['label']==0] # get only label == 0\n",
        "\n",
        "range_1 = int(0.9*len(df_1))\n",
        "range_2 = int(0.9*len(df_0))\n",
        "\n",
        "np.random.seed(42) # fix the seed value\n",
        "df_1 = df_1.iloc[np.random.permutation(len(df_1))] # shuffled the data set of label==1\n",
        "df_0 = df_0.iloc[np.random.permutation(len(df_0))] # shuffled the data set of label==0\n",
        "\n",
        "temp_1_train = df_1.iloc[:range_1]\n",
        "temp_2_train = df_0.iloc[:range_2]\n",
        "df_train = pd.concat([temp_1_train, temp_2_train])\n",
        "\n",
        "temp_1_val = df_1.iloc[range_1:]\n",
        "temp_2_val = df_0.iloc[range_2:]\n",
        "df_val = pd.concat([temp_1_val, temp_2_val])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T9FBTqABuRlO",
        "colab_type": "code",
        "outputId": "af7dac4b-6f3d-43cb-f6d6-02ed1f1838f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "df_train = df_train.iloc[np.random.permutation(len(df_train))] # shuffled the data set of label==1\n",
        "df_val = df_val.iloc[np.random.permutation(len(df_val))] # shuffled the data set of label==1\n",
        "len(df_train), len(df_val)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16198, 1801)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "0WFB1TtsxD3F",
        "colab_type": "code",
        "outputId": "98d05c04-5955-4eaa-af90-a3da2d93fb5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "print('Number of Negative movie reviews', len(df_train[df_train['label']==0]))\n",
        "print('Number of Positive movie reviews', len(df_train[df_train['label']==1]))\n",
        "print('Number of movie reviews', len(df_train['label']))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Negative movie reviews 8094\n",
            "Number of Positive movie reviews 8104\n",
            "Number of movie reviews 16198\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sYtzJk8r3lm5",
        "colab_type": "code",
        "outputId": "c13a3b32-f76e-45d7-cff9-672c21e62e8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "print('Number of Negative movie reviews', len(df_val[df_val['label']==0]))\n",
        "print('Number of Positive movie reviews', len(df_val[df_val['label']==1]))\n",
        "print('Number of movie reviews', len(df_val['label']))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Negative movie reviews 900\n",
            "Number of Positive movie reviews 901\n",
            "Number of movie reviews 1801\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uuZxOMpkQoka",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/binny-mathew/IITKGP_CS69002_Spring_2019/master/Dataset/Test_5K.csv\"\n",
        "df_test = pd.read_csv(url, sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TVzhPcGGxf2-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data pre-processing"
      ]
    },
    {
      "metadata": {
        "id": "u-8IR64TKH0F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Get Train Data"
      ]
    },
    {
      "metadata": {
        "id": "JprKnOXgxXkE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_raw_text_reviews = df_train['text'].astype(str).tolist()\n",
        "train_text_labels = df_train['label'].astype(int).tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TwU4zROpKLdQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Get Validation Data"
      ]
    },
    {
      "metadata": {
        "id": "7TsL1ACWJ_3V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "val_raw_text_reviews = df_val['text'].astype(str).tolist()\n",
        "val_text_labels = df_val['label'].astype(int).tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YixDzEnXLZN-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Get Test Data"
      ]
    },
    {
      "metadata": {
        "id": "MQHaSpnJJ9eT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_raw_text_reviews = df_test['text'].astype(str).tolist()\n",
        "test_text_labels = df_test['label'].astype(int).tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ui-q6WRqOrB4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Cleaning the raw input data"
      ]
    },
    {
      "metadata": {
        "id": "8_4b4FmShb0u",
        "colab_type": "code",
        "outputId": "55a2ba0d-7b24-4acb-de0a-0a1f825f870d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Finding stop words\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "s3AxEhD_z1DM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess_document(doc):\n",
        "    # negative sense should not be eleminated + some short representation\n",
        "    CONTRACTIONS = {\"mayn't\":\"may not\", \"can't\":\"can not\", \"won't\":\"will not\", \"isn't\":\"is not\", \"amn't\":\"am not\",\\\n",
        "                  \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\", \"couldn't\":\"could not\", \\\n",
        "                  \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\\\n",
        "                  \"i'll\":\"i will\", \"you'll\":\"you will\", \"they'll\":\"they will\",\\\n",
        "                  \"may've\":\"may have\", \"can've\":\"can have\", \"will've\":\"will have\", \"you've\":\"you have\", \\\n",
        "                  \"could've\":\"could have\", \"would've\":\"would have\", \"you've\":\"you have\", \"they\":\"they have\",\\\n",
        "                  \"i've\":\"i have\", \"you've\":\"you have\", \"we've\":\"we have\", \"there's\":\"there is\", \"i'm\":\"i am\",\\\n",
        "                  \"it's\":\"it is\", \"what's\":\"what is\", \"where's\":\"where is\", \"how's\":\"how is\", \"i'd\":\"i had\"}\n",
        "    punctuation = string.punctuation + \"\\n\\n\"\n",
        "    punc_replace = ''.join([' ' for s in punctuation]) # required for replacing punctuation with null ('')\n",
        "    doc_clean = doc.replace('-', ' ') # replace - with null str\n",
        "    doc_clean = (doc_clean.encode('ascii', 'ignore')).decode(\"utf-8\")\n",
        "    doc_clean = doc_clean.replace('<br />', '') # replace <br /> with ''\n",
        "    doc_clean = doc_clean.replace(\"’\", \"'\") # replace <br /> with null str\n",
        "    doc_clean = [CONTRACTIONS[word] if word in CONTRACTIONS else word for word in doc_clean.split(' ')] # replacing some common short forms\n",
        "    doc_clean = \" \".join(doc_clean) # list to sentence of strings\n",
        "    doc_clean = re.sub(r'\\W +', ' ', doc_clean) # except [a-zA-Z0-9_]\n",
        "    doc_clean = re.sub(r'\\d+', ' ', doc_clean) # remove numbers [0-9]\n",
        "    trans_table = str.maketrans(punctuation, punc_replace); # replace punctuations with ' '\n",
        "    doc_clean = ' '.join([word.translate(trans_table) for word in doc_clean.split(' ')])\n",
        "    doc_clean = doc_clean.split(' ');\n",
        "    doc_clean = [word for word in doc_clean if len(word) > 0];\n",
        "    # removing the stopwords from a sentence\n",
        "    doc_clean = [word for word in doc_clean if not word.lower() in stop_words or word.lower() == 'not' or word.lower() == 'no']\n",
        "    return doc_clean;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rSNuXUopzavX",
        "colab_type": "code",
        "outputId": "3bb9e91a-fe7d-4d35-b6dc-eb9a6a079620",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "train_text_reviews = [preprocess_document(review.lower()) for review in train_raw_text_reviews]\n",
        "print (train_text_reviews[len(train_text_reviews)-2])\n",
        "print (train_text_labels[len(train_text_labels)-2])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['three', 'tales', 'told', 'film', 'seemed', 'shot', 'without', 'knowledge', 'combined', 'vignette', 'film', 'makers', 'relate', 'three', 'vignettes', 'connected', 'shrink', 'martin', 'kove', 'although', 'never', 'see', 'leads', 'kove', 'first', 'vignette', 'sexy', 'vivian', 'schilling', 'woman', 'afraid', 'everything', 'sun', 'makes', 'adrian', 'monk', 'look', 'brave', 'paranoia', 'laced', 'evening', 'home', 'alone', 'literally', 'scream', 'vivian', 'ridiculous', 'things', 'spends', 'majority', 'time', 'nighty', 'shows', 'amazing', 'features', 'film', 'worst', 'not', 'nail', 'biting', 'second', 'vignette', 'owned', 'bill', 'paxton', 'portrays', 'roommate', 'hell', 'geeky', 'roommate', 'allows', 'take', 'complete', 'advantage', 'bill', 'whenever', 'last', 'vignette', 'funny', 'man', 'fears', 'death', 'take', 'moment', 'much', 'like', 'pal', 'choked', 'death', 'olive', 'not', 'interesting', 'movie', 'whole', 'seems', 'chopped', 'together', 'little', 'thought', 'involved', 'must', 'bill', 'paxton', 'fans']\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "W6psZ2jxNOoP",
        "colab_type": "code",
        "outputId": "2e2aebb5-2473-4edf-9f3d-00c05ad92fb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "val_text_reviews = [preprocess_document(review.lower()) for review in val_raw_text_reviews]\n",
        "print (val_text_reviews[len(val_text_reviews)-2])\n",
        "print (val_text_labels[len(val_text_labels)-2])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['even', 'one', 'realize', 'sellers', 'poor', 'health', 'time', 'filming', 'passed', 'away', 'film', 'release', 'fiendish', 'plot', 'dr', 'fu', 'manchu', 'would', 'painful', 'viewing', 'supposedly', 'lampoon', 'sax', 'rohmer', 'famous', 'oriental', 'villain', 'lacks', 'focus', 'potential', 'satirical', 'commentary', 'anti', 'oriental', 'overtones', 'rohmer', 'concept', 'ignored', 'indeed', 'movie', 'employs', 'racist', 'insults', 'hardly', 'actual', 'jokes', 'gags', 'mostly', 'actors', 'behaving', 'idiotically', 'spouting', 'dreary', 'lines', 'especially', 'distressing', 'see', 'sid', 'caesar', 'forced', 'spout', 'curses', 'racial', 'slurs', 'attempted', 'laughs', 'actors', 'embarrass', 'well', 'peter', 'sellers', 'plays', 'dual', 'roles', 'sinister', 'fu', 'manchu', 'trying', 'concoct', 'formula', 'regain', 'youth', 'stalwart', 'british', 'foe', 'nayland', 'smith', 'sellers', 'not', 'one', 'hundred', 'per', 'cent', 'bad', 'conveys', 'quirky', 'warmth', 'smith', 'discusses', 'fetishistic', 'attachment', 'lawn', 'mower', 'oddly', 'moving', 'manchu', 'expresses', 'love', 'english', 'music', 'hall', 'entertainment', 'time', 'plays', 'roles', 'weary', 'grimness', 'thus', 'sabotaging', 'comical', 'possibilities', 'sellers', 'routines', 'revitalizes', 'fading', 'strength', 'electric', 'shocks', 'particularly', 'excruciating', 'seems', 'convincingly', 'agonized', 'funny', 'genuinely', 'witty', 'lines', 'apt', 'slapstick', 'bit', 'burt', 'kwouk', 'cato', 'pink', 'panther', 'films', 'one', 'manchu', 'minions', 'helen', 'mirren', 'amusing', 'musical', 'numbers', 'cannot', 'salvage', 'mess', 'anyone', 'wants', 'understand', 'peter', 'sellers', 'considered', 'comedic', 'genius', 'not', 'learn', 'anything', 'fiendish', 'plot', 'dr', 'fu', 'manchu']\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "REdaGj3VNOEX",
        "colab_type": "code",
        "outputId": "cb14248f-9798-43d4-e3ea-d84f09575d53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "test_text_reviews = [preprocess_document(review.lower()) for review in test_raw_text_reviews]\n",
        "print (test_text_reviews[len(test_text_reviews)-2])\n",
        "print (test_text_labels[len(test_text_labels)-2])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['shining', 'know', 'weird', 'movie', 'movie', 'everyone', 'people', 'claim', 'not', 'like', 'horror', 'films', 'always', 'say', 'shining', 'terrific', 'film', 'stanley', 'kubrick', 'classic', 'vision', 'stephen', 'king', 'horror', 'tale', 'madness', 'blood', 'incredible', 'film', 'wither', 'seen', 'not', 'heard', 'know', 'lines', 'know', 'classic', 'images', 'could', 'forget', 'jack', 'johnny', 'could', 'forget', 'work', 'no', 'play', 'make', 'jack', 'dull', 'boy', 'could', 'forget', 'chilling', 'ending', 'film', 'unforgettable', 'honestly', 'opinion', 'kubrick', 'best', 'work', 'know', 'lot', 'argument', 'department', 'lot', 'people', 'say', 'space', 'odyssey', 'clockwork', 'orange', 'even', 'dr', 'strangelove', 'film', 'pioneered', 'film', 'making', 'shining', 'perfected', 'tale', 'isolation', 'madness', 'terrifying', 'images', 'ultimate', 'ghost', 'story', 'crawl', 'underneath', 'skin', 'jack', 'torrance', 'jack', 'son', 'danny', 'jack', 'wife', 'wendy', 'arrive', 'overlook', 'hotel', 'closing', 'day', 'elderly', 'african', 'american', 'chef', 'dick', 'hallorann', 'surprises', 'danny', 'speaking', 'telepathically', 'offering', 'ice', 'cream', 'explains', 'danny', 'grandmother', 'shared', 'gift', 'called', 'communication', 'shining', 'danny', 'asks', 'anything', 'afraid', 'hotel', 'particularly', 'room', 'dick', 'tells', 'danny', 'hotel', 'certain', 'shine', 'many', 'memories', 'not', 'good', 'advises', 'stay', 'room', 'circumstances', 'danny', 'curiosity', 'room', 'finally', 'gets', 'better', 'sees', 'room', 'opened', 'danny', 'shows', 'injured', 'visibly', 'traumatized', 'jack', 'tells', 'wendy', 'loves', 'family', 'seeing', 'wendy', 'thinks', 'jack', 'abusing', 'danny', 'jack', 'wanders', 'hotel', 'gold', 'room', 'meets', 'ghostly', 'bartender', 'named', 'lloyd', 'danny', 'starts', 'calling', 'word', 'redrum', 'frantically', 'scribbling', 'walls', 'goes', 'trance', 'withdraws', 'says', 'tony', 'imaginary', 'friend', 'jack', 'sabotages', 'hotel', 'radio', 'cutting', 'communication', 'outside', 'world', 'hallorann', 'received', 'danny', 'telepathic', 'cry', 'help', 'way', 'wendy', 'discovers', 'jack', 'typing', 'endless', 'pages', 'manuscript', 'repeating', 'work', 'no', 'play', 'makes', 'jack', 'dull', 'boy', 'formatted', 'various', 'ways', 'horrified', 'jack', 'threatens', 'knocks', 'unconscious', 'baseball', 'bat', 'locking', 'storage', 'locker', 'kitchen', 'jack', 'converses', 'grady', 'door', 'locker', 'unlocks', 'releasing', 'danny', 'written', 'redrum', 'lipstick', 'door', 'wendy', 'bedroom', 'looks', 'mirror', 'sees', 'murder', 'spelled', 'backwards', 'jack', 'picks', 'axe', 'begins', 'chop', 'door', 'leading', 'family', 'living', 'quarters', 'johnny', 'jack', 'legendary', 'image', 'born', 'shining', 'one', 'films', 'seriously', 'make', 'time', 'see', 'incredible', 'film', 'still', 'gives', 'nightmares', 'jack', 'nicholson', 'performance', 'timeless', 'unforgettable', 'one', 'also', 'feel', 'extremely', 'overlooked', 'shelley', 'duvall', 'scene', 'finding', 'jack', 'rant', 'work', 'incredible', 'look', 'horror', 'see', 'fear', 'face', 'realizing', 'husband', 'mad', 'also', 'another', 'incredible', 'scene', 'jack', 'sees', 'ghost', 'woman', 'bathtub', 'honestly', 'one', 'terrifying', 'scenes', 'horror', 'cinema', 'reason', 'film', 'well', 'known', 'film', 'perfection', 'simpsons', 'shown', 'films', 'film', 'forever', 'stay', 'see', 'trust']\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ydrqMX261kQ-",
        "colab_type": "code",
        "outputId": "6fa041fa-cac9-4466-fe9a-7e52364e1ee9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "xWfJK2HvwwQn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_vocabulary(sentences):\n",
        "    # Build vocabulary\n",
        "    dictWordCount = {}\n",
        "    for sent in sentences:\n",
        "        for word in sent:\n",
        "            dictWordCount[word] = 0 # initialising the dict value to zero\n",
        "    for sent in sentences:\n",
        "        for word in sent:\n",
        "            dictWordCount[word] += 1 # updating the dictionary count\n",
        "    \n",
        "    temp = dictWordCount.copy()\n",
        "    for key, val in temp.items():\n",
        "        if(dictWordCount[key] <= 3):\n",
        "            del dictWordCount[key]\n",
        "    \n",
        "    # Mapping from index to word\n",
        "    vocabulary_inv = sorted(dictWordCount, key=dictWordCount.__getitem__, reverse=True)\n",
        "    \n",
        "    # Mapping from word to index\n",
        "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
        "    return vocabulary, vocabulary_inv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MKifaS85Dkap",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Creating Tokens and Types"
      ]
    },
    {
      "metadata": {
        "id": "9cpQlJ3TytgZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_to_ix, ix_to_word = build_vocabulary(train_text_reviews+val_text_reviews+test_text_reviews)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PLBgxuf0xDB9",
        "colab_type": "code",
        "outputId": "087801de-71d8-486f-8b19-ed0e53e78689",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = len(word_to_ix)\n",
        "ix_to_word[word_to_ix['kick']]=='kick', word_to_ix['kick'], VOCAB_SIZE"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, 1829, 30943)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "GMbWbZdr1reC",
        "colab_type": "code",
        "outputId": "1475c620-96c0-439f-aa90-f2686c80cb32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "403"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "yNUlKuJeNJEu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## PyTorch"
      ]
    },
    {
      "metadata": {
        "id": "SqLOymKPLPug",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable # Automatic gradients are calculated and back-propagated through the computational graph\n",
        "import copy\n",
        "import csv\n",
        "import time\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sWqAwOAI7D54",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generate the BOW Vectors"
      ]
    },
    {
      "metadata": {
        "id": "UZK9TAQI7Fbm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_bow_vector(sentence, word_to_ix):\n",
        "    # create a vector of zeros of vocab size = len(word_to_idx)\n",
        "    vec = torch.zeros(len(word_to_ix)).to('cuda:0') #, device=device) # make 1D vector of len = vocab size\n",
        "    for word in sentence:\n",
        "        if word not in word_to_ix:            \n",
        "#             raise ValueError('Word',word,' not present in the dictionary. Sorry!')\n",
        "            pass\n",
        "        else:\n",
        "            vec[word_to_ix[word]]+=1 # count the number of occurance of same word in a sentences\n",
        "            \n",
        "    return vec.view(1, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6p3T6oo_T99z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Store in input sentences torch vector"
      ]
    },
    {
      "metadata": {
        "id": "W7xoiOiAyZN5",
        "colab_type": "code",
        "outputId": "3864550d-49fc-4854-bf98-3c5acf477ffe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# store the bag of word vectors for each sentences and wraping to tensor of torch type\n",
        "tic = time.time()\n",
        "train_data = [Variable(make_bow_vector(instance, word_to_ix)).to('cuda:0') for instance in train_text_reviews]\n",
        "num_train_data = len(train_text_reviews)\n",
        "\n",
        "val_data = [Variable(make_bow_vector(instance, word_to_ix)).to('cuda:0') for instance in val_text_reviews]\n",
        "num_val_data = len(val_text_reviews)\n",
        "toc = time.time()\n",
        "num_train_data, num_val_data, (toc-tic)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16198, 1801, 107.45636749267578)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "deO_hz8vT19u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Validation Accuracy Computation"
      ]
    },
    {
      "metadata": {
        "id": "cStQIVFfESWM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate_validation_accuracy(data, net):\n",
        "    sum_loss = 0\n",
        "    \n",
        "    true_positive = 0\n",
        "    true_negative = 0\n",
        "    false_positive = 0\n",
        "    false_negative = 0\n",
        "    \n",
        "    for i, instance in enumerate(data):\n",
        "        label = val_text_labels[i] # get the label of the corresponding instace\n",
        "        label = Variable(torch.FloatTensor([label])).resize_((1,1)).to('cuda:0') # wraps a tensor for label\n",
        "        \n",
        "#         vec = Variable(make_bow_vector(instance, word_to_ix)).to('cuda:0') # wrap to tensor of torch type for instance\n",
        "        \n",
        "#         prob = net.forward(vec) # forward pass\n",
        "\n",
        "        prob = net.forward(instance)\n",
        "    \n",
        "        _class = 1 if prob.item() > 0.5 else 0 # sigmoid activated\n",
        "        \n",
        "        loss = loss_function(prob, label) # compute the loss\n",
        "        \n",
        "        sum_loss += float(loss.item())\n",
        "#         print (int(label), _class)        \n",
        "        if(int(label) == _class and _class == 1):\n",
        "            true_positive += 1\n",
        "            \n",
        "        if(int(label) == _class and _class == 0):\n",
        "            true_negative += 1\n",
        "        \n",
        "        if(_class == 1 and int(label) == 0):\n",
        "            false_positive += 1\n",
        "            \n",
        "        if(_class == 0 and int(label) == 1):\n",
        "            false_negative += 1\n",
        "    \n",
        "    \n",
        "    precision = float(true_positive) / (true_positive + false_positive)\n",
        "    recall = float(true_positive) / (true_positive + false_negative)\n",
        "    f_score = float(2)*true_positive / (2*true_positive + false_positive + false_negative)\n",
        "    \n",
        "    return float(sum_loss)/len(data), float(100)*(true_positive+true_negative)/(true_positive+true_negative+false_positive+false_negative), precision, recall, f_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eKo4yKlVxOYd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SaSwXEBh6565",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Task-1"
      ]
    },
    {
      "metadata": {
        "id": "aCYGwbP7zy8i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "task1 = [(50, 'Task1A.mdl'), (100, 'Task1B.mdl'), (200, 'Task1C'), (500, 'Task1D.mdl')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PM5I4GP6lFGu",
        "colab_type": "code",
        "outputId": "87e7fe36-06ef-432b-d183-bd6ea23a45cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5045
        }
      },
      "cell_type": "code",
      "source": [
        "for num_of_hidden, task_name in task1:\n",
        "    class BOWClassifier(nn.Module):\n",
        "        def __init__(self, input_size, hidden_size, output_size):\n",
        "            super(BOWClassifier, self).__init__()\n",
        "            SEED = 42\n",
        "            torch.manual_seed(SEED)\n",
        "            torch.cuda.manual_seed(SEED)\n",
        "            self.hidden_size = hidden_size\n",
        "            self.i2h = nn.Linear(input_size, hidden_size) # initialises weights and biases i2h\n",
        "            self.h2o = nn.Linear(hidden_size, output_size) # initialises weights and biases h2o\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = torch.relu(self.i2h(x)) # relu activation @ hidden layer\n",
        "            x = torch.sigmoid(self.h2o(x)) # sigmoid activation @ output layer\n",
        "            return x\n",
        "\n",
        "    num_of_input = VOCAB_SIZE # Bag word restriction has to be equal to vocabulary\n",
        "    num_of_output = 1 # binary sentiment classes (+ve, -ve)\n",
        "\n",
        "    bow = BOWClassifier(num_of_input, num_of_hidden, num_of_output).to('cuda:0') # initialises weights and biases\n",
        "    \n",
        "    # define a loss function and an optimizer\n",
        "    loss_function = nn.BCELoss()\n",
        "    opt = torch.optim.SGD(bow.parameters(), lr = 0.0001)\n",
        "    \n",
        "    gc.collect()\n",
        "\n",
        "    # Train The Model\n",
        "    epochs = 100\n",
        "    # the training loop\n",
        "    total_time = 0.0\n",
        "    prev_val_loss = 0\n",
        "    no_change = 0\n",
        "\n",
        "    for e in range(epochs):\n",
        "        tic = time.time() # start the timer\n",
        "        correct = 0\n",
        "        cumulative_loss = 0\n",
        "        incorrect = 0\n",
        "\n",
        "        for i, instance in enumerate(train_data): # train_text_reviews \n",
        "            # get the training data\n",
        "            label = train_text_labels[i] # get the label of the corresponding instace\n",
        "            label = Variable(torch.FloatTensor([label])).resize_((1,1)).to('cuda:0') # wraps a tensor for label\n",
        "\n",
        "            bow.zero_grad() # reset the gradient for each instance\n",
        "\n",
        "            pred = bow.forward(instance)\n",
        "\n",
        "            loss = loss_function(pred, label) # compute the loss\n",
        "            loss.backward() # backprop the loss\n",
        "            opt.step() # performs parameter updation based on the current gradient\n",
        "\n",
        "            cumulative_loss += float(loss.item()) # accumulate the loss over whole training sample\n",
        "\n",
        "            pred_class = 1 if pred.item() > 0.5 else 0 # as sigmoid activated\n",
        "            if(int(label) == pred_class): # counting correct prediction in each epoch\n",
        "                correct += 1\n",
        "\n",
        "        train_loss = float(cumulative_loss)/num_train_data\n",
        "        train_accuracy = correct*float(100)/num_train_data\n",
        "\n",
        "        val_loss, val_accuracy, p, r, f = evaluate_validation_accuracy(val_data, bow) # test val-data-set on currently trained model\n",
        "\n",
        "        if(abs(prev_val_loss-val_loss) < 0.001): # less delta validation loss\n",
        "            no_change = 1\n",
        "\n",
        "        if(no_change == 1 and abs(prev_val_loss-val_loss) > 0.001): # early stop if nearly no change\n",
        "            break\n",
        "\n",
        "\n",
        "        prev_val_loss = val_loss\n",
        "\n",
        "        toc = time.time() # final time\n",
        "        total_time += (toc-tic)\n",
        "\n",
        "        print(\"Epoch {}/{}\\n[On Training] ==> Time: {:.2f}s, Train Loss: {:.9f}, Train Accuracy: {:.2f}%\".format(e+1, epochs, (toc-tic), train_loss, train_accuracy))\n",
        "        print(\"[On Validation] ==> Precision: {:.3f}, Recall: {:.3f}, F-Score: {:.3f}, Val loss: {:.9f}, Val Accuracy: {:.2f}% Total Time: {:.2f}s\".format(p, r, f, val_loss, val_accuracy, total_time))\n",
        "\n",
        "    torch.save(bow, task_name)\n",
        "    from google.colab import files\n",
        "    files.download(task_name)\n",
        "    print ('\\n')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "[On Training] ==> Time: 29.02s, Train Loss: 0.681805475, Train Accuracy: 55.40%\n",
            "[On Validation] ==> Precision: 0.864, Recall: 0.431, F-Score: 0.575, Val loss: 0.661491506, Val Accuracy: 68.13% Total Time: 29.02s\n",
            "Epoch 2/100\n",
            "[On Training] ==> Time: 27.76s, Train Loss: 0.638137053, Train Accuracy: 74.11%\n",
            "[On Validation] ==> Precision: 0.802, Recall: 0.724, F-Score: 0.761, Val loss: 0.612378043, Val Accuracy: 77.23% Total Time: 56.78s\n",
            "Epoch 3/100\n",
            "[On Training] ==> Time: 27.75s, Train Loss: 0.585768936, Train Accuracy: 78.78%\n",
            "[On Validation] ==> Precision: 0.795, Recall: 0.794, F-Score: 0.794, Val loss: 0.559371899, Val Accuracy: 79.46% Total Time: 84.52s\n",
            "Epoch 4/100\n",
            "[On Training] ==> Time: 27.82s, Train Loss: 0.532907354, Train Accuracy: 80.61%\n",
            "[On Validation] ==> Precision: 0.808, Recall: 0.828, F-Score: 0.818, Val loss: 0.511127691, Val Accuracy: 81.57% Total Time: 112.34s\n",
            "Epoch 5/100\n",
            "[On Training] ==> Time: 27.88s, Train Loss: 0.487021461, Train Accuracy: 81.77%\n",
            "[On Validation] ==> Precision: 0.814, Recall: 0.852, F-Score: 0.833, Val loss: 0.472548046, Val Accuracy: 82.90% Total Time: 140.22s\n",
            "Epoch 6/100\n",
            "[On Training] ==> Time: 27.76s, Train Loss: 0.450151947, Train Accuracy: 83.02%\n",
            "[On Validation] ==> Precision: 0.820, Recall: 0.857, F-Score: 0.838, Val loss: 0.443157029, Val Accuracy: 83.40% Total Time: 167.98s\n",
            "Epoch 7/100\n",
            "[On Training] ==> Time: 27.99s, Train Loss: 0.420768544, Train Accuracy: 83.98%\n",
            "[On Validation] ==> Precision: 0.826, Recall: 0.857, F-Score: 0.841, Val loss: 0.420730087, Val Accuracy: 83.79% Total Time: 195.98s\n",
            "Epoch 8/100\n",
            "[On Training] ==> Time: 27.73s, Train Loss: 0.396737865, Train Accuracy: 85.01%\n",
            "[On Validation] ==> Precision: 0.834, Recall: 0.857, F-Score: 0.845, Val loss: 0.403225587, Val Accuracy: 84.29% Total Time: 223.70s\n",
            "Epoch 9/100\n",
            "[On Training] ==> Time: 27.66s, Train Loss: 0.376408787, Train Accuracy: 85.74%\n",
            "[On Validation] ==> Precision: 0.839, Recall: 0.861, F-Score: 0.850, Val loss: 0.389197865, Val Accuracy: 84.79% Total Time: 251.36s\n",
            "Epoch 10/100\n",
            "[On Training] ==> Time: 27.73s, Train Loss: 0.358716081, Train Accuracy: 86.39%\n",
            "[On Validation] ==> Precision: 0.846, Recall: 0.869, F-Score: 0.858, Val loss: 0.377692858, Val Accuracy: 85.56% Total Time: 279.10s\n",
            "Epoch 11/100\n",
            "[On Training] ==> Time: 27.67s, Train Loss: 0.343000432, Train Accuracy: 87.08%\n",
            "[On Validation] ==> Precision: 0.848, Recall: 0.873, F-Score: 0.861, Val loss: 0.368086056, Val Accuracy: 85.84% Total Time: 306.77s\n",
            "Epoch 12/100\n",
            "[On Training] ==> Time: 27.74s, Train Loss: 0.328857956, Train Accuracy: 87.63%\n",
            "[On Validation] ==> Precision: 0.850, Recall: 0.875, F-Score: 0.862, Val loss: 0.359941975, Val Accuracy: 86.01% Total Time: 334.51s\n",
            "Epoch 13/100\n",
            "[On Training] ==> Time: 27.74s, Train Loss: 0.316025268, Train Accuracy: 88.22%\n",
            "[On Validation] ==> Precision: 0.850, Recall: 0.877, F-Score: 0.863, Val loss: 0.352981144, Val Accuracy: 86.12% Total Time: 362.25s\n",
            "Epoch 14/100\n",
            "[On Training] ==> Time: 27.70s, Train Loss: 0.304309009, Train Accuracy: 88.79%\n",
            "[On Validation] ==> Precision: 0.852, Recall: 0.871, F-Score: 0.862, Val loss: 0.347022434, Val Accuracy: 86.01% Total Time: 389.96s\n",
            "Epoch 15/100\n",
            "[On Training] ==> Time: 27.72s, Train Loss: 0.293567328, Train Accuracy: 89.28%\n",
            "[On Validation] ==> Precision: 0.856, Recall: 0.875, F-Score: 0.865, Val loss: 0.341908911, Val Accuracy: 86.34% Total Time: 417.68s\n",
            "Epoch 16/100\n",
            "[On Training] ==> Time: 27.90s, Train Loss: 0.283677610, Train Accuracy: 89.76%\n",
            "[On Validation] ==> Precision: 0.854, Recall: 0.873, F-Score: 0.863, Val loss: 0.337513846, Val Accuracy: 86.17% Total Time: 445.58s\n",
            "Epoch 17/100\n",
            "[On Training] ==> Time: 27.77s, Train Loss: 0.274544089, Train Accuracy: 90.13%\n",
            "[On Validation] ==> Precision: 0.860, Recall: 0.879, F-Score: 0.869, Val loss: 0.333727514, Val Accuracy: 86.79% Total Time: 473.35s\n",
            "Epoch 18/100\n",
            "[On Training] ==> Time: 27.98s, Train Loss: 0.266079638, Train Accuracy: 90.57%\n",
            "[On Validation] ==> Precision: 0.859, Recall: 0.881, F-Score: 0.870, Val loss: 0.330462760, Val Accuracy: 86.84% Total Time: 501.33s\n",
            "Epoch 19/100\n",
            "[On Training] ==> Time: 27.76s, Train Loss: 0.258206970, Train Accuracy: 90.80%\n",
            "[On Validation] ==> Precision: 0.860, Recall: 0.880, F-Score: 0.870, Val loss: 0.327644819, Val Accuracy: 86.84% Total Time: 529.09s\n",
            "Epoch 20/100\n",
            "[On Training] ==> Time: 27.71s, Train Loss: 0.250855240, Train Accuracy: 91.13%\n",
            "[On Validation] ==> Precision: 0.863, Recall: 0.881, F-Score: 0.872, Val loss: 0.325207732, Val Accuracy: 87.06% Total Time: 556.80s\n",
            "Epoch 21/100\n",
            "[On Training] ==> Time: 27.75s, Train Loss: 0.243965972, Train Accuracy: 91.42%\n",
            "[On Validation] ==> Precision: 0.865, Recall: 0.882, F-Score: 0.874, Val loss: 0.323097163, Val Accuracy: 87.23% Total Time: 584.55s\n",
            "Epoch 22/100\n",
            "[On Training] ==> Time: 27.76s, Train Loss: 0.237488179, Train Accuracy: 91.73%\n",
            "[On Validation] ==> Precision: 0.865, Recall: 0.883, F-Score: 0.874, Val loss: 0.321266231, Val Accuracy: 87.28% Total Time: 612.31s\n",
            "Epoch 23/100\n",
            "[On Training] ==> Time: 27.77s, Train Loss: 0.231377846, Train Accuracy: 92.02%\n",
            "[On Validation] ==> Precision: 0.865, Recall: 0.881, F-Score: 0.873, Val loss: 0.319681084, Val Accuracy: 87.17% Total Time: 640.08s\n",
            "Epoch 24/100\n",
            "[On Training] ==> Time: 27.81s, Train Loss: 0.225597062, Train Accuracy: 92.31%\n",
            "[On Validation] ==> Precision: 0.866, Recall: 0.881, F-Score: 0.873, Val loss: 0.318313273, Val Accuracy: 87.23% Total Time: 667.89s\n",
            "Epoch 25/100\n",
            "[On Training] ==> Time: 27.82s, Train Loss: 0.220112714, Train Accuracy: 92.53%\n",
            "[On Validation] ==> Precision: 0.870, Recall: 0.881, F-Score: 0.875, Val loss: 0.317133546, Val Accuracy: 87.45% Total Time: 695.71s\n",
            "Epoch 26/100\n",
            "[On Training] ==> Time: 27.85s, Train Loss: 0.214895511, Train Accuracy: 92.73%\n",
            "[On Validation] ==> Precision: 0.868, Recall: 0.880, F-Score: 0.874, Val loss: 0.316122817, Val Accuracy: 87.28% Total Time: 723.56s\n",
            "Epoch 27/100\n",
            "[On Training] ==> Time: 27.91s, Train Loss: 0.209919793, Train Accuracy: 92.95%\n",
            "[On Validation] ==> Precision: 0.867, Recall: 0.880, F-Score: 0.873, Val loss: 0.315261751, Val Accuracy: 87.23% Total Time: 751.47s\n",
            "Epoch 28/100\n",
            "[On Training] ==> Time: 27.74s, Train Loss: 0.205164012, Train Accuracy: 93.19%\n",
            "[On Validation] ==> Precision: 0.867, Recall: 0.879, F-Score: 0.873, Val loss: 0.314538316, Val Accuracy: 87.23% Total Time: 779.21s\n",
            "Epoch 29/100\n",
            "[On Training] ==> Time: 27.82s, Train Loss: 0.200609383, Train Accuracy: 93.33%\n",
            "[On Validation] ==> Precision: 0.869, Recall: 0.879, F-Score: 0.874, Val loss: 0.313934955, Val Accuracy: 87.34% Total Time: 807.03s\n",
            "Epoch 30/100\n",
            "[On Training] ==> Time: 27.95s, Train Loss: 0.196239249, Train Accuracy: 93.53%\n",
            "[On Validation] ==> Precision: 0.870, Recall: 0.880, F-Score: 0.875, Val loss: 0.313447539, Val Accuracy: 87.45% Total Time: 834.98s\n",
            "Epoch 31/100\n",
            "[On Training] ==> Time: 27.76s, Train Loss: 0.192040013, Train Accuracy: 93.70%\n",
            "[On Validation] ==> Precision: 0.873, Recall: 0.886, F-Score: 0.879, Val loss: 0.313068014, Val Accuracy: 87.84% Total Time: 862.74s\n",
            "Epoch 32/100\n",
            "[On Training] ==> Time: 27.85s, Train Loss: 0.187998628, Train Accuracy: 93.86%\n",
            "[On Validation] ==> Precision: 0.874, Recall: 0.888, F-Score: 0.881, Val loss: 0.312787782, Val Accuracy: 88.01% Total Time: 890.59s\n",
            "Epoch 33/100\n",
            "[On Training] ==> Time: 27.82s, Train Loss: 0.184103908, Train Accuracy: 94.02%\n",
            "[On Validation] ==> Precision: 0.874, Recall: 0.888, F-Score: 0.881, Val loss: 0.312598195, Val Accuracy: 88.01% Total Time: 918.41s\n",
            "Epoch 34/100\n",
            "[On Training] ==> Time: 27.91s, Train Loss: 0.180345223, Train Accuracy: 94.14%\n",
            "[On Validation] ==> Precision: 0.877, Recall: 0.889, F-Score: 0.883, Val loss: 0.312490760, Val Accuracy: 88.23% Total Time: 946.33s\n",
            "Epoch 35/100\n",
            "[On Training] ==> Time: 27.83s, Train Loss: 0.176714010, Train Accuracy: 94.30%\n",
            "[On Validation] ==> Precision: 0.877, Recall: 0.888, F-Score: 0.883, Val loss: 0.312457038, Val Accuracy: 88.17% Total Time: 974.15s\n",
            "Epoch 36/100\n",
            "[On Training] ==> Time: 27.86s, Train Loss: 0.173201993, Train Accuracy: 94.40%\n",
            "[On Validation] ==> Precision: 0.876, Recall: 0.889, F-Score: 0.883, Val loss: 0.312494764, Val Accuracy: 88.17% Total Time: 1002.02s\n",
            "Epoch 37/100\n",
            "[On Training] ==> Time: 27.85s, Train Loss: 0.169801864, Train Accuracy: 94.56%\n",
            "[On Validation] ==> Precision: 0.877, Recall: 0.888, F-Score: 0.883, Val loss: 0.312598547, Val Accuracy: 88.17% Total Time: 1029.86s\n",
            "Epoch 38/100\n",
            "[On Training] ==> Time: 28.03s, Train Loss: 0.166507290, Train Accuracy: 94.75%\n",
            "[On Validation] ==> Precision: 0.876, Recall: 0.888, F-Score: 0.882, Val loss: 0.312764798, Val Accuracy: 88.12% Total Time: 1057.89s\n",
            "Epoch 39/100\n",
            "[On Training] ==> Time: 27.86s, Train Loss: 0.163312633, Train Accuracy: 94.93%\n",
            "[On Validation] ==> Precision: 0.875, Recall: 0.888, F-Score: 0.882, Val loss: 0.312988502, Val Accuracy: 88.06% Total Time: 1085.75s\n",
            "Epoch 40/100\n",
            "[On Training] ==> Time: 27.80s, Train Loss: 0.160212834, Train Accuracy: 95.06%\n",
            "[On Validation] ==> Precision: 0.875, Recall: 0.889, F-Score: 0.882, Val loss: 0.313267442, Val Accuracy: 88.12% Total Time: 1113.55s\n",
            "Epoch 41/100\n",
            "[On Training] ==> Time: 28.06s, Train Loss: 0.157203116, Train Accuracy: 95.20%\n",
            "[On Validation] ==> Precision: 0.875, Recall: 0.888, F-Score: 0.882, Val loss: 0.313595507, Val Accuracy: 88.06% Total Time: 1141.61s\n",
            "Epoch 42/100\n",
            "[On Training] ==> Time: 27.77s, Train Loss: 0.154279013, Train Accuracy: 95.32%\n",
            "[On Validation] ==> Precision: 0.875, Recall: 0.889, F-Score: 0.882, Val loss: 0.313971270, Val Accuracy: 88.12% Total Time: 1169.38s\n",
            "Epoch 43/100\n",
            "[On Training] ==> Time: 27.80s, Train Loss: 0.151436400, Train Accuracy: 95.39%\n",
            "[On Validation] ==> Precision: 0.876, Recall: 0.888, F-Score: 0.882, Val loss: 0.314389554, Val Accuracy: 88.12% Total Time: 1197.19s\n",
            "Epoch 44/100\n",
            "[On Training] ==> Time: 27.71s, Train Loss: 0.148671873, Train Accuracy: 95.46%\n",
            "[On Validation] ==> Precision: 0.877, Recall: 0.890, F-Score: 0.883, Val loss: 0.314848574, Val Accuracy: 88.23% Total Time: 1224.89s\n",
            "Epoch 45/100\n",
            "[On Training] ==> Time: 27.71s, Train Loss: 0.145981942, Train Accuracy: 95.58%\n",
            "[On Validation] ==> Precision: 0.878, Recall: 0.892, F-Score: 0.885, Val loss: 0.315343938, Val Accuracy: 88.40% Total Time: 1252.60s\n",
            "Epoch 46/100\n",
            "[On Training] ==> Time: 27.64s, Train Loss: 0.143363193, Train Accuracy: 95.70%\n",
            "[On Validation] ==> Precision: 0.878, Recall: 0.893, F-Score: 0.886, Val loss: 0.315873991, Val Accuracy: 88.45% Total Time: 1280.24s\n",
            "Epoch 47/100\n",
            "[On Training] ==> Time: 27.76s, Train Loss: 0.140812571, Train Accuracy: 95.79%\n",
            "[On Validation] ==> Precision: 0.879, Recall: 0.893, F-Score: 0.886, Val loss: 0.316434008, Val Accuracy: 88.51% Total Time: 1308.00s\n",
            "Epoch 48/100\n",
            "[On Training] ==> Time: 27.67s, Train Loss: 0.138327466, Train Accuracy: 95.88%\n",
            "[On Validation] ==> Precision: 0.879, Recall: 0.895, F-Score: 0.887, Val loss: 0.317023468, Val Accuracy: 88.56% Total Time: 1335.68s\n",
            "Epoch 49/100\n",
            "[On Training] ==> Time: 27.86s, Train Loss: 0.135905055, Train Accuracy: 95.96%\n",
            "[On Validation] ==> Precision: 0.879, Recall: 0.893, F-Score: 0.886, Val loss: 0.317638931, Val Accuracy: 88.51% Total Time: 1363.53s\n",
            "Epoch 50/100\n",
            "[On Training] ==> Time: 27.70s, Train Loss: 0.133542822, Train Accuracy: 96.09%\n",
            "[On Validation] ==> Precision: 0.878, Recall: 0.891, F-Score: 0.884, Val loss: 0.318279241, Val Accuracy: 88.34% Total Time: 1391.23s\n",
            "Epoch 51/100\n",
            "[On Training] ==> Time: 27.73s, Train Loss: 0.131238528, Train Accuracy: 96.20%\n",
            "[On Validation] ==> Precision: 0.879, Recall: 0.891, F-Score: 0.885, Val loss: 0.318941102, Val Accuracy: 88.40% Total Time: 1418.96s\n",
            "Epoch 52/100\n",
            "[On Training] ==> Time: 28.02s, Train Loss: 0.128989777, Train Accuracy: 96.30%\n",
            "[On Validation] ==> Precision: 0.879, Recall: 0.890, F-Score: 0.885, Val loss: 0.319624065, Val Accuracy: 88.40% Total Time: 1446.98s\n",
            "Epoch 53/100\n",
            "[On Training] ==> Time: 27.68s, Train Loss: 0.126794719, Train Accuracy: 96.43%\n",
            "[On Validation] ==> Precision: 0.879, Recall: 0.890, F-Score: 0.885, Val loss: 0.320327767, Val Accuracy: 88.40% Total Time: 1474.66s\n",
            "Epoch 54/100\n",
            "[On Training] ==> Time: 27.68s, Train Loss: 0.124651422, Train Accuracy: 96.51%\n",
            "[On Validation] ==> Precision: 0.880, Recall: 0.890, F-Score: 0.885, Val loss: 0.321048729, Val Accuracy: 88.45% Total Time: 1502.35s\n",
            "Epoch 55/100\n",
            "[On Training] ==> Time: 27.67s, Train Loss: 0.122557916, Train Accuracy: 96.60%\n",
            "[On Validation] ==> Precision: 0.880, Recall: 0.890, F-Score: 0.885, Val loss: 0.321788295, Val Accuracy: 88.45% Total Time: 1530.02s\n",
            "Epoch 56/100\n",
            "[On Training] ==> Time: 27.76s, Train Loss: 0.120512775, Train Accuracy: 96.70%\n",
            "[On Validation] ==> Precision: 0.880, Recall: 0.890, F-Score: 0.885, Val loss: 0.322544681, Val Accuracy: 88.45% Total Time: 1557.78s\n",
            "Epoch 57/100\n",
            "[On Training] ==> Time: 27.66s, Train Loss: 0.118514067, Train Accuracy: 96.80%\n",
            "[On Validation] ==> Precision: 0.881, Recall: 0.889, F-Score: 0.885, Val loss: 0.323316535, Val Accuracy: 88.45% Total Time: 1585.44s\n",
            "Epoch 58/100\n",
            "[On Training] ==> Time: 27.68s, Train Loss: 0.116560717, Train Accuracy: 96.88%\n",
            "[On Validation] ==> Precision: 0.882, Recall: 0.889, F-Score: 0.886, Val loss: 0.324103383, Val Accuracy: 88.51% Total Time: 1613.11s\n",
            "Epoch 59/100\n",
            "[On Training] ==> Time: 27.64s, Train Loss: 0.114651220, Train Accuracy: 96.96%\n",
            "[On Validation] ==> Precision: 0.881, Recall: 0.890, F-Score: 0.886, Val loss: 0.324904575, Val Accuracy: 88.51% Total Time: 1640.75s\n",
            "Epoch 60/100\n",
            "[On Training] ==> Time: 27.91s, Train Loss: 0.112784434, Train Accuracy: 97.07%\n",
            "[On Validation] ==> Precision: 0.881, Recall: 0.890, F-Score: 0.886, Val loss: 0.325718445, Val Accuracy: 88.51% Total Time: 1668.66s\n",
            "Epoch 61/100\n",
            "[On Training] ==> Time: 27.69s, Train Loss: 0.110958926, Train Accuracy: 97.15%\n",
            "[On Validation] ==> Precision: 0.880, Recall: 0.891, F-Score: 0.886, Val loss: 0.326546123, Val Accuracy: 88.51% Total Time: 1696.35s\n",
            "Epoch 62/100\n",
            "[On Training] ==> Time: 27.65s, Train Loss: 0.109173374, Train Accuracy: 97.22%\n",
            "[On Validation] ==> Precision: 0.879, Recall: 0.891, F-Score: 0.885, Val loss: 0.327385113, Val Accuracy: 88.40% Total Time: 1724.00s\n",
            "Epoch 63/100\n",
            "[On Training] ==> Time: 27.94s, Train Loss: 0.107426751, Train Accuracy: 97.30%\n",
            "[On Validation] ==> Precision: 0.878, Recall: 0.890, F-Score: 0.884, Val loss: 0.328234062, Val Accuracy: 88.34% Total Time: 1751.94s\n",
            "Epoch 64/100\n",
            "[On Training] ==> Time: 27.73s, Train Loss: 0.105717771, Train Accuracy: 97.38%\n",
            "[On Validation] ==> Precision: 0.878, Recall: 0.889, F-Score: 0.884, Val loss: 0.329095162, Val Accuracy: 88.28% Total Time: 1779.67s\n",
            "Epoch 65/100\n",
            "[On Training] ==> Time: 27.69s, Train Loss: 0.104045160, Train Accuracy: 97.45%\n",
            "[On Validation] ==> Precision: 0.878, Recall: 0.889, F-Score: 0.884, Val loss: 0.329965484, Val Accuracy: 88.28% Total Time: 1807.36s\n",
            "Epoch 66/100\n",
            "[On Training] ==> Time: 27.74s, Train Loss: 0.102408179, Train Accuracy: 97.52%\n",
            "[On Validation] ==> Precision: 0.879, Recall: 0.889, F-Score: 0.884, Val loss: 0.330843981, Val Accuracy: 88.34% Total Time: 1835.09s\n",
            "Epoch 67/100\n",
            "[On Training] ==> Time: 27.75s, Train Loss: 0.100805509, Train Accuracy: 97.55%\n",
            "[On Validation] ==> Precision: 0.879, Recall: 0.889, F-Score: 0.884, Val loss: 0.331730513, Val Accuracy: 88.34% Total Time: 1862.84s\n",
            "Epoch 68/100\n",
            "[On Training] ==> Time: 27.74s, Train Loss: 0.099236476, Train Accuracy: 97.59%\n",
            "[On Validation] ==> Precision: 0.878, Recall: 0.889, F-Score: 0.884, Val loss: 0.332626747, Val Accuracy: 88.28% Total Time: 1890.58s\n",
            "Epoch 69/100\n",
            "[On Training] ==> Time: 27.83s, Train Loss: 0.097699922, Train Accuracy: 97.67%\n",
            "[On Validation] ==> Precision: 0.878, Recall: 0.890, F-Score: 0.884, Val loss: 0.333528354, Val Accuracy: 88.34% Total Time: 1918.41s\n",
            "Epoch 70/100\n",
            "[On Training] ==> Time: 27.69s, Train Loss: 0.096194951, Train Accuracy: 97.73%\n",
            "[On Validation] ==> Precision: 0.878, Recall: 0.890, F-Score: 0.884, Val loss: 0.334439441, Val Accuracy: 88.34% Total Time: 1946.10s\n",
            "Epoch 71/100\n",
            "[On Training] ==> Time: 27.90s, Train Loss: 0.094721030, Train Accuracy: 97.79%\n",
            "[On Validation] ==> Precision: 0.877, Recall: 0.890, F-Score: 0.884, Val loss: 0.335355496, Val Accuracy: 88.28% Total Time: 1974.00s\n",
            "Epoch 72/100\n",
            "[On Training] ==> Time: 27.71s, Train Loss: 0.093277011, Train Accuracy: 97.87%\n",
            "[On Validation] ==> Precision: 0.878, Recall: 0.890, F-Score: 0.884, Val loss: 0.336280089, Val Accuracy: 88.34% Total Time: 2001.71s\n",
            "Epoch 73/100\n",
            "[On Training] ==> Time: 27.73s, Train Loss: 0.091862149, Train Accuracy: 97.94%\n",
            "[On Validation] ==> Precision: 0.878, Recall: 0.890, F-Score: 0.884, Val loss: 0.337210102, Val Accuracy: 88.34% Total Time: 2029.44s\n",
            "Epoch 74/100\n",
            "[On Training] ==> Time: 28.00s, Train Loss: 0.090475713, Train Accuracy: 98.00%\n",
            "[On Validation] ==> Precision: 0.878, Recall: 0.888, F-Score: 0.883, Val loss: 0.338145057, Val Accuracy: 88.23% Total Time: 2057.44s\n",
            "Epoch 75/100\n",
            "[On Training] ==> Time: 27.72s, Train Loss: 0.089116937, Train Accuracy: 98.07%\n",
            "[On Validation] ==> Precision: 0.878, Recall: 0.888, F-Score: 0.883, Val loss: 0.339084983, Val Accuracy: 88.23% Total Time: 2085.16s\n",
            "Epoch 76/100\n",
            "[On Training] ==> Time: 27.75s, Train Loss: 0.087784825, Train Accuracy: 98.12%\n",
            "[On Validation] ==> Precision: 0.877, Recall: 0.888, F-Score: 0.883, Val loss: 0.340032132, Val Accuracy: 88.17% Total Time: 2112.91s\n",
            "Epoch 77/100\n",
            "[On Training] ==> Time: 27.79s, Train Loss: 0.086479056, Train Accuracy: 98.17%\n",
            "[On Validation] ==> Precision: 0.877, Recall: 0.888, F-Score: 0.883, Val loss: 0.340983528, Val Accuracy: 88.17% Total Time: 2140.70s\n",
            "Epoch 78/100\n",
            "[On Training] ==> Time: 27.80s, Train Loss: 0.085198623, Train Accuracy: 98.23%\n",
            "[On Validation] ==> Precision: 0.876, Recall: 0.887, F-Score: 0.881, Val loss: 0.341940508, Val Accuracy: 88.06% Total Time: 2168.50s\n",
            "Epoch 79/100\n",
            "[On Training] ==> Time: 27.76s, Train Loss: 0.083942922, Train Accuracy: 98.25%\n",
            "[On Validation] ==> Precision: 0.876, Recall: 0.887, F-Score: 0.881, Val loss: 0.342901505, Val Accuracy: 88.06% Total Time: 2196.26s\n",
            "Epoch 80/100\n",
            "[On Training] ==> Time: 27.78s, Train Loss: 0.082711237, Train Accuracy: 98.30%\n",
            "[On Validation] ==> Precision: 0.876, Recall: 0.887, F-Score: 0.881, Val loss: 0.343866867, Val Accuracy: 88.06% Total Time: 2224.04s\n",
            "Epoch 81/100\n",
            "[On Training] ==> Time: 27.78s, Train Loss: 0.081503193, Train Accuracy: 98.35%\n",
            "[On Validation] ==> Precision: 0.877, Recall: 0.887, F-Score: 0.882, Val loss: 0.344837765, Val Accuracy: 88.12% Total Time: 2251.82s\n",
            "Epoch 82/100\n",
            "[On Training] ==> Time: 27.99s, Train Loss: 0.080318096, Train Accuracy: 98.41%\n",
            "[On Validation] ==> Precision: 0.877, Recall: 0.886, F-Score: 0.881, Val loss: 0.345813252, Val Accuracy: 88.06% Total Time: 2279.81s\n",
            "Epoch 83/100\n",
            "[On Training] ==> Time: 27.74s, Train Loss: 0.079155110, Train Accuracy: 98.46%\n",
            "[On Validation] ==> Precision: 0.878, Recall: 0.886, F-Score: 0.882, Val loss: 0.346791454, Val Accuracy: 88.12% Total Time: 2307.55s\n",
            "Epoch 84/100\n",
            "[On Training] ==> Time: 27.75s, Train Loss: 0.078013775, Train Accuracy: 98.49%\n",
            "[On Validation] ==> Precision: 0.879, Recall: 0.886, F-Score: 0.882, Val loss: 0.347776524, Val Accuracy: 88.17% Total Time: 2335.30s\n",
            "Epoch 85/100\n",
            "[On Training] ==> Time: 27.95s, Train Loss: 0.076893724, Train Accuracy: 98.54%\n",
            "[On Validation] ==> Precision: 0.879, Recall: 0.886, F-Score: 0.882, Val loss: 0.348763135, Val Accuracy: 88.17% Total Time: 2363.25s\n",
            "Epoch 86/100\n",
            "[On Training] ==> Time: 27.90s, Train Loss: 0.075794243, Train Accuracy: 98.56%\n",
            "[On Validation] ==> Precision: 0.879, Recall: 0.886, F-Score: 0.882, Val loss: 0.349757315, Val Accuracy: 88.17% Total Time: 2391.14s\n",
            "Epoch 87/100\n",
            "[On Training] ==> Time: 27.78s, Train Loss: 0.074715109, Train Accuracy: 98.57%\n",
            "[On Validation] ==> Precision: 0.879, Recall: 0.886, F-Score: 0.882, Val loss: 0.350754946, Val Accuracy: 88.17% Total Time: 2418.92s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type BOWClassifier. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-877c603546cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0;34m'port'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m       \u001b[0;34m'path'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m       \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m   })\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: TypeError: Failed to fetch"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "FxO_e5P1mX3I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Task-2"
      ]
    },
    {
      "metadata": {
        "id": "lpUB7Obs197T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "task2 = [(10, 10, 'Task2A.mdl'), (20, 10, 'Task2B.mdl'), (30, 30, 'Task2C.mdl'), (50, 50, 'Task2D.mdl'), (100, 50, 'Task2E.mdl')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UiqcS_4XmXbd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for num_of_hidden1, num_of_hidden2, task_name in task2:\n",
        "    class BOWClassifier(nn.Module):\n",
        "        def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
        "            super(BOWClassifier, self).__init__()\n",
        "            SEED = 42\n",
        "            torch.manual_seed(SEED)\n",
        "            torch.cuda.manual_seed(SEED)\n",
        "            self.hidden_size = hidden_size\n",
        "            self.i2h = nn.Linear(input_size, hidden_size1) # initialises weights and biases i2h\n",
        "            self.h2h = nn.Linear(hidden_size1, hidden_size2) # initialises weights and biases i2h\n",
        "            self.h2o = nn.Linear(hidden_size2, output_size) # initialises weights and biases h2o\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = torch.relu(self.i2h(x)) # relu activation @ hidden layer\n",
        "            x = torch.relu(self.h2h(x)) # relu activation @ hidden layer\n",
        "            x = torch.sigmoid(self.h2o(x)) # sigmoid activation @ output layer\n",
        "            return x\n",
        "\n",
        "    num_of_input = VOCAB_SIZE # Bag word restriction has to be equal to vocabulary\n",
        "    num_of_output = 1 # binary sentiment classes (+ve, -ve)\n",
        "\n",
        "    bow = BOWClassifier(num_of_input, num_of_hidden1, num_of_hidden2, num_of_output).to('cuda:0') # initialises weights and biases\n",
        "\n",
        "    # define a loss function and an optimizer\n",
        "    loss_function = nn.BCELoss()\n",
        "    opt = torch.optim.SGD(bow.parameters(), lr = 0.0001)\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    # Train The Model\n",
        "    epochs = 100\n",
        "    # the training loop\n",
        "    total_time = 0.0\n",
        "    prev_val_loss = 0\n",
        "    no_change = 0\n",
        "\n",
        "    for e in range(epochs):\n",
        "        tic = time.time() # start the timer\n",
        "        correct = 0\n",
        "        cumulative_loss = 0\n",
        "        incorrect = 0\n",
        "\n",
        "        for i, instance in enumerate(train_data): # train_text_reviews \n",
        "            # get the training data\n",
        "            label = train_text_labels[i] # get the label of the corresponding instace\n",
        "            label = Variable(torch.FloatTensor([label])).resize_((1,1)).to('cuda:0') # wraps a tensor for label\n",
        "\n",
        "            bow.zero_grad() # reset the gradient for each instance\n",
        "\n",
        "            pred = bow.forward(instance)\n",
        "\n",
        "            loss = loss_function(pred, label) # compute the loss\n",
        "            loss.backward() # backprop the loss\n",
        "            opt.step() # performs parameter updation based on the current gradient\n",
        "\n",
        "            cumulative_loss += float(loss.item()) # accumulate the loss over whole training sample\n",
        "\n",
        "            pred_class = 1 if pred.item() > 0.5 else 0 # as sigmoid activated\n",
        "            if(int(label) == pred_class): # counting correct prediction in each epoch\n",
        "                correct += 1\n",
        "\n",
        "        train_loss = float(cumulative_loss)/num_train_data\n",
        "        train_accuracy = correct*float(100)/num_train_data\n",
        "\n",
        "        val_loss, val_accuracy, p, r, f = evaluate_validation_accuracy(val_data, bow) # test val-data-set on currently trained model\n",
        "\n",
        "        if(abs(prev_val_loss-val_loss) < 0.001): # less delta validation loss\n",
        "            no_change = 1\n",
        "\n",
        "        if(no_change == 1 and abs(prev_val_loss-val_loss) > 0.001): # early stop if nearly no change\n",
        "            break\n",
        "\n",
        "\n",
        "        prev_val_loss = val_loss\n",
        "\n",
        "        toc = time.time() # final time\n",
        "        total_time += (toc-tic)\n",
        "\n",
        "        print(\"Epoch {}/{}\\n[On Training] ==> Time: {:.2f}s, Train Loss: {:.9f}, Train Accuracy: {:.2f}%\".format(e+1, epochs, (toc-tic), train_loss, train_accuracy))\n",
        "        print(\"[On Validation] ==> Precision: {:.3f}, Recall: {:.3f}, F-Score: {:.3f}, Val loss: {:.9f}, Val Accuracy: {:.2f}% Total Time: {:.2f}s\".format(p, r, f, val_loss, val_accuracy, total_time))\n",
        "\n",
        "\n",
        "    torch.save(bow, task_name)\n",
        "    from google.colab import files\n",
        "    files.download(task_name)\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n2JByG_xnrr1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Task-3"
      ]
    },
    {
      "metadata": {
        "id": "vtAGGeuU2ect",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "task3 = [(100, 50, 10, 'Task3A.mdl'), (200, 100, 10, 'Task3B.mdl')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rz1rm-runrPt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for num_of_hidden1, num_of_hidden2, num_of_hidden3, task_name in task3:\n",
        "    class BOWClassifier(nn.Module):\n",
        "        def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
        "            super(BOWClassifier, self).__init__()\n",
        "            SEED = 42\n",
        "            torch.manual_seed(SEED)\n",
        "            torch.cuda.manual_seed(SEED)\n",
        "            self.hidden_size = hidden_size\n",
        "            self.i2h1 = nn.Linear(input_size, hidden_size1) # initialises weights and biases i2h1\n",
        "            self.h12h2 = nn.Linear(hidden_size1, hidden_size2) # initialises weights and biases h12h2\n",
        "            self.h22h3 = nn.Linear(hidden_size1, hidden_size2) # initialises weights and biases h22h3\n",
        "            self.h32o = nn.Linear(hidden_size2, output_size) # initialises weights and biases h32o\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = torch.relu(self.i2h1(x)) # relu activation @ hidden layer\n",
        "            x = torch.relu(self.h12h2(x)) # relu activation @ hidden layer\n",
        "            x = torch.relu(self.h22h3(x)) # relu activation @ hidden layer\n",
        "            x = torch.sigmoid(self.h32o(x)) # sigmoid activation @ output layer\n",
        "            return x\n",
        "\n",
        "    num_of_input = VOCAB_SIZE # Bag word restriction has to be equal to vocabulary\n",
        "    num_of_output = 1 # binary sentiment classes (+ve, -ve)\n",
        "\n",
        "    bow = BOWClassifier(num_of_input, num_of_hidden1, num_of_hidden2, num_of_hidden3, num_of_output).to('cuda:0') # initialises weights and biases\n",
        "\n",
        "    # define a loss function and an optimizer\n",
        "    loss_function = nn.BCELoss()\n",
        "    opt = torch.optim.SGD(bow.parameters(), lr = 0.0001)\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    # Train The Model\n",
        "    epochs = 100\n",
        "    # the training loop\n",
        "    total_time = 0.0\n",
        "    prev_val_loss = 0\n",
        "    no_change = 0\n",
        "\n",
        "    for e in range(epochs):\n",
        "        tic = time.time() # start the timer\n",
        "        correct = 0\n",
        "        cumulative_loss = 0\n",
        "        incorrect = 0\n",
        "\n",
        "        for i, instance in enumerate(train_data): # train_text_reviews \n",
        "            # get the training data\n",
        "            label = train_text_labels[i] # get the label of the corresponding instace\n",
        "            label = Variable(torch.FloatTensor([label])).resize_((1,1)).to('cuda:0') # wraps a tensor for label\n",
        "\n",
        "            bow.zero_grad() # reset the gradient for each instance\n",
        "\n",
        "            pred = bow.forward(instance)\n",
        "\n",
        "            loss = loss_function(pred, label) # compute the loss\n",
        "            loss.backward() # backprop the loss\n",
        "            opt.step() # performs parameter updation based on the current gradient\n",
        "\n",
        "            cumulative_loss += float(loss.item()) # accumulate the loss over whole training sample\n",
        "\n",
        "            pred_class = 1 if pred.item() > 0.5 else 0 # as sigmoid activated\n",
        "            if(int(label) == pred_class): # counting correct prediction in each epoch\n",
        "                correct += 1\n",
        "\n",
        "        train_loss = float(cumulative_loss)/num_train_data\n",
        "        train_accuracy = correct*float(100)/num_train_data\n",
        "\n",
        "        val_loss, val_accuracy, p, r, f = evaluate_validation_accuracy(val_data, bow) # test val-data-set on currently trained model\n",
        "\n",
        "        if(abs(prev_val_loss-val_loss) < 0.001): # less delta validation loss\n",
        "            no_change = 1\n",
        "\n",
        "        if(no_change == 1 and abs(prev_val_loss-val_loss) > 0.001): # early stop if nearly no change\n",
        "            break\n",
        "\n",
        "\n",
        "        prev_val_loss = val_loss\n",
        "\n",
        "        toc = time.time() # final time\n",
        "        total_time += (toc-tic)\n",
        "\n",
        "        print(\"Epoch {}/{}\\n[On Training] ==> Time: {:.2f}s, Train Loss: {:.9f}, Train Accuracy: {:.2f}%\".format(e+1, epochs, (toc-tic), train_loss, train_accuracy))\n",
        "        print(\"[On Validation] ==> Precision: {:.3f}, Recall: {:.3f}, F-Score: {:.3f}, Val loss: {:.9f}, Val Accuracy: {:.2f}% Total Time: {:.2f}s\".format(p, r, f, val_loss, val_accuracy, total_time))\n",
        "\n",
        "\n",
        "    torch.save(bow, task_name)\n",
        "    from google.colab import files\n",
        "    files.download(task_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FDUyFfwmXSew",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# plt.figure(\"Image\")\n",
        "# plt.title(\"Loss vs Epoch\")\n",
        "# val_loss_history_plt =  [float(i)/sum(val_loss_history) for i in val_loss_history] # normalised between 0-1\n",
        "# train_loss_history_plt =  [float(i)/sum(train_loss_history) for i in train_loss_history] # normalised between 0-1\n",
        "# f_score_plt = [float(i)/sum(f_score) for i in f_score] # normalised between 0-1\n",
        "# plt.plot(val_loss_history_plt, c=\"red\", label=\"Validation Loss\")\n",
        "# plt.plot(train_loss_history_plt, c=\"blue\", label = \"Training Loss\")\n",
        "# plt.plot(f_score_plt, c=\"green\", label = \"F-Score\")\n",
        "# plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fkYBaktsF-sm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_test = []\n",
        "data_test = [Variable(make_bow_vector(instance, word_to_ix)).to('cuda:0') for instance in test_text_reviews]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ffYv5PAmjyp-",
        "colab_type": "code",
        "outputId": "22e349f5-319f-4bb0-9d38-a284b0b92668",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print('--- AFTER TRAINING ---')\n",
        "correct = 0\n",
        "tic = time.time()\n",
        "for i, instance in enumerate(data_test):\n",
        "    label = test_text_labels[i]\n",
        "    pred = bow.forward(instance)\n",
        "#     pred = more_bow.forward(instance)\n",
        "#     pred = stop_bow.forward(instance)\n",
        "    pred_class = 1 if pred.item() > 0.5 else 0 # sigmoid activated\n",
        "    \n",
        "    if(int(label) == pred_class):\n",
        "        correct += 1\n",
        "toc = time.time()\n",
        "print(\"Time: {}, Test Accuracy: {:.2f}%\".format((toc-tic), correct*100/len(data_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- AFTER TRAINING ---\n",
            "Time: 2.3347599506378174, Test Accuracy: 88.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hBL6jT8ReP6p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# predict on test dataset\n",
        "# with open(\"Task1A.csv\", \"wb\") as _f:\n",
        "#     writer = csv.writer(_f)\n",
        "#     writer.writerows(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0VkVoMzMUdeh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.save('vocab.npy', word_to_ix)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"vocab.npy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zz5PwYjVudEk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# temp_test = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "29twCfxpuc8r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# _model_ = torch.load(io.BytesIO(temp_test['Task1A.mdl']))\n",
        "_model_ = torch.load('Task1A.mdl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ixy30WkrH3_-",
        "colab_type": "code",
        "outputId": "5051fde4-bd84-4be0-e928-43ba4981e7a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "cell_type": "code",
      "source": [
        "for params in _model_.parameters():\n",
        "    print (params)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.0151,  0.0169,  0.0136,  ..., -0.0055, -0.0031,  0.0019],\n",
            "        [ 0.0034,  0.0107,  0.0036,  ..., -0.0053, -0.0017, -0.0016],\n",
            "        [ 0.0010,  0.0303,  0.0249,  ..., -0.0022, -0.0006, -0.0046],\n",
            "        ...,\n",
            "        [ 0.0217,  0.0183,  0.0223,  ..., -0.0057,  0.0012,  0.0054],\n",
            "        [ 0.0069,  0.0192,  0.0174,  ..., -0.0051,  0.0045, -0.0035],\n",
            "        [ 0.0195,  0.0276,  0.0161,  ...,  0.0043, -0.0042,  0.0040]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 1.9388e-02,  1.2652e-02,  4.1159e-02,  2.0152e-03,  7.0361e-02,\n",
            "         2.8925e-02,  3.4369e-03,  7.5747e-02,  5.0110e-02,  7.8395e-02,\n",
            "         8.7255e-03,  6.0778e-02,  1.6623e-02,  5.4828e-02,  6.3786e-02,\n",
            "         4.4470e-02,  3.7429e-02,  8.7677e-03,  8.2076e-03,  6.4079e-02,\n",
            "         5.6118e-02,  1.5269e-05,  9.1932e-03,  5.0826e-02,  3.5060e-02,\n",
            "         2.8042e-02,  4.6040e-02,  7.2489e-02,  2.3590e-02,  4.1974e-02,\n",
            "         6.0417e-02,  6.6461e-02,  6.8698e-02,  3.9010e-02,  7.1245e-02,\n",
            "         2.4061e-02, -2.3293e-03,  3.5910e-02,  1.3214e-02,  6.7373e-02,\n",
            "         4.2543e-02,  1.3523e-03,  2.2665e-02,  4.8760e-02,  6.7036e-03,\n",
            "         3.9539e-02,  3.1032e-02,  2.7630e-02,  3.3611e-02,  2.7030e-02],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.2971,  0.2937,  0.7619,  0.0139,  1.2378, -0.4903, -0.0358, -1.1436,\n",
            "          0.9706, -1.2221,  0.1101,  1.1065,  0.2754,  1.1788,  0.9083, -0.6790,\n",
            "          0.5652, -0.1738, -0.1145,  1.1612, -0.9057, -0.0058,  0.1727, -0.7861,\n",
            "          0.6577, -0.4439, -0.7056, -1.0424,  0.3608,  0.6924, -1.0036,  1.3303,\n",
            "         -1.1616, -0.5692,  1.2612,  0.4606, -0.0224, -0.5349,  0.2989, -1.1362,\n",
            "          0.7330, -0.0127, -0.4002,  0.8672, -0.0716, -0.5672, -0.5046,  0.5731,\n",
            "          0.5314, -0.5340]], device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0867], device='cuda:0', requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ub0HKgL8H37z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lCraW41hH32S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mhftDt1DH3x6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kE_TcX1KH3tQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T60wqxmoH3Zl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}