{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "18CS60R19_Assignment5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajeshsahu09/CS69002_9A_18CS60R19/blob/master/18CS60R19_Assignment5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "A83kqH2ssH4i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading the MNIST Dataset"
      ]
    },
    {
      "metadata": {
        "id": "RnusCPY-sH4k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install mxnet-cu100\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import matplotlib as mpl\n",
        "import tarfile\n",
        "import matplotlib.image as mpimg\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import mxnet as mx\n",
        "from mxnet import gluon\n",
        "from mxnet import ndarray as nd\n",
        "from mxnet.gluon import nn, utils\n",
        "from mxnet import autograd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U-E8Kjp_sH4n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c46c8ab5-1f1a-4944-d585-90e2628997b5"
      },
      "cell_type": "code",
      "source": [
        "mnist_train = mx.gluon.data.vision.datasets.MNIST(train=True)\n",
        "mnist_test = mx.gluon.data.vision.datasets.MNIST(train=False)\n",
        "\n",
        "x_train = np.zeros((70000, 28, 28))\n",
        "for i, (data, label) in enumerate(mnist_train):\n",
        "    x_train[i] = data.asnumpy()[:,:,0]\n",
        "for i, (data, label) in enumerate(mnist_test):\n",
        "    x_train[len(mnist_train)+i] = data.asnumpy()[:,:,0]\n",
        "x_train[0].shape, x_train.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((28, 28), (70000, 28, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "ZHrHs3wXsH4t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "de408595-dfa4-495f-c2c2-cba54de47688"
      },
      "cell_type": "code",
      "source": [
        "# shuffle the dataset\n",
        "#Use a seed so that we get the same random permutation each time\n",
        "np.random.seed(42)\n",
        "x_train = x_train[np.random.permutation(x_train.shape[0])]\n",
        "x_train[0].shape, x_train.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((28, 28), (70000, 28, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "84iDLH9RsH4v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "630bf165-2e8a-4dd8-f693-caaa713945ff"
      },
      "cell_type": "code",
      "source": [
        "'''DCGAN take 64*64 image as input so reshaped the train data'''\n",
        "import cv2\n",
        "x_train = np.asarray([cv2.resize(x, (64,64)) for x in x_train])\n",
        "x_train[0].shape, x_train.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((64, 64), (70000, 64, 64))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "MXYFl7TVsH4y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bd31b9c3-5341-4593-b67b-55a1030e5904"
      },
      "cell_type": "code",
      "source": [
        "'''Each pixel in the 64x64 image is represented by a number between 0-255, that represents the intensity of the pixel. \n",
        "However, we want to input numbers between -1 and 1 into the DCGAN. To rescale the pixel values, we will \n",
        "divide it by (255/2). This changes the scale to 0-2. We then subtract by 1 to get them in the range of -1 to 1. '''\n",
        "\n",
        "x_train = x_train.astype(np.float32, copy=False)/(255.0/2) - 1.0\n",
        "x_train[0].shape, x_train.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((64, 64), (70000, 64, 64))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "pqlIsXHLsH41",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''Ultimately, images are fed into the neural net through a 70000x3x64x64 array \n",
        "but they are currently in a 70000x64x64 array. We need to add 3 channels to the images. \n",
        "Typically, when we are working with the images, the 3 channels represent \n",
        "the red, green, and blue (RGB) components of each image. Since the MNIST dataset is grayscale, \n",
        "we only need 1 channel to represent the dataset. We will pad the other channels with 0's:'''\n",
        "\n",
        "x_train = x_train.reshape((70000, 1, 64, 64))\n",
        "x_train = np.tile(x_train, (1, 3, 1, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8oCBYuRVsH44",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "15a1b567-f9a2-430c-b0a4-393616466bae"
      },
      "cell_type": "code",
      "source": [
        "x_train[0].shape, x_train.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3, 64, 64), (70000, 3, 64, 64))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "rCAkGnjzsH48",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochs = 20 # Set low by default for tests, set higher when you actually run this code.\n",
        "batch_size = 64\n",
        "latent_z_size = 100\n",
        "\n",
        "ctx = mx.gpu() if mx.test_utils.list_gpus() else mx.cpu()\n",
        "\n",
        "lr = 0.0002\n",
        "beta1 = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JI7Hlp6AsH5A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data = mx.io.NDArrayIter(x_train, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QcLlx6W4sH5D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define the network"
      ]
    },
    {
      "metadata": {
        "id": "1cYUoRDZsH5J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "real_label = nd.ones((batch_size,), ctx=ctx)\n",
        "fake_label = nd.zeros((batch_size,),ctx=ctx)\n",
        "\n",
        "def facc(label, pred):\n",
        "    pred = pred.ravel()\n",
        "    label = label.ravel()\n",
        "    return ((pred > 0.5) == label).mean()\n",
        "metric = mx.metric.CustomMetric(facc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R8WUPzUfvYYA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def visualize(img_arr):\n",
        "    plt.imshow(((img_arr.asnumpy().transpose(1, 2, 0) + 1.0) * 127.5).astype(np.uint8))\n",
        "    plt.axis('off')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ycaPUjj_uow2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Task-3"
      ]
    },
    {
      "metadata": {
        "id": "tk0IrVaDxVBx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# build the generator\n",
        "nc = 3\n",
        "ngf = 64\n",
        "netG = nn.Sequential()\n",
        "with netG.name_scope():\n",
        "    # input is Z, going into a convolution\n",
        "    netG.add(nn.Conv2DTranspose(ngf * 8, 4, 1, 0, use_bias=False))\n",
        "    netG.add(nn.BatchNorm())\n",
        "    netG.add(nn.Activation('relu'))\n",
        "    # state size. (ngf*8) x 4 x 4\n",
        "    netG.add(nn.Conv2DTranspose(ngf * 4, 4, 2, 1, use_bias=False))\n",
        "    netG.add(nn.BatchNorm())\n",
        "    netG.add(nn.Activation('relu'))\n",
        "    # state size. (ngf*8) x 8 x 8\n",
        "    netG.add(nn.Conv2DTranspose(ngf * 2, 4, 2, 1, use_bias=False))\n",
        "    netG.add(nn.BatchNorm())\n",
        "    netG.add(nn.Activation('relu'))\n",
        "    # state size. (ngf*8) x 16 x 16\n",
        "    netG.add(nn.Conv2DTranspose(ngf, 4, 2, 1, use_bias=False))\n",
        "    netG.add(nn.BatchNorm())\n",
        "    netG.add(nn.Activation('relu'))\n",
        "    # state size. (ngf*8) x 32 x 32\n",
        "    netG.add(nn.Conv2DTranspose(nc, 4, 2, 1, use_bias=False))\n",
        "    netG.add(nn.Activation('tanh'))\n",
        "    # state size. (nc) x 64 x 64\n",
        "\n",
        "# build the discriminator\n",
        "ndf = 64\n",
        "netD = nn.Sequential()\n",
        "with netD.name_scope():\n",
        "    # input is (nc) x 64 x 64\n",
        "    netD.add(nn.Conv2D(ndf, 4, 2, 1, use_bias=False))\n",
        "    netD.add(nn.LeakyReLU(0.2))\n",
        "    # state size. (ndf) x 32 x 32\n",
        "    netD.add(nn.Conv2D(ndf * 2, 4, 2, 1, use_bias=False))\n",
        "    netD.add(nn.BatchNorm())\n",
        "    netD.add(nn.LeakyReLU(0.2))\n",
        "    # state size. (ndf) x 16 x 16\n",
        "    netD.add(nn.Conv2D(ndf * 4, 4, 2, 1, use_bias=False))\n",
        "    netD.add(nn.BatchNorm())\n",
        "    netD.add(nn.LeakyReLU(0.2))\n",
        "    # state size. (ndf) x 8 x 8\n",
        "    netD.add(nn.Conv2D(ndf * 8, 4, 2, 1, use_bias=False))\n",
        "    netD.add(nn.BatchNorm())\n",
        "    netD.add(nn.LeakyReLU(0.2))\n",
        "    # state size. (ndf) x 4 x 4\n",
        "    netD.add(nn.Conv2D(1, 4, 1, 0, use_bias=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EdbFIQo9sH5G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# loss\n",
        "loss = gluon.loss.SigmoidBinaryCrossEntropyLoss()\n",
        "\n",
        "# initialize the generator and the discriminator\n",
        "netG.initialize(mx.init.Normal(0.02), ctx=ctx)\n",
        "netD.initialize(mx.init.Normal(0.02), ctx=ctx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mWJfq8ycv_AO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### ADAM"
      ]
    },
    {
      "metadata": {
        "id": "dkeNyJ-ov-Qp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# trainer for the generator and the discriminator\n",
        "trainerG = gluon.Trainer(netG.collect_params(), 'adam', {'learning_rate': lr, 'beta1': beta1})\n",
        "trainerD = gluon.Trainer(netD.collect_params(), 'adam', {'learning_rate': lr, 'beta1': beta1})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wasn8CV8sH5M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1904
        },
        "outputId": "0c81ada8-730a-4aef-92a3-dd8095c6d4da"
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    print ('epoch:'+str(epoch+1))\n",
        "    tic = time.time()\n",
        "    btic = time.time()\n",
        "    train_data.reset()\n",
        "    iter = 0\n",
        "    for batch in train_data:\n",
        "        ############################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "        data = batch.data[0].as_in_context(ctx)\n",
        "        latent_z = mx.nd.random_normal(0, 1, shape=(batch_size, latent_z_size, 1, 1), ctx=ctx)\n",
        "\n",
        "        with autograd.record():\n",
        "            # train with real image\n",
        "            output = netD(data).reshape((-1, 1))\n",
        "            errD_real = loss(output, real_label)\n",
        "            metric.update([real_label,], [output,])\n",
        "\n",
        "            # train with fake image\n",
        "            fake = netG(latent_z)\n",
        "            output = netD(fake.detach()).reshape((-1, 1))\n",
        "            errD_fake = loss(output, fake_label)\n",
        "            errD = errD_real + errD_fake\n",
        "            errD.backward()\n",
        "            metric.update([fake_label,], [output,])\n",
        "\n",
        "        trainerD.step(batch.data[0].shape[0])\n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "        with autograd.record():\n",
        "            fake = netG(latent_z)\n",
        "            output = netD(fake).reshape((-1, 1))\n",
        "            errG = loss(output, real_label)\n",
        "            errG.backward()\n",
        "\n",
        "        trainerG.step(batch.data[0].shape[0])\n",
        "\n",
        "        # Print log infomation every ten batches\n",
        "        if iter % 100 == 0:\n",
        "            name, acc = metric.get()\n",
        "            print('speed: {} samples/s'.format(batch_size / (time.time() - btic)))\n",
        "            print('discriminator loss = %f, generator loss = %f, binary training acc = %f at iter %d epoch %d'\n",
        "                     %(nd.mean(errD).asscalar(),\n",
        "                       nd.mean(errG).asscalar(), acc, iter, epoch))\n",
        "        iter = iter + 1\n",
        "        btic = time.time()\n",
        "\n",
        "    name, acc = metric.get()\n",
        "    metric.reset()\n",
        "    print('\\nbinary training acc at epoch %d: %s=%f' % (epoch, name, acc))\n",
        "    print('time: %f' % (time.time() - tic))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:1\n",
            "speed: 95.76186840832261 samples/s\n",
            "discriminator loss = 1.440271, generator loss = 5.971055, binary training acc = 0.578125 at iter 0 epoch 0\n",
            "speed: 832.1386797278237 samples/s\n",
            "discriminator loss = 0.046643, generator loss = 7.193783, binary training acc = 0.937809 at iter 100 epoch 0\n",
            "speed: 844.107455402486 samples/s\n",
            "discriminator loss = 0.117289, generator loss = 3.946844, binary training acc = 0.933652 at iter 200 epoch 0\n",
            "speed: 801.3716332805923 samples/s\n",
            "discriminator loss = 0.143659, generator loss = 4.830194, binary training acc = 0.935605 at iter 300 epoch 0\n",
            "speed: 799.4170625005584 samples/s\n",
            "discriminator loss = 0.033756, generator loss = 7.072913, binary training acc = 0.937286 at iter 400 epoch 0\n",
            "speed: 782.31871488191 samples/s\n",
            "discriminator loss = 0.346571, generator loss = 2.709261, binary training acc = 0.930748 at iter 500 epoch 0\n",
            "speed: 796.0695727474118 samples/s\n",
            "discriminator loss = 0.892779, generator loss = 2.766939, binary training acc = 0.933600 at iter 600 epoch 0\n",
            "speed: 754.3394247657052 samples/s\n",
            "discriminator loss = 0.667028, generator loss = 2.037327, binary training acc = 0.927525 at iter 700 epoch 0\n",
            "speed: 767.8733119173415 samples/s\n",
            "discriminator loss = 0.236035, generator loss = 2.998059, binary training acc = 0.926625 at iter 800 epoch 0\n",
            "speed: 749.660563679219 samples/s\n",
            "discriminator loss = 1.663052, generator loss = 1.233248, binary training acc = 0.921129 at iter 900 epoch 0\n",
            "speed: 779.9320584116358 samples/s\n",
            "discriminator loss = 0.300885, generator loss = 1.648014, binary training acc = 0.919463 at iter 1000 epoch 0\n",
            "\n",
            "binary training acc at epoch 0: facc=0.917426\n",
            "time: 89.149782\n",
            "epoch:2\n",
            "speed: 860.0806010797648 samples/s\n",
            "discriminator loss = 0.376992, generator loss = 3.371228, binary training acc = 0.992188 at iter 0 epoch 1\n",
            "speed: 781.3099941496808 samples/s\n",
            "discriminator loss = 0.318357, generator loss = 2.026038, binary training acc = 0.902847 at iter 100 epoch 1\n",
            "speed: 784.7223169052581 samples/s\n",
            "discriminator loss = 0.450570, generator loss = 2.746444, binary training acc = 0.889731 at iter 200 epoch 1\n",
            "speed: 780.1904761904761 samples/s\n",
            "discriminator loss = 0.412927, generator loss = 2.117061, binary training acc = 0.891689 at iter 300 epoch 1\n",
            "speed: 858.7461403115902 samples/s\n",
            "discriminator loss = 0.322566, generator loss = 1.660718, binary training acc = 0.891190 at iter 400 epoch 1\n",
            "speed: 840.1130935801178 samples/s\n",
            "discriminator loss = 0.229091, generator loss = 2.767596, binary training acc = 0.886196 at iter 500 epoch 1\n",
            "speed: 780.2358303007754 samples/s\n",
            "discriminator loss = 0.181156, generator loss = 2.780925, binary training acc = 0.887869 at iter 600 epoch 1\n",
            "speed: 759.6033153267363 samples/s\n",
            "discriminator loss = 0.360368, generator loss = 2.890799, binary training acc = 0.889020 at iter 700 epoch 1\n",
            "speed: 776.116621852141 samples/s\n",
            "discriminator loss = 1.071477, generator loss = 2.971988, binary training acc = 0.882383 at iter 800 epoch 1\n",
            "speed: 784.5136190409389 samples/s\n",
            "discriminator loss = 0.786946, generator loss = 0.994077, binary training acc = 0.879561 at iter 900 epoch 1\n",
            "speed: 776.9860051001062 samples/s\n",
            "discriminator loss = 0.251603, generator loss = 3.058143, binary training acc = 0.881220 at iter 1000 epoch 1\n",
            "\n",
            "binary training acc at epoch 1: facc=0.880606\n",
            "time: 89.806339\n",
            "epoch:3\n",
            "speed: 793.6382977465305 samples/s\n",
            "discriminator loss = 0.290181, generator loss = 3.774663, binary training acc = 1.000000 at iter 0 epoch 2\n",
            "speed: 770.4762199987371 samples/s\n",
            "discriminator loss = 0.269737, generator loss = 4.100622, binary training acc = 0.918704 at iter 100 epoch 2\n",
            "speed: 761.1368362552704 samples/s\n",
            "discriminator loss = 0.196489, generator loss = 2.344529, binary training acc = 0.914062 at iter 200 epoch 2\n",
            "speed: 794.4180738794088 samples/s\n",
            "discriminator loss = 0.428538, generator loss = 1.532979, binary training acc = 0.913569 at iter 300 epoch 2\n",
            "speed: 819.2900098887817 samples/s\n",
            "discriminator loss = 0.457682, generator loss = 4.308160, binary training acc = 0.891151 at iter 400 epoch 2\n",
            "speed: 777.064727542206 samples/s\n",
            "discriminator loss = 0.150685, generator loss = 4.906131, binary training acc = 0.898063 at iter 500 epoch 2\n",
            "speed: 723.3624256926817 samples/s\n",
            "discriminator loss = 0.592002, generator loss = 1.527380, binary training acc = 0.899464 at iter 600 epoch 2\n",
            "speed: 786.2669146614725 samples/s\n",
            "discriminator loss = 0.373716, generator loss = 3.572407, binary training acc = 0.900310 at iter 700 epoch 2\n",
            "speed: 790.0132907182756 samples/s\n",
            "discriminator loss = 0.250886, generator loss = 3.850554, binary training acc = 0.898213 at iter 800 epoch 2\n",
            "speed: 767.4693610317726 samples/s\n",
            "discriminator loss = 0.162604, generator loss = 3.061638, binary training acc = 0.903154 at iter 900 epoch 2\n",
            "speed: 779.1329546542981 samples/s\n",
            "discriminator loss = 0.077280, generator loss = 3.980951, binary training acc = 0.903456 at iter 1000 epoch 2\n",
            "\n",
            "binary training acc at epoch 2: facc=0.904950\n",
            "time: 89.950356\n",
            "epoch:4\n",
            "speed: 788.1391216521724 samples/s\n",
            "discriminator loss = 0.660592, generator loss = 4.111461, binary training acc = 0.882812 at iter 0 epoch 3\n",
            "speed: 719.0107033802968 samples/s\n",
            "discriminator loss = 0.296961, generator loss = 2.947946, binary training acc = 0.885288 at iter 100 epoch 3\n",
            "speed: 791.0283070576102 samples/s\n",
            "discriminator loss = 0.273114, generator loss = 2.381556, binary training acc = 0.909165 at iter 200 epoch 3\n",
            "speed: 789.1307655673994 samples/s\n",
            "discriminator loss = 1.691528, generator loss = 1.398935, binary training acc = 0.919954 at iter 300 epoch 3\n",
            "speed: 789.7855032893575 samples/s\n",
            "discriminator loss = 0.538433, generator loss = 2.834392, binary training acc = 0.906854 at iter 400 epoch 3\n",
            "speed: 772.9632660583217 samples/s\n",
            "discriminator loss = 0.207806, generator loss = 3.520823, binary training acc = 0.906422 at iter 500 epoch 3\n",
            "speed: 786.7070401566173 samples/s\n",
            "discriminator loss = 0.186657, generator loss = 4.040283, binary training acc = 0.911164 at iter 600 epoch 3\n",
            "speed: 792.0435979522889 samples/s\n",
            "discriminator loss = 0.291679, generator loss = 2.060368, binary training acc = 0.916347 at iter 700 epoch 3\n",
            "speed: 806.3982312050516 samples/s\n",
            "discriminator loss = 0.454662, generator loss = 2.332180, binary training acc = 0.915009 at iter 800 epoch 3\n",
            "speed: 791.427110758626 samples/s\n",
            "discriminator loss = 0.994813, generator loss = 1.897217, binary training acc = 0.911609 at iter 900 epoch 3\n",
            "speed: 791.3664480181601 samples/s\n",
            "discriminator loss = 0.186512, generator loss = 3.568425, binary training acc = 0.906898 at iter 1000 epoch 3\n",
            "\n",
            "binary training acc at epoch 3: facc=0.914177\n",
            "time: 90.213975\n",
            "epoch:5\n",
            "speed: 819.5826786799256 samples/s\n",
            "discriminator loss = 0.051778, generator loss = 4.573313, binary training acc = 1.000000 at iter 0 epoch 4\n",
            "speed: 811.677222519556 samples/s\n",
            "discriminator loss = 0.332413, generator loss = 4.086510, binary training acc = 0.936417 at iter 100 epoch 4\n",
            "speed: 774.4931577201056 samples/s\n",
            "discriminator loss = 1.085295, generator loss = 10.662330, binary training acc = 0.925645 at iter 200 epoch 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iFa2SY4LsH5O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bDlq1YKqsH5R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_image = 8\n",
        "for i in range(num_image):\n",
        "    latent_z = mx.nd.random_normal(0, 1, shape=(1, latent_z_size, 1, 1), ctx=ctx)\n",
        "    img = netG(latent_z)\n",
        "    plt.subplot(2,4,i+1)\n",
        "    visualize(img[0])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rMgZM-AysH5U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_image = 12\n",
        "latent_z = mx.nd.random_normal(0, 1, shape=(1, latent_z_size, 1, 1), ctx=ctx)\n",
        "step = 0.05\n",
        "for i in range(num_image):\n",
        "    img = netG(latent_z)\n",
        "    plt.subplot(3,4,i+1)\n",
        "    visualize(img[0])\n",
        "    latent_z += 0.05\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yVYgfYLBxCUr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### SGD"
      ]
    },
    {
      "metadata": {
        "id": "Hft-PAgewTD6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# trainer for the generator and the discriminator\n",
        "trainerG = gluon.Trainer(netG.collect_params(), 'sgd', {'learning_rate': lr, 'beta1': beta1})\n",
        "trainerD = gluon.Trainer(netD.collect_params(), 'sgd', {'learning_rate': lr, 'beta1': beta1})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jn16-N5vsH5W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    print ('epoch:'+str(epoch+1))\n",
        "    tic = time.time()\n",
        "    btic = time.time()\n",
        "    train_data.reset()\n",
        "    iter = 0\n",
        "    for batch in train_data:\n",
        "        ############################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "        data = batch.data[0].as_in_context(ctx)\n",
        "        latent_z = mx.nd.random_normal(0, 1, shape=(batch_size, latent_z_size, 1, 1), ctx=ctx)\n",
        "\n",
        "        with autograd.record():\n",
        "            # train with real image\n",
        "            output = netD(data).reshape((-1, 1))\n",
        "            errD_real = loss(output, real_label)\n",
        "            metric.update([real_label,], [output,])\n",
        "\n",
        "            # train with fake image\n",
        "            fake = netG(latent_z)\n",
        "            output = netD(fake.detach()).reshape((-1, 1))\n",
        "            errD_fake = loss(output, fake_label)\n",
        "            errD = errD_real + errD_fake\n",
        "            errD.backward()\n",
        "            metric.update([fake_label,], [output,])\n",
        "\n",
        "        trainerD.step(batch.data[0].shape[0])\n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "        with autograd.record():\n",
        "            fake = netG(latent_z)\n",
        "            output = netD(fake).reshape((-1, 1))\n",
        "            errG = loss(output, real_label)\n",
        "            errG.backward()\n",
        "\n",
        "        trainerG.step(batch.data[0].shape[0])\n",
        "\n",
        "        # Print log infomation every ten batches\n",
        "        if iter % 100 == 0:\n",
        "            name, acc = metric.get()\n",
        "            print('speed: {} samples/s'.format(batch_size / (time.time() - btic)))\n",
        "            print('discriminator loss = %f, generator loss = %f, binary training acc = %f at iter %d epoch %d'\n",
        "                     %(nd.mean(errD).asscalar(),\n",
        "                       nd.mean(errG).asscalar(), acc, iter, epoch))\n",
        "        iter = iter + 1\n",
        "        btic = time.time()\n",
        "\n",
        "    name, acc = metric.get()\n",
        "    metric.reset()\n",
        "    print('\\nbinary training acc at epoch %d: %s=%f' % (epoch, name, acc))\n",
        "    print('time: %f' % (time.time() - tic))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u820p-v2sH5a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_image = 8\n",
        "for i in range(num_image):\n",
        "    latent_z = mx.nd.random_normal(0, 1, shape=(1, latent_z_size, 1, 1), ctx=ctx)\n",
        "    img = netG(latent_z)\n",
        "    plt.subplot(2,4,i+1)\n",
        "    visualize(img[0])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8XqefC80sH5e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_image = 12\n",
        "latent_z = mx.nd.random_normal(0, 1, shape=(1, latent_z_size, 1, 1), ctx=ctx)\n",
        "step = 0.05\n",
        "for i in range(num_image):\n",
        "    img = netG(latent_z)\n",
        "    plt.subplot(3,4,i+1)\n",
        "    visualize(img[0])\n",
        "    latent_z += 0.05\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wVsfMnfBsH5i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QDJeBrcDsH5l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zVlbmUMHsH5o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}